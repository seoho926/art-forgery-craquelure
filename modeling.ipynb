{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 17.6MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from keras) (1.14.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from keras) (1.11.0)\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/56/4bcec5a8d9503a87e58e814c4e32ac2b32c37c685672c30bc8c54c6e478a/Keras_Applications-1.0.8.tar.gz (289kB)\n",
      "\u001b[K    100% |████████████████████████████████| 296kB 26.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from keras) (2.8.0)\n",
      "Building wheels for collected packages: keras-applications\n",
      "  Running setup.py bdist_wheel for keras-applications ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/dd/f2/5d/2689b5547f32c4e258c3b7ccbe7f1d0f2afbb84fb01e830792\n",
      "Successfully built keras-applications\n",
      "\u001b[31mtyping-extensions 3.7.4.1 has requirement typing>=3.7.4; python_version < \"3.5\", but you'll have typing 3.6.4 which is incompatible.\u001b[0m\n",
      "Installing collected packages: keras-preprocessing, keras-applications, keras\n",
      "Successfully installed keras-2.3.1 keras-applications-1.0.8 keras-preprocessing-1.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting pip\n",
      "  Using cached https://files.pythonhosted.org/packages/00/b6/9cfa56b4081ad13874b0c6f96af8ce16cfbc1cb06bedf8e9164ce5551ec1/pip-19.3.1-py2.py3-none-any.whl\n",
      "\u001b[31mtyping-extensions 3.7.4.1 has requirement typing>=3.7.4; python_version < \"3.5\", but you'll have typing 3.6.4 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pip\n",
      "  Found existing installation: pip 10.0.1\n",
      "    Uninstalling pip-10.0.1:\n",
      "      Successfully uninstalled pip-10.0.1\n",
      "Successfully installed pip-19.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 463 images belonging to 2 classes.\n",
      "Found 1854 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Directory path\n",
    "train_data_dir = 'data/Cracks/train'\n",
    "test_data_dir = 'data/Cracks/test'\n",
    "\n",
    "# Get all the data in the directory data/validation, and reshape them\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "        test_data_dir, \n",
    "        target_size=(256, 256), batch_size=463, color_mode='grayscale')\n",
    "\n",
    "# Get all the data in the directory data/train, and reshape them\n",
    "train_generator = ImageDataGenerator().flow_from_directory(\n",
    "        train_data_dir, \n",
    "        target_size=(256, 256), batch_size=1854, color_mode='grayscale')\n",
    "\n",
    "# Create the datasets\n",
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256 at 0x7FA064426CC0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previewing an image\n",
    "array_to_img(train_images[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (1854, 256, 256, 1) (1854, 2)\n",
      "test data shape: (463, 256, 256, 1) (463, 2)\n"
     ]
    }
   ],
   "source": [
    "# Checking shape of data\n",
    "print('train data shape:', np.shape(train_images), np.shape(train_labels))\n",
    "print('test data shape:', np.shape(test_images), np.shape(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_img: (1854, 65536)\n",
      "test_img: (463, 65536)\n"
     ]
    }
   ],
   "source": [
    "# Unrowing/reshaping\n",
    "train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "print('train_img:', np.shape(train_img))\n",
    "\n",
    "test_img = test_images.reshape(test_images.shape[0], -1)\n",
    "print('test_img:', np.shape(test_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the labels\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dutch': 0, 'Flemish': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels final: (1854, 1)\n",
      "test labels final: (463, 1)\n"
     ]
    }
   ],
   "source": [
    "# Transposing the labels\n",
    "train_y = np.reshape(train_labels[:,0], (1854,1))\n",
    "print('train labels final:', np.shape(train_y))\n",
    "\n",
    "test_y = np.reshape(test_labels[:,0], (463,1))\n",
    "print('test labels final:', np.shape(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(20, activation='relu', input_shape=(65536,))) #2 hidden layers\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/5\n",
      "1297/1297 [==============================] - 1s 580us/step - loss: 7.6516 - accuracy: 0.5112 - val_loss: 6.8120 - val_accuracy: 0.5727\n",
      "Epoch 2/5\n",
      "1297/1297 [==============================] - 0s 377us/step - loss: 7.0800 - accuracy: 0.5559 - val_loss: 6.8120 - val_accuracy: 0.5727\n",
      "Epoch 3/5\n",
      "1297/1297 [==============================] - 0s 374us/step - loss: 7.0800 - accuracy: 0.5559 - val_loss: 6.8120 - val_accuracy: 0.5727\n",
      "Epoch 4/5\n",
      "1297/1297 [==============================] - 0s 372us/step - loss: 7.0800 - accuracy: 0.5559 - val_loss: 6.8120 - val_accuracy: 0.5727\n",
      "Epoch 5/5\n",
      "1297/1297 [==============================] - 0s 370us/step - loss: 7.0800 - accuracy: 0.5559 - val_loss: 6.8120 - val_accuracy: 0.5727\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "histoire = model.fit(train_img,\n",
    "                    train_y,\n",
    "                    epochs=5,\n",
    "                    batch_size=100,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First CNN\n",
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(200, (3, 3), activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(100, (4, 4), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(70, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/5\n",
      "1297/1297 [==============================] - 673s 519ms/step - loss: 8.6901 - accuracy: 0.4549 - val_loss: 9.2310 - val_accuracy: 0.4273\n",
      "Epoch 2/5\n",
      "1297/1297 [==============================] - 668s 515ms/step - loss: 8.9600 - accuracy: 0.4441 - val_loss: 9.2310 - val_accuracy: 0.4273\n",
      "Epoch 3/5\n",
      "1297/1297 [==============================] - 664s 512ms/step - loss: 8.9600 - accuracy: 0.4441 - val_loss: 9.2310 - val_accuracy: 0.4273\n",
      "Epoch 4/5\n",
      " 900/1297 [===================>..........] - ETA: 3:01 - loss: 9.1157 - accuracy: 0.4344"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b2a4256263c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                     validation_split=0.3)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=5,\n",
    "                    batch_size=50,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(20, (3, 3), activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(70, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(150, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(200, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(250, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/5\n",
      "1297/1297 [==============================] - 134s 104ms/step - loss: 8.5945 - accuracy: 0.4549 - val_loss: 9.2310 - val_accuracy: 0.4273\n",
      "Epoch 2/5\n",
      "1297/1297 [==============================] - 134s 103ms/step - loss: 8.9600 - accuracy: 0.4441 - val_loss: 9.2310 - val_accuracy: 0.4273\n",
      "Epoch 3/5\n",
      " 350/1297 [=======>......................] - ETA: 1:25 - loss: 8.8880 - accuracy: 0.4486"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3df4d6f24b64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                     validation_split=0.3)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_2 = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=5,\n",
    "                    batch_size=50,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(20, (3, 3), activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(70, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(150, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(200, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(250, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/5\n",
      "1297/1297 [==============================] - 133s 103ms/step - loss: 0.7820 - accuracy: 0.4965 - val_loss: 0.6893 - val_accuracy: 0.5727\n",
      "Epoch 2/5\n",
      "1297/1297 [==============================] - 134s 103ms/step - loss: 0.6865 - accuracy: 0.5559 - val_loss: 0.6751 - val_accuracy: 0.5727\n",
      "Epoch 3/5\n",
      "1297/1297 [==============================] - 133s 102ms/step - loss: 0.6688 - accuracy: 0.5559 - val_loss: 0.6672 - val_accuracy: 0.5727\n",
      "Epoch 4/5\n",
      "1297/1297 [==============================] - 133s 103ms/step - loss: 0.6643 - accuracy: 0.5659 - val_loss: 0.6906 - val_accuracy: 0.5566\n",
      "Epoch 5/5\n",
      "1297/1297 [==============================] - 132s 102ms/step - loss: 0.6893 - accuracy: 0.5389 - val_loss: 0.6850 - val_accuracy: 0.5727\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_3 = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=5,\n",
    "                    batch_size=50,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(20, (3, 3), activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(70, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(150, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(200, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(250, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/10\n",
      "1297/1297 [==============================] - 135s 104ms/step - loss: 0.7362 - accuracy: 0.4811 - val_loss: 0.6913 - val_accuracy: 0.5637\n",
      "Epoch 2/10\n",
      "1297/1297 [==============================] - 135s 104ms/step - loss: 0.6900 - accuracy: 0.5598 - val_loss: 0.6872 - val_accuracy: 0.5637\n",
      "Epoch 3/10\n",
      "1297/1297 [==============================] - 135s 104ms/step - loss: 0.6883 - accuracy: 0.5598 - val_loss: 0.6815 - val_accuracy: 0.5637\n",
      "Epoch 4/10\n",
      "1297/1297 [==============================] - 134s 103ms/step - loss: 0.6872 - accuracy: 0.5598 - val_loss: 0.6840 - val_accuracy: 0.5637\n",
      "Epoch 5/10\n",
      "1297/1297 [==============================] - 135s 104ms/step - loss: 0.6876 - accuracy: 0.5598 - val_loss: 0.6818 - val_accuracy: 0.5637\n",
      "Epoch 6/10\n",
      "1297/1297 [==============================] - 134s 104ms/step - loss: 0.6865 - accuracy: 0.5598 - val_loss: 0.6857 - val_accuracy: 0.5637\n",
      "Epoch 7/10\n",
      "1297/1297 [==============================] - 135s 104ms/step - loss: 0.6863 - accuracy: 0.5598 - val_loss: 0.6840 - val_accuracy: 0.5637\n",
      "Epoch 8/10\n",
      "1297/1297 [==============================] - 134s 103ms/step - loss: 0.6825 - accuracy: 0.5598 - val_loss: 0.6756 - val_accuracy: 0.5637\n",
      "Epoch 9/10\n",
      "1297/1297 [==============================] - 135s 104ms/step - loss: 0.6717 - accuracy: 0.5598 - val_loss: 0.6869 - val_accuracy: 0.5637\n",
      "Epoch 10/10\n",
      "1297/1297 [==============================] - 134s 104ms/step - loss: 0.6845 - accuracy: 0.5598 - val_loss: 0.6769 - val_accuracy: 0.5637\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_3 = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=10,\n",
    "                    batch_size=100,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(20, (3, 3), activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(250, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(100, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(50, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(250, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/10\n",
      "1297/1297 [==============================] - 231s 178ms/step - loss: 0.8093 - accuracy: 0.5544 - val_loss: 0.6850 - val_accuracy: 0.5566\n",
      "Epoch 2/10\n",
      "1297/1297 [==============================] - 229s 176ms/step - loss: 0.6949 - accuracy: 0.5628 - val_loss: 0.6839 - val_accuracy: 0.5566\n",
      "Epoch 3/10\n",
      "1297/1297 [==============================] - 228s 176ms/step - loss: 0.6953 - accuracy: 0.5305 - val_loss: 0.6844 - val_accuracy: 0.5566\n",
      "Epoch 4/10\n",
      "1297/1297 [==============================] - 228s 176ms/step - loss: 0.6849 - accuracy: 0.5628 - val_loss: 0.6793 - val_accuracy: 0.5566\n",
      "Epoch 5/10\n",
      "1297/1297 [==============================] - 229s 177ms/step - loss: 0.6785 - accuracy: 0.5628 - val_loss: 0.6784 - val_accuracy: 0.5566\n",
      "Epoch 6/10\n",
      "1297/1297 [==============================] - 228s 175ms/step - loss: 0.6769 - accuracy: 0.5628 - val_loss: 0.6636 - val_accuracy: 0.5566\n",
      "Epoch 7/10\n",
      " 100/1297 [=>............................] - ETA: 3:04 - loss: 0.6396 - accuracy: 0.6500"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_4 = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=10,\n",
    "                    batch_size=100,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=3, kernel_size=128, strides=(3, 3), activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=3, kernel_size=128, strides=(3, 3), activation='relu'))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=3, kernel_size=64, strides=(3, 3), activation='relu'))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=3, kernel_size=32, strides=(3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=3, kernel_size=1, strides=(3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/10\n",
      "1297/1297 [==============================] - 241s 186ms/step - loss: 0.9748 - accuracy: 0.4796 - val_loss: 0.6925 - val_accuracy: 0.5530\n",
      "Epoch 2/10\n",
      "1297/1297 [==============================] - 239s 184ms/step - loss: 0.6914 - accuracy: 0.5644 - val_loss: 0.6905 - val_accuracy: 0.5530\n",
      "Epoch 3/10\n",
      "1297/1297 [==============================] - 239s 185ms/step - loss: 0.6888 - accuracy: 0.5644 - val_loss: 0.6886 - val_accuracy: 0.5530\n",
      "Epoch 4/10\n",
      "1297/1297 [==============================] - 240s 185ms/step - loss: 0.6870 - accuracy: 0.5644 - val_loss: 0.6876 - val_accuracy: 0.5530\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-1956eb95c245>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                     validation_split=0.3)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_5 = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=10,\n",
    "                    batch_size=100,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=10, kernel_size=10, strides=(3, 3), activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=20, kernel_size=5, strides=(3, 3), activation='relu'))\n",
    "model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=40, kernel_size=3, strides=(3, 3), activation='relu'))\n",
    "model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=80, kernel_size=3, strides=(3, 3), activation='relu'))\n",
    "model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=160, kernel_size=1, strides=(3, 3), activation='relu'))\n",
    "model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/50\n",
      "1297/1297 [==============================] - 13s 10ms/step - loss: 0.6833 - accuracy: 0.5659 - val_loss: 0.6885 - val_accuracy: 0.5530\n",
      "Epoch 2/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6822 - accuracy: 0.5652 - val_loss: 0.7032 - val_accuracy: 0.5530\n",
      "Epoch 3/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6802 - accuracy: 0.5644 - val_loss: 0.6854 - val_accuracy: 0.5530\n",
      "Epoch 4/50\n",
      "1297/1297 [==============================] - 13s 10ms/step - loss: 0.6641 - accuracy: 0.5644 - val_loss: 0.6800 - val_accuracy: 0.5530\n",
      "Epoch 5/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6601 - accuracy: 0.5590 - val_loss: 0.6719 - val_accuracy: 0.5314\n",
      "Epoch 6/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6547 - accuracy: 0.5520 - val_loss: 0.6764 - val_accuracy: 0.5278\n",
      "Epoch 7/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6471 - accuracy: 0.5821 - val_loss: 0.6755 - val_accuracy: 0.5206\n",
      "Epoch 8/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6351 - accuracy: 0.5682 - val_loss: 0.6731 - val_accuracy: 0.5566\n",
      "Epoch 9/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6454 - accuracy: 0.6060 - val_loss: 0.6812 - val_accuracy: 0.5278\n",
      "Epoch 10/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6696 - accuracy: 0.5891 - val_loss: 0.6932 - val_accuracy: 0.5296\n",
      "Epoch 11/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6849 - accuracy: 0.5428 - val_loss: 0.6910 - val_accuracy: 0.4847\n",
      "Epoch 12/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6832 - accuracy: 0.5405 - val_loss: 0.6900 - val_accuracy: 0.5332\n",
      "Epoch 13/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6808 - accuracy: 0.5636 - val_loss: 0.6890 - val_accuracy: 0.5224\n",
      "Epoch 14/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6806 - accuracy: 0.5636 - val_loss: 0.6870 - val_accuracy: 0.5530\n",
      "Epoch 15/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6696 - accuracy: 0.5644 - val_loss: 0.6723 - val_accuracy: 0.5422\n",
      "Epoch 16/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6554 - accuracy: 0.5736 - val_loss: 0.6744 - val_accuracy: 0.5135\n",
      "Epoch 17/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6589 - accuracy: 0.5883 - val_loss: 0.6828 - val_accuracy: 0.5458\n",
      "Epoch 18/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6478 - accuracy: 0.5790 - val_loss: 0.6735 - val_accuracy: 0.5206\n",
      "Epoch 19/50\n",
      "1297/1297 [==============================] - 13s 10ms/step - loss: 0.6805 - accuracy: 0.5297 - val_loss: 0.6908 - val_accuracy: 0.4973\n",
      "Epoch 20/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6865 - accuracy: 0.5289 - val_loss: 0.6896 - val_accuracy: 0.5278\n",
      "Epoch 21/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6820 - accuracy: 0.5690 - val_loss: 0.6742 - val_accuracy: 0.5332\n",
      "Epoch 22/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6791 - accuracy: 0.5374 - val_loss: 0.7106 - val_accuracy: 0.5458\n",
      "Epoch 23/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6858 - accuracy: 0.5536 - val_loss: 0.6886 - val_accuracy: 0.5530\n",
      "Epoch 24/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6809 - accuracy: 0.5644 - val_loss: 0.6955 - val_accuracy: 0.5530\n",
      "Epoch 25/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6703 - accuracy: 0.5644 - val_loss: 0.6765 - val_accuracy: 0.5530\n",
      "Epoch 26/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6600 - accuracy: 0.5644 - val_loss: 0.6871 - val_accuracy: 0.5530\n",
      "Epoch 27/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6826 - accuracy: 0.5644 - val_loss: 0.6864 - val_accuracy: 0.5530\n",
      "Epoch 28/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6617 - accuracy: 0.5644 - val_loss: 0.6846 - val_accuracy: 0.5530\n",
      "Epoch 29/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6748 - accuracy: 0.5644 - val_loss: 0.6838 - val_accuracy: 0.5530\n",
      "Epoch 30/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6666 - accuracy: 0.5644 - val_loss: 0.6986 - val_accuracy: 0.5530\n",
      "Epoch 31/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6651 - accuracy: 0.5644 - val_loss: 0.6797 - val_accuracy: 0.5530\n",
      "Epoch 32/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6397 - accuracy: 0.5744 - val_loss: 0.6988 - val_accuracy: 0.5458\n",
      "Epoch 33/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6391 - accuracy: 0.5983 - val_loss: 0.6813 - val_accuracy: 0.5673\n",
      "Epoch 34/50\n",
      "1297/1297 [==============================] - 12s 10ms/step - loss: 0.6170 - accuracy: 0.6191 - val_loss: 0.7396 - val_accuracy: 0.5332\n",
      "Epoch 35/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6071 - accuracy: 0.6345 - val_loss: 0.7588 - val_accuracy: 0.5476\n",
      "Epoch 36/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6010 - accuracy: 0.6523 - val_loss: 0.9549 - val_accuracy: 0.5583\n",
      "Epoch 37/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6282 - accuracy: 0.6176 - val_loss: 0.8407 - val_accuracy: 0.5512\n",
      "Epoch 38/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6126 - accuracy: 0.6415 - val_loss: 0.7106 - val_accuracy: 0.5637\n",
      "Epoch 39/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6049 - accuracy: 0.6554 - val_loss: 0.7641 - val_accuracy: 0.5512\n",
      "Epoch 40/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.5889 - accuracy: 0.6484 - val_loss: 1.1228 - val_accuracy: 0.5242\n",
      "Epoch 41/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6858 - accuracy: 0.5837 - val_loss: 0.6947 - val_accuracy: 0.5117\n",
      "Epoch 42/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6683 - accuracy: 0.5821 - val_loss: 0.6876 - val_accuracy: 0.5224\n",
      "Epoch 43/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6292 - accuracy: 0.6160 - val_loss: 0.8359 - val_accuracy: 0.5458\n",
      "Epoch 44/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6168 - accuracy: 0.6307 - val_loss: 0.8044 - val_accuracy: 0.5171\n",
      "Epoch 45/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6060 - accuracy: 0.6430 - val_loss: 0.8263 - val_accuracy: 0.5260\n",
      "Epoch 46/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6013 - accuracy: 0.6469 - val_loss: 0.7822 - val_accuracy: 0.5673\n",
      "Epoch 47/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.5962 - accuracy: 0.6638 - val_loss: 0.8260 - val_accuracy: 0.5512\n",
      "Epoch 48/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6029 - accuracy: 0.6500 - val_loss: 0.8352 - val_accuracy: 0.5619\n",
      "Epoch 49/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6063 - accuracy: 0.6500 - val_loss: 0.7191 - val_accuracy: 0.5781\n",
      "Epoch 50/50\n",
      "1297/1297 [==============================] - 12s 10ms/step - loss: 0.5967 - accuracy: 0.6631 - val_loss: 0.7523 - val_accuracy: 0.5619\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_6 = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    batch_size=100,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=10, kernel_size=10, strides=2, activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "model.add(layers.AveragePooling2D((10, 10)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=20, kernel_size=5, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((6, 6)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=40, kernel_size=3, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=80, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=160, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 1.2859 - accuracy: 0.4603 - val_loss: 0.7118 - val_accuracy: 0.4057\n",
      "Epoch 2/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.7160 - accuracy: 0.4534 - val_loss: 1.0765 - val_accuracy: 0.5907\n",
      "Epoch 3/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 1.0607 - accuracy: 0.5359 - val_loss: 0.8590 - val_accuracy: 0.4093\n",
      "Epoch 4/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.7960 - accuracy: 0.4518 - val_loss: 0.6941 - val_accuracy: 0.4811\n",
      "Epoch 5/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6934 - accuracy: 0.4904 - val_loss: 0.6943 - val_accuracy: 0.4758\n",
      "Epoch 6/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6921 - accuracy: 0.4927 - val_loss: 0.6756 - val_accuracy: 0.5907\n",
      "Epoch 7/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6833 - accuracy: 0.5482 - val_loss: 0.6894 - val_accuracy: 0.5099\n",
      "Epoch 8/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6841 - accuracy: 0.5582 - val_loss: 0.6727 - val_accuracy: 0.5907\n",
      "Epoch 9/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6838 - accuracy: 0.5482 - val_loss: 0.6749 - val_accuracy: 0.5907\n",
      "Epoch 10/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6858 - accuracy: 0.5482 - val_loss: 0.6761 - val_accuracy: 0.5907\n",
      "Epoch 11/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6866 - accuracy: 0.5482 - val_loss: 0.6766 - val_accuracy: 0.5907\n",
      "Epoch 12/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.6868 - accuracy: 0.5482 - val_loss: 0.6771 - val_accuracy: 0.5907\n",
      "Epoch 13/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6859 - accuracy: 0.5482 - val_loss: 0.6761 - val_accuracy: 0.5907\n",
      "Epoch 14/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6847 - accuracy: 0.5482 - val_loss: 0.6744 - val_accuracy: 0.5907\n",
      "Epoch 15/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6829 - accuracy: 0.5482 - val_loss: 0.6716 - val_accuracy: 0.5907\n",
      "Epoch 16/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6807 - accuracy: 0.5482 - val_loss: 0.6690 - val_accuracy: 0.5907\n",
      "Epoch 17/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6780 - accuracy: 0.5482 - val_loss: 0.6659 - val_accuracy: 0.5907\n",
      "Epoch 18/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6749 - accuracy: 0.5482 - val_loss: 0.6629 - val_accuracy: 0.5907\n",
      "Epoch 19/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6716 - accuracy: 0.5482 - val_loss: 0.6634 - val_accuracy: 0.5907\n",
      "Epoch 20/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.6703 - accuracy: 0.5482 - val_loss: 0.6609 - val_accuracy: 0.5889\n",
      "Epoch 21/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6683 - accuracy: 0.5490 - val_loss: 0.6499 - val_accuracy: 0.5907\n",
      "Epoch 22/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6638 - accuracy: 0.5490 - val_loss: 0.6509 - val_accuracy: 0.6140\n",
      "Epoch 23/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6652 - accuracy: 0.5251 - val_loss: 0.6436 - val_accuracy: 0.6086\n",
      "Epoch 24/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6635 - accuracy: 0.5520 - val_loss: 0.6392 - val_accuracy: 0.5943\n",
      "Epoch 25/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6604 - accuracy: 0.5536 - val_loss: 0.6513 - val_accuracy: 0.6014\n",
      "Epoch 26/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6596 - accuracy: 0.5628 - val_loss: 0.6539 - val_accuracy: 0.5763\n",
      "Epoch 27/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6603 - accuracy: 0.5790 - val_loss: 0.6477 - val_accuracy: 0.5943\n",
      "Epoch 28/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.6572 - accuracy: 0.5644 - val_loss: 0.6409 - val_accuracy: 0.5853\n",
      "Epoch 29/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6552 - accuracy: 0.5667 - val_loss: 0.6370 - val_accuracy: 0.6230\n",
      "Epoch 30/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6553 - accuracy: 0.5821 - val_loss: 0.6361 - val_accuracy: 0.5996\n",
      "Epoch 31/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6526 - accuracy: 0.5705 - val_loss: 0.6355 - val_accuracy: 0.6104\n",
      "Epoch 32/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6499 - accuracy: 0.5806 - val_loss: 0.6353 - val_accuracy: 0.6140\n",
      "Epoch 33/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6476 - accuracy: 0.5883 - val_loss: 0.6302 - val_accuracy: 0.6050\n",
      "Epoch 34/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6452 - accuracy: 0.5906 - val_loss: 0.6398 - val_accuracy: 0.6176\n",
      "Epoch 35/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6456 - accuracy: 0.6191 - val_loss: 0.6208 - val_accuracy: 0.6230\n",
      "Epoch 36/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.6412 - accuracy: 0.5921 - val_loss: 0.6208 - val_accuracy: 0.6320\n",
      "Epoch 37/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6354 - accuracy: 0.6122 - val_loss: 0.6372 - val_accuracy: 0.5691\n",
      "Epoch 38/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6397 - accuracy: 0.5952 - val_loss: 0.6204 - val_accuracy: 0.6320\n",
      "Epoch 39/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6319 - accuracy: 0.6137 - val_loss: 0.6205 - val_accuracy: 0.6355\n",
      "Epoch 40/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6275 - accuracy: 0.6153 - val_loss: 0.6112 - val_accuracy: 0.6427\n",
      "Epoch 41/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6199 - accuracy: 0.6207 - val_loss: 0.6143 - val_accuracy: 0.6409\n",
      "Epoch 42/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6158 - accuracy: 0.6145 - val_loss: 0.6235 - val_accuracy: 0.5907\n",
      "Epoch 43/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.6155 - accuracy: 0.6307 - val_loss: 0.6262 - val_accuracy: 0.6320\n",
      "Epoch 44/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.6316 - accuracy: 0.6222 - val_loss: 0.7848 - val_accuracy: 0.4722\n",
      "Epoch 45/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.7072 - accuracy: 0.5420 - val_loss: 0.6337 - val_accuracy: 0.6517\n",
      "Epoch 46/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6332 - accuracy: 0.6130 - val_loss: 0.6596 - val_accuracy: 0.5889\n",
      "Epoch 47/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6627 - accuracy: 0.5490 - val_loss: 0.6579 - val_accuracy: 0.5907\n",
      "Epoch 48/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6575 - accuracy: 0.5482 - val_loss: 0.6586 - val_accuracy: 0.5907\n",
      "Epoch 49/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6558 - accuracy: 0.5482 - val_loss: 0.6560 - val_accuracy: 0.5907\n",
      "Epoch 50/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6533 - accuracy: 0.5482 - val_loss: 0.6495 - val_accuracy: 0.5907\n",
      "Epoch 51/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.6485 - accuracy: 0.5482 - val_loss: 0.6437 - val_accuracy: 0.5907\n",
      "Epoch 52/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.6439 - accuracy: 0.5497 - val_loss: 0.6386 - val_accuracy: 0.6014\n",
      "Epoch 53/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6370 - accuracy: 0.5613 - val_loss: 0.6350 - val_accuracy: 0.6463\n",
      "Epoch 54/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6302 - accuracy: 0.6145 - val_loss: 0.6258 - val_accuracy: 0.6697\n",
      "Epoch 55/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6217 - accuracy: 0.6438 - val_loss: 0.6234 - val_accuracy: 0.6840\n",
      "Epoch 56/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6103 - accuracy: 0.6777 - val_loss: 0.6171 - val_accuracy: 0.6750\n",
      "Epoch 57/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5983 - accuracy: 0.6692 - val_loss: 0.6173 - val_accuracy: 0.6373\n",
      "Epoch 58/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5822 - accuracy: 0.6731 - val_loss: 0.6380 - val_accuracy: 0.6014\n",
      "Epoch 59/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.5952 - accuracy: 0.6739 - val_loss: 0.7106 - val_accuracy: 0.4955\n",
      "Epoch 60/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6650 - accuracy: 0.5906 - val_loss: 0.6576 - val_accuracy: 0.6553\n",
      "Epoch 61/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6596 - accuracy: 0.6507 - val_loss: 0.6570 - val_accuracy: 0.6445\n",
      "Epoch 62/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6430 - accuracy: 0.6315 - val_loss: 0.6276 - val_accuracy: 0.6194\n",
      "Epoch 63/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6075 - accuracy: 0.6369 - val_loss: 0.6378 - val_accuracy: 0.6068\n",
      "Epoch 64/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6107 - accuracy: 0.6261 - val_loss: 0.6196 - val_accuracy: 0.6499\n",
      "Epoch 65/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6041 - accuracy: 0.6554 - val_loss: 0.6248 - val_accuracy: 0.6535\n",
      "Epoch 66/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6072 - accuracy: 0.6500 - val_loss: 0.6215 - val_accuracy: 0.6176\n",
      "Epoch 67/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.5951 - accuracy: 0.6677 - val_loss: 0.6263 - val_accuracy: 0.6176\n",
      "Epoch 68/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5890 - accuracy: 0.6677 - val_loss: 0.5969 - val_accuracy: 0.6697\n",
      "Epoch 69/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5739 - accuracy: 0.6746 - val_loss: 0.6498 - val_accuracy: 0.5871\n",
      "Epoch 70/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.5736 - accuracy: 0.6847 - val_loss: 0.6041 - val_accuracy: 0.6804\n",
      "Epoch 71/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5836 - accuracy: 0.6608 - val_loss: 0.5980 - val_accuracy: 0.6894\n",
      "Epoch 72/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5632 - accuracy: 0.6901 - val_loss: 0.6024 - val_accuracy: 0.6517\n",
      "Epoch 73/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5423 - accuracy: 0.7101 - val_loss: 0.6210 - val_accuracy: 0.6553\n",
      "Epoch 74/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5698 - accuracy: 0.6715 - val_loss: 0.5848 - val_accuracy: 0.6804\n",
      "Epoch 75/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.5447 - accuracy: 0.6893 - val_loss: 0.6066 - val_accuracy: 0.6715\n",
      "Epoch 76/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.5553 - accuracy: 0.6862 - val_loss: 0.6095 - val_accuracy: 0.6732\n",
      "Epoch 77/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5486 - accuracy: 0.6931 - val_loss: 0.6334 - val_accuracy: 0.6750\n",
      "Epoch 78/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.5488 - accuracy: 0.6870 - val_loss: 0.6385 - val_accuracy: 0.6427\n",
      "Epoch 79/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.5333 - accuracy: 0.6870 - val_loss: 0.6291 - val_accuracy: 0.6697\n",
      "Epoch 80/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5164 - accuracy: 0.7093 - val_loss: 0.6236 - val_accuracy: 0.6553\n",
      "Epoch 81/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5032 - accuracy: 0.7155 - val_loss: 0.6364 - val_accuracy: 0.6445\n",
      "Epoch 82/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4938 - accuracy: 0.7247 - val_loss: 0.6467 - val_accuracy: 0.6463\n",
      "Epoch 83/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.4965 - accuracy: 0.7294 - val_loss: 0.6474 - val_accuracy: 0.6050\n",
      "Epoch 84/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4819 - accuracy: 0.7294 - val_loss: 0.6352 - val_accuracy: 0.6589\n",
      "Epoch 85/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.4732 - accuracy: 0.7386 - val_loss: 0.6344 - val_accuracy: 0.6786\n",
      "Epoch 86/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4626 - accuracy: 0.7448 - val_loss: 0.6572 - val_accuracy: 0.6661\n",
      "Epoch 87/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4486 - accuracy: 0.7625 - val_loss: 0.6715 - val_accuracy: 0.6643\n",
      "Epoch 88/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.4360 - accuracy: 0.7641 - val_loss: 0.7322 - val_accuracy: 0.6571\n",
      "Epoch 89/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.4441 - accuracy: 0.7679 - val_loss: 0.6906 - val_accuracy: 0.6535\n",
      "Epoch 90/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4262 - accuracy: 0.7918 - val_loss: 0.6937 - val_accuracy: 0.6284\n",
      "Epoch 91/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.4567 - accuracy: 0.7641 - val_loss: 0.6842 - val_accuracy: 0.6571\n",
      "Epoch 92/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4473 - accuracy: 0.7502 - val_loss: 0.6850 - val_accuracy: 0.6248\n",
      "Epoch 93/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4393 - accuracy: 0.7564 - val_loss: 0.6552 - val_accuracy: 0.6625\n",
      "Epoch 94/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4129 - accuracy: 0.7918 - val_loss: 0.6953 - val_accuracy: 0.6481\n",
      "Epoch 95/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.4211 - accuracy: 0.7795 - val_loss: 0.6442 - val_accuracy: 0.6517\n",
      "Epoch 96/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4090 - accuracy: 0.7926 - val_loss: 0.7113 - val_accuracy: 0.6230\n",
      "Epoch 97/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4137 - accuracy: 0.7857 - val_loss: 0.8536 - val_accuracy: 0.6230\n",
      "Epoch 98/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3917 - accuracy: 0.7826 - val_loss: 0.6926 - val_accuracy: 0.6535\n",
      "Epoch 99/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.4027 - accuracy: 0.8011 - val_loss: 0.7286 - val_accuracy: 0.6553\n",
      "Epoch 100/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3983 - accuracy: 0.8026 - val_loss: 0.7429 - val_accuracy: 0.6230\n",
      "Epoch 101/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.3960 - accuracy: 0.8134 - val_loss: 0.7510 - val_accuracy: 0.6122\n",
      "Epoch 102/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4043 - accuracy: 0.7911 - val_loss: 0.7057 - val_accuracy: 0.6643\n",
      "Epoch 103/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3677 - accuracy: 0.8126 - val_loss: 0.7889 - val_accuracy: 0.6355\n",
      "Epoch 104/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3734 - accuracy: 0.8196 - val_loss: 0.7482 - val_accuracy: 0.6427\n",
      "Epoch 105/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3632 - accuracy: 0.8227 - val_loss: 0.7024 - val_accuracy: 0.6661\n",
      "Epoch 106/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3462 - accuracy: 0.8443 - val_loss: 0.7406 - val_accuracy: 0.6661\n",
      "Epoch 107/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.3321 - accuracy: 0.8396 - val_loss: 0.7909 - val_accuracy: 0.6607\n",
      "Epoch 108/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3103 - accuracy: 0.8527 - val_loss: 0.8074 - val_accuracy: 0.6571\n",
      "Epoch 109/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3073 - accuracy: 0.8589 - val_loss: 0.8451 - val_accuracy: 0.6535\n",
      "Epoch 110/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3063 - accuracy: 0.8581 - val_loss: 0.9177 - val_accuracy: 0.6589\n",
      "Epoch 111/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2832 - accuracy: 0.8720 - val_loss: 1.1454 - val_accuracy: 0.6427\n",
      "Epoch 112/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5090 - accuracy: 0.7926 - val_loss: 1.1000 - val_accuracy: 0.6409\n",
      "Epoch 113/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6093 - accuracy: 0.7556 - val_loss: 0.7613 - val_accuracy: 0.6463\n",
      "Epoch 114/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4474 - accuracy: 0.7764 - val_loss: 0.7598 - val_accuracy: 0.6302\n",
      "Epoch 115/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.4723 - accuracy: 0.7672 - val_loss: 0.7355 - val_accuracy: 0.5763\n",
      "Epoch 116/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5355 - accuracy: 0.7224 - val_loss: 0.7415 - val_accuracy: 0.6050\n",
      "Epoch 117/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4771 - accuracy: 0.7672 - val_loss: 0.7024 - val_accuracy: 0.6194\n",
      "Epoch 118/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4131 - accuracy: 0.8150 - val_loss: 0.6951 - val_accuracy: 0.6409\n",
      "Epoch 119/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3914 - accuracy: 0.8165 - val_loss: 0.6674 - val_accuracy: 0.6320\n",
      "Epoch 120/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3733 - accuracy: 0.8304 - val_loss: 0.6813 - val_accuracy: 0.6373\n",
      "Epoch 121/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3541 - accuracy: 0.8489 - val_loss: 0.7130 - val_accuracy: 0.6463\n",
      "Epoch 122/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3334 - accuracy: 0.8358 - val_loss: 0.7456 - val_accuracy: 0.6517\n",
      "Epoch 123/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.3003 - accuracy: 0.8674 - val_loss: 0.8520 - val_accuracy: 0.6607\n",
      "Epoch 124/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2935 - accuracy: 0.8736 - val_loss: 0.9049 - val_accuracy: 0.6409\n",
      "Epoch 125/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.2728 - accuracy: 0.8813 - val_loss: 0.9159 - val_accuracy: 0.6409\n",
      "Epoch 126/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2628 - accuracy: 0.8805 - val_loss: 0.9145 - val_accuracy: 0.6373\n",
      "Epoch 127/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2610 - accuracy: 0.8890 - val_loss: 0.8690 - val_accuracy: 0.6571\n",
      "Epoch 128/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2620 - accuracy: 0.8843 - val_loss: 0.8782 - val_accuracy: 0.6571\n",
      "Epoch 129/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2462 - accuracy: 0.8913 - val_loss: 0.9131 - val_accuracy: 0.6571\n",
      "Epoch 130/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2302 - accuracy: 0.9052 - val_loss: 0.9133 - val_accuracy: 0.6643\n",
      "Epoch 131/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.2284 - accuracy: 0.8998 - val_loss: 0.9936 - val_accuracy: 0.6625\n",
      "Epoch 132/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2246 - accuracy: 0.9082 - val_loss: 1.0289 - val_accuracy: 0.6553\n",
      "Epoch 133/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2070 - accuracy: 0.9129 - val_loss: 1.0721 - val_accuracy: 0.6553\n",
      "Epoch 134/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2064 - accuracy: 0.9152 - val_loss: 1.0562 - val_accuracy: 0.6732\n",
      "Epoch 135/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.1948 - accuracy: 0.9229 - val_loss: 0.9783 - val_accuracy: 0.6607\n",
      "Epoch 136/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.2023 - accuracy: 0.9183 - val_loss: 0.9592 - val_accuracy: 0.6535\n",
      "Epoch 137/200\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_7 = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=200,\n",
    "                    batch_size=1000,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=10, kernel_size=10, strides=2, activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "model.add(layers.MaxPooling2D((10, 10)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=20, kernel_size=5, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((4, 4)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=40, kernel_size=3, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=80, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=160, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "# model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/500\n",
      "1297/1297 [==============================] - 6s 4ms/step - loss: 1.4068 - acc: 0.5104 - val_loss: 2.6783 - val_acc: 0.5566\n",
      "Epoch 2/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 2.3669 - acc: 0.5598 - val_loss: 4.1451 - val_acc: 0.4434\n",
      "Epoch 3/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 3.9325 - acc: 0.4765 - val_loss: 1.1120 - val_acc: 0.5566\n",
      "Epoch 4/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 2.2152 - acc: 0.5235 - val_loss: 3.6232 - val_acc: 0.4470\n",
      "Epoch 5/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 4.0168 - acc: 0.4680 - val_loss: 5.3214 - val_acc: 0.5566\n",
      "Epoch 6/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 5.4088 - acc: 0.5628 - val_loss: 5.1461 - val_acc: 0.5566\n",
      "Epoch 7/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 5.0393 - acc: 0.5628 - val_loss: 0.7293 - val_acc: 0.4524\n",
      "Epoch 8/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 1.1827 - acc: 0.4426 - val_loss: 2.7867 - val_acc: 0.5566\n",
      "Epoch 9/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 2.9748 - acc: 0.5628 - val_loss: 0.7460 - val_acc: 0.4434\n",
      "Epoch 10/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.9852 - acc: 0.4379 - val_loss: 1.6471 - val_acc: 0.5566\n",
      "Epoch 11/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 1.7169 - acc: 0.5628 - val_loss: 2.0572 - val_acc: 0.4434\n",
      "Epoch 12/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 1.9118 - acc: 0.4372 - val_loss: 2.8649 - val_acc: 0.5566\n",
      "Epoch 13/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 3.1762 - acc: 0.5628 - val_loss: 1.4843 - val_acc: 0.5566\n",
      "Epoch 14/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 1.8387 - acc: 0.5359 - val_loss: 3.1046 - val_acc: 0.4434\n",
      "Epoch 15/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 2.7422 - acc: 0.4395 - val_loss: 2.6071 - val_acc: 0.5566\n",
      "Epoch 16/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 2.9121 - acc: 0.5628 - val_loss: 2.5868 - val_acc: 0.5566\n",
      "Epoch 17/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 2.4278 - acc: 0.5628 - val_loss: 1.3141 - val_acc: 0.4614\n",
      "Epoch 18/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 1.5115 - acc: 0.4549 - val_loss: 1.2939 - val_acc: 0.4883\n",
      "Epoch 19/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 1.2444 - acc: 0.4834 - val_loss: 1.0482 - val_acc: 0.5566\n",
      "Epoch 20/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 1.1142 - acc: 0.5628 - val_loss: 0.8867 - val_acc: 0.5566\n",
      "Epoch 21/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.8549 - acc: 0.5628 - val_loss: 0.7439 - val_acc: 0.4919\n",
      "Epoch 22/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7850 - acc: 0.4842 - val_loss: 0.7941 - val_acc: 0.5009\n",
      "Epoch 23/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7963 - acc: 0.4880 - val_loss: 0.6623 - val_acc: 0.5637\n",
      "Epoch 24/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6730 - acc: 0.5713 - val_loss: 0.7060 - val_acc: 0.5566\n",
      "Epoch 25/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7149 - acc: 0.5628 - val_loss: 0.6956 - val_acc: 0.5566\n",
      "Epoch 26/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6990 - acc: 0.5628 - val_loss: 0.6699 - val_acc: 0.5566\n",
      "Epoch 27/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6705 - acc: 0.5659 - val_loss: 0.6617 - val_acc: 0.5601\n",
      "Epoch 28/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6641 - acc: 0.5867 - val_loss: 0.6639 - val_acc: 0.5817\n",
      "Epoch 29/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6676 - acc: 0.5675 - val_loss: 0.6628 - val_acc: 0.5619\n",
      "Epoch 30/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6681 - acc: 0.5428 - val_loss: 0.6594 - val_acc: 0.5835\n",
      "Epoch 31/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6639 - acc: 0.5590 - val_loss: 0.6547 - val_acc: 0.5853\n",
      "Epoch 32/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6577 - acc: 0.5744 - val_loss: 0.6522 - val_acc: 0.5871\n",
      "Epoch 33/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6534 - acc: 0.5844 - val_loss: 0.6527 - val_acc: 0.5907\n",
      "Epoch 34/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6526 - acc: 0.5929 - val_loss: 0.6535 - val_acc: 0.5907\n",
      "Epoch 35/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6527 - acc: 0.5960 - val_loss: 0.6527 - val_acc: 0.5925\n",
      "Epoch 36/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6514 - acc: 0.5937 - val_loss: 0.6509 - val_acc: 0.5961\n",
      "Epoch 37/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6494 - acc: 0.6037 - val_loss: 0.6504 - val_acc: 0.5978\n",
      "Epoch 38/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6485 - acc: 0.6014 - val_loss: 0.6501 - val_acc: 0.5996\n",
      "Epoch 39/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6477 - acc: 0.6029 - val_loss: 0.6499 - val_acc: 0.5961\n",
      "Epoch 40/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6468 - acc: 0.6037 - val_loss: 0.6502 - val_acc: 0.5925\n",
      "Epoch 41/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6463 - acc: 0.6122 - val_loss: 0.6497 - val_acc: 0.5925\n",
      "Epoch 42/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6453 - acc: 0.6106 - val_loss: 0.6493 - val_acc: 0.5835\n",
      "Epoch 43/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6439 - acc: 0.6076 - val_loss: 0.6495 - val_acc: 0.5871\n",
      "Epoch 44/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6439 - acc: 0.5998 - val_loss: 0.6487 - val_acc: 0.5853\n",
      "Epoch 45/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6424 - acc: 0.5975 - val_loss: 0.6478 - val_acc: 0.5889\n",
      "Epoch 46/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6410 - acc: 0.6091 - val_loss: 0.6483 - val_acc: 0.5925\n",
      "Epoch 47/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6404 - acc: 0.6153 - val_loss: 0.6481 - val_acc: 0.5978\n",
      "Epoch 48/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6388 - acc: 0.6176 - val_loss: 0.6480 - val_acc: 0.5978\n",
      "Epoch 49/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6377 - acc: 0.6099 - val_loss: 0.6469 - val_acc: 0.5961\n",
      "Epoch 50/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6355 - acc: 0.6168 - val_loss: 0.6463 - val_acc: 0.6050\n",
      "Epoch 51/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6351 - acc: 0.6168 - val_loss: 0.6479 - val_acc: 0.6050\n",
      "Epoch 52/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6371 - acc: 0.6199 - val_loss: 0.6433 - val_acc: 0.6104\n",
      "Epoch 53/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6318 - acc: 0.6253 - val_loss: 0.6440 - val_acc: 0.5925\n",
      "Epoch 54/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6321 - acc: 0.6160 - val_loss: 0.6454 - val_acc: 0.5871\n",
      "Epoch 55/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6311 - acc: 0.6184 - val_loss: 0.6452 - val_acc: 0.6158\n",
      "Epoch 56/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6273 - acc: 0.6284 - val_loss: 0.6447 - val_acc: 0.6122\n",
      "Epoch 57/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6267 - acc: 0.6268 - val_loss: 0.6407 - val_acc: 0.6050\n",
      "Epoch 58/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6246 - acc: 0.6338 - val_loss: 0.6412 - val_acc: 0.5943\n",
      "Epoch 59/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6239 - acc: 0.6276 - val_loss: 0.6429 - val_acc: 0.6086\n",
      "Epoch 60/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6209 - acc: 0.6338 - val_loss: 0.6455 - val_acc: 0.6122\n",
      "Epoch 61/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6221 - acc: 0.6361 - val_loss: 0.6444 - val_acc: 0.6266\n",
      "Epoch 62/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6179 - acc: 0.6423 - val_loss: 0.6460 - val_acc: 0.6086\n",
      "Epoch 63/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6189 - acc: 0.6423 - val_loss: 0.6369 - val_acc: 0.6068\n",
      "Epoch 64/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6132 - acc: 0.6399 - val_loss: 0.6454 - val_acc: 0.6212\n",
      "Epoch 65/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6183 - acc: 0.6399 - val_loss: 0.6470 - val_acc: 0.6176\n",
      "Epoch 66/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6122 - acc: 0.6530 - val_loss: 0.6525 - val_acc: 0.5889\n",
      "Epoch 67/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6141 - acc: 0.6430 - val_loss: 0.6392 - val_acc: 0.6176\n",
      "Epoch 68/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6053 - acc: 0.6484 - val_loss: 0.6419 - val_acc: 0.6068\n",
      "Epoch 69/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6104 - acc: 0.6507 - val_loss: 0.6399 - val_acc: 0.6176\n",
      "Epoch 70/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6048 - acc: 0.6469 - val_loss: 0.6420 - val_acc: 0.6122\n",
      "Epoch 71/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6023 - acc: 0.6530 - val_loss: 0.6445 - val_acc: 0.6230\n",
      "Epoch 72/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5968 - acc: 0.6538 - val_loss: 0.6445 - val_acc: 0.6176\n",
      "Epoch 73/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5966 - acc: 0.6662 - val_loss: 0.6434 - val_acc: 0.6050\n",
      "Epoch 74/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5897 - acc: 0.6662 - val_loss: 0.6469 - val_acc: 0.6104\n",
      "Epoch 75/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5830 - acc: 0.6669 - val_loss: 0.6508 - val_acc: 0.6158\n",
      "Epoch 76/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5808 - acc: 0.6723 - val_loss: 0.6545 - val_acc: 0.6050\n",
      "Epoch 77/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5767 - acc: 0.6708 - val_loss: 0.6559 - val_acc: 0.6032\n",
      "Epoch 78/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5716 - acc: 0.6854 - val_loss: 0.6540 - val_acc: 0.6266\n",
      "Epoch 79/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5673 - acc: 0.6823 - val_loss: 0.6736 - val_acc: 0.6086\n",
      "Epoch 80/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5802 - acc: 0.6654 - val_loss: 0.6735 - val_acc: 0.5781\n",
      "Epoch 81/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5760 - acc: 0.6338 - val_loss: 0.6785 - val_acc: 0.6230\n",
      "Epoch 82/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5737 - acc: 0.6769 - val_loss: 0.6814 - val_acc: 0.5727\n",
      "Epoch 83/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5809 - acc: 0.6353 - val_loss: 0.6658 - val_acc: 0.6284\n",
      "Epoch 84/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5593 - acc: 0.6931 - val_loss: 0.6628 - val_acc: 0.6266\n",
      "Epoch 85/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5525 - acc: 0.7008 - val_loss: 0.6792 - val_acc: 0.6050\n",
      "Epoch 86/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5530 - acc: 0.6847 - val_loss: 0.7058 - val_acc: 0.6194\n",
      "Epoch 87/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5711 - acc: 0.6700 - val_loss: 0.6791 - val_acc: 0.6194\n",
      "Epoch 88/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5427 - acc: 0.7178 - val_loss: 0.7113 - val_acc: 0.5907\n",
      "Epoch 89/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5597 - acc: 0.6800 - val_loss: 0.7060 - val_acc: 0.6050\n",
      "Epoch 90/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5668 - acc: 0.6700 - val_loss: 0.6798 - val_acc: 0.6248\n",
      "Epoch 91/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5402 - acc: 0.7055 - val_loss: 0.7029 - val_acc: 0.5996\n",
      "Epoch 92/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5380 - acc: 0.6993 - val_loss: 0.7137 - val_acc: 0.6104\n",
      "Epoch 93/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5466 - acc: 0.6978 - val_loss: 0.7123 - val_acc: 0.6158\n",
      "Epoch 94/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5259 - acc: 0.7170 - val_loss: 0.7353 - val_acc: 0.6068\n",
      "Epoch 95/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5326 - acc: 0.7078 - val_loss: 0.7312 - val_acc: 0.6122\n",
      "Epoch 96/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5253 - acc: 0.6978 - val_loss: 0.7114 - val_acc: 0.6230\n",
      "Epoch 97/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.5134 - acc: 0.7170 - val_loss: 0.7247 - val_acc: 0.6104\n",
      "Epoch 98/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.5094 - acc: 0.7217 - val_loss: 0.7329 - val_acc: 0.6032\n",
      "Epoch 99/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5132 - acc: 0.7147 - val_loss: 0.7257 - val_acc: 0.6050\n",
      "Epoch 100/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4979 - acc: 0.7325 - val_loss: 0.7390 - val_acc: 0.6014\n",
      "Epoch 101/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4965 - acc: 0.7448 - val_loss: 0.7766 - val_acc: 0.6230\n",
      "Epoch 102/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5110 - acc: 0.7170 - val_loss: 0.7520 - val_acc: 0.6248\n",
      "Epoch 103/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4842 - acc: 0.7402 - val_loss: 0.7496 - val_acc: 0.6086\n",
      "Epoch 104/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4732 - acc: 0.7487 - val_loss: 0.7242 - val_acc: 0.6212\n",
      "Epoch 105/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4769 - acc: 0.7425 - val_loss: 0.7717 - val_acc: 0.6050\n",
      "Epoch 106/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4683 - acc: 0.7456 - val_loss: 0.8192 - val_acc: 0.5709\n",
      "Epoch 107/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5005 - acc: 0.7147 - val_loss: 0.8247 - val_acc: 0.6068\n",
      "Epoch 108/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5111 - acc: 0.7217 - val_loss: 0.7278 - val_acc: 0.6445\n",
      "Epoch 109/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5033 - acc: 0.7394 - val_loss: 0.7211 - val_acc: 0.6248\n",
      "Epoch 110/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4715 - acc: 0.7564 - val_loss: 0.8731 - val_acc: 0.6212\n",
      "Epoch 111/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4840 - acc: 0.7217 - val_loss: 0.8236 - val_acc: 0.6176\n",
      "Epoch 112/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4751 - acc: 0.7525 - val_loss: 0.7828 - val_acc: 0.5781\n",
      "Epoch 113/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4850 - acc: 0.7548 - val_loss: 0.7932 - val_acc: 0.5996\n",
      "Epoch 114/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4618 - acc: 0.7540 - val_loss: 0.8560 - val_acc: 0.6104\n",
      "Epoch 115/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4519 - acc: 0.7571 - val_loss: 0.8951 - val_acc: 0.6194\n",
      "Epoch 116/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4460 - acc: 0.7803 - val_loss: 0.8472 - val_acc: 0.6014\n",
      "Epoch 117/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4293 - acc: 0.7803 - val_loss: 0.8918 - val_acc: 0.5925\n",
      "Epoch 118/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4175 - acc: 0.7718 - val_loss: 0.9176 - val_acc: 0.5799\n",
      "Epoch 119/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4076 - acc: 0.7887 - val_loss: 1.0410 - val_acc: 0.6068\n",
      "Epoch 120/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4210 - acc: 0.7810 - val_loss: 0.8998 - val_acc: 0.5996\n",
      "Epoch 121/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4110 - acc: 0.7772 - val_loss: 0.9359 - val_acc: 0.5978\n",
      "Epoch 122/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4061 - acc: 0.7826 - val_loss: 0.9435 - val_acc: 0.6140\n",
      "Epoch 123/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3836 - acc: 0.8088 - val_loss: 0.9378 - val_acc: 0.6032\n",
      "Epoch 124/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3770 - acc: 0.8034 - val_loss: 0.9820 - val_acc: 0.6176\n",
      "Epoch 125/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3710 - acc: 0.8080 - val_loss: 0.9514 - val_acc: 0.5978\n",
      "Epoch 126/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3683 - acc: 0.8019 - val_loss: 1.0221 - val_acc: 0.6086\n",
      "Epoch 127/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3635 - acc: 0.8096 - val_loss: 1.0549 - val_acc: 0.5853\n",
      "Epoch 128/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3535 - acc: 0.8134 - val_loss: 1.1039 - val_acc: 0.5996\n",
      "Epoch 129/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3568 - acc: 0.8173 - val_loss: 1.1517 - val_acc: 0.5709\n",
      "Epoch 130/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3498 - acc: 0.8157 - val_loss: 1.2533 - val_acc: 0.6032\n",
      "Epoch 131/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4140 - acc: 0.7895 - val_loss: 1.4661 - val_acc: 0.6212\n",
      "Epoch 132/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5859 - acc: 0.7417 - val_loss: 1.2100 - val_acc: 0.5673\n",
      "Epoch 133/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7077 - acc: 0.7101 - val_loss: 1.1499 - val_acc: 0.6355\n",
      "Epoch 134/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4593 - acc: 0.7733 - val_loss: 0.9649 - val_acc: 0.6212\n",
      "Epoch 135/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4037 - acc: 0.7918 - val_loss: 1.1363 - val_acc: 0.5835\n",
      "Epoch 136/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6959 - acc: 0.6970 - val_loss: 1.0349 - val_acc: 0.5440\n",
      "Epoch 137/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7021 - acc: 0.6369 - val_loss: 0.9293 - val_acc: 0.6140\n",
      "Epoch 138/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6817 - acc: 0.6723 - val_loss: 0.9016 - val_acc: 0.5961\n",
      "Epoch 139/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7059 - acc: 0.6153 - val_loss: 0.7049 - val_acc: 0.5512\n",
      "Epoch 140/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6317 - acc: 0.6091 - val_loss: 0.7130 - val_acc: 0.5242\n",
      "Epoch 141/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6576 - acc: 0.5212 - val_loss: 0.7305 - val_acc: 0.5548\n",
      "Epoch 142/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6651 - acc: 0.5613 - val_loss: 0.6927 - val_acc: 0.5386\n",
      "Epoch 143/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6420 - acc: 0.5744 - val_loss: 0.6821 - val_acc: 0.5314\n",
      "Epoch 144/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6443 - acc: 0.5497 - val_loss: 0.6755 - val_acc: 0.5512\n",
      "Epoch 145/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6441 - acc: 0.5605 - val_loss: 0.6745 - val_acc: 0.5530\n",
      "Epoch 146/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6423 - acc: 0.5659 - val_loss: 0.6753 - val_acc: 0.5494\n",
      "Epoch 147/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6420 - acc: 0.5790 - val_loss: 0.6748 - val_acc: 0.5583\n",
      "Epoch 148/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6407 - acc: 0.5636 - val_loss: 0.6766 - val_acc: 0.5673\n",
      "Epoch 149/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6330 - acc: 0.5682 - val_loss: 0.6896 - val_acc: 0.5224\n",
      "Epoch 150/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6475 - acc: 0.5397 - val_loss: 0.6790 - val_acc: 0.5943\n",
      "Epoch 151/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6367 - acc: 0.5767 - val_loss: 0.6785 - val_acc: 0.5799\n",
      "Epoch 152/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6373 - acc: 0.5736 - val_loss: 0.6629 - val_acc: 0.5853\n",
      "Epoch 153/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6287 - acc: 0.6145 - val_loss: 0.6604 - val_acc: 0.5907\n",
      "Epoch 154/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6282 - acc: 0.6214 - val_loss: 0.6599 - val_acc: 0.5943\n",
      "Epoch 155/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6261 - acc: 0.6299 - val_loss: 0.6620 - val_acc: 0.6086\n",
      "Epoch 156/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6235 - acc: 0.6307 - val_loss: 0.6644 - val_acc: 0.6122\n",
      "Epoch 157/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6213 - acc: 0.6338 - val_loss: 0.6626 - val_acc: 0.6140\n",
      "Epoch 158/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6184 - acc: 0.6361 - val_loss: 0.6561 - val_acc: 0.6122\n",
      "Epoch 159/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6146 - acc: 0.6369 - val_loss: 0.6531 - val_acc: 0.6068\n",
      "Epoch 160/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6125 - acc: 0.6322 - val_loss: 0.6538 - val_acc: 0.6122\n",
      "Epoch 161/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6099 - acc: 0.6369 - val_loss: 0.6575 - val_acc: 0.6122\n",
      "Epoch 162/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6065 - acc: 0.6322 - val_loss: 0.6616 - val_acc: 0.6050\n",
      "Epoch 163/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6054 - acc: 0.6345 - val_loss: 0.6654 - val_acc: 0.6068\n",
      "Epoch 164/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6041 - acc: 0.6384 - val_loss: 0.6638 - val_acc: 0.6086\n",
      "Epoch 165/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6008 - acc: 0.6361 - val_loss: 0.6595 - val_acc: 0.6140\n",
      "Epoch 166/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5998 - acc: 0.6446 - val_loss: 0.6620 - val_acc: 0.6140\n",
      "Epoch 167/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5961 - acc: 0.6407 - val_loss: 0.6661 - val_acc: 0.6068\n",
      "Epoch 168/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5930 - acc: 0.6430 - val_loss: 0.6686 - val_acc: 0.6032\n",
      "Epoch 169/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5908 - acc: 0.6430 - val_loss: 0.6679 - val_acc: 0.6086\n",
      "Epoch 170/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5866 - acc: 0.6523 - val_loss: 0.6704 - val_acc: 0.6140\n",
      "Epoch 171/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5873 - acc: 0.6500 - val_loss: 0.6788 - val_acc: 0.6014\n",
      "Epoch 172/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5829 - acc: 0.6500 - val_loss: 0.6826 - val_acc: 0.5996\n",
      "Epoch 173/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5812 - acc: 0.6446 - val_loss: 0.7036 - val_acc: 0.6086\n",
      "Epoch 174/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.5825 - acc: 0.6561 - val_loss: 0.6903 - val_acc: 0.5943\n",
      "Epoch 175/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.5832 - acc: 0.6392 - val_loss: 0.7038 - val_acc: 0.5925\n",
      "Epoch 176/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5739 - acc: 0.6615 - val_loss: 0.6953 - val_acc: 0.5889\n",
      "Epoch 177/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5767 - acc: 0.6322 - val_loss: 0.7336 - val_acc: 0.5961\n",
      "Epoch 178/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5858 - acc: 0.6615 - val_loss: 0.7130 - val_acc: 0.5458\n",
      "Epoch 179/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6018 - acc: 0.5898 - val_loss: 0.6886 - val_acc: 0.5763\n",
      "Epoch 180/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5747 - acc: 0.6230 - val_loss: 0.7211 - val_acc: 0.5817\n",
      "Epoch 181/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6054 - acc: 0.6461 - val_loss: 0.7141 - val_acc: 0.5799\n",
      "Epoch 182/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5925 - acc: 0.6538 - val_loss: 0.6957 - val_acc: 0.5996\n",
      "Epoch 183/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5755 - acc: 0.6484 - val_loss: 0.7055 - val_acc: 0.5978\n",
      "Epoch 184/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5804 - acc: 0.6168 - val_loss: 0.7093 - val_acc: 0.5943\n",
      "Epoch 185/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5725 - acc: 0.6191 - val_loss: 0.7025 - val_acc: 0.5996\n",
      "Epoch 186/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5597 - acc: 0.6561 - val_loss: 0.7079 - val_acc: 0.5978\n",
      "Epoch 187/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5556 - acc: 0.6870 - val_loss: 0.7158 - val_acc: 0.5817\n",
      "Epoch 188/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5456 - acc: 0.6862 - val_loss: 0.7283 - val_acc: 0.5763\n",
      "Epoch 189/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5353 - acc: 0.7001 - val_loss: 0.7548 - val_acc: 0.6158\n",
      "Epoch 190/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5287 - acc: 0.7124 - val_loss: 0.7478 - val_acc: 0.5943\n",
      "Epoch 191/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5277 - acc: 0.6978 - val_loss: 0.9134 - val_acc: 0.5548\n",
      "Epoch 192/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5762 - acc: 0.6746 - val_loss: 0.7654 - val_acc: 0.5619\n",
      "Epoch 193/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5171 - acc: 0.7016 - val_loss: 0.7565 - val_acc: 0.5889\n",
      "Epoch 194/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5149 - acc: 0.6978 - val_loss: 0.7836 - val_acc: 0.5871\n",
      "Epoch 195/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5091 - acc: 0.6993 - val_loss: 0.8082 - val_acc: 0.5566\n",
      "Epoch 196/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5030 - acc: 0.7008 - val_loss: 0.8296 - val_acc: 0.5781\n",
      "Epoch 197/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4984 - acc: 0.7055 - val_loss: 0.8072 - val_acc: 0.5961\n",
      "Epoch 198/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4734 - acc: 0.7278 - val_loss: 0.8112 - val_acc: 0.6032\n",
      "Epoch 199/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4568 - acc: 0.7417 - val_loss: 0.8420 - val_acc: 0.6014\n",
      "Epoch 200/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4516 - acc: 0.7409 - val_loss: 0.8669 - val_acc: 0.6122\n",
      "Epoch 201/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4276 - acc: 0.7625 - val_loss: 0.8557 - val_acc: 0.6230\n",
      "Epoch 202/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4209 - acc: 0.7710 - val_loss: 0.8763 - val_acc: 0.6212\n",
      "Epoch 203/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4072 - acc: 0.7749 - val_loss: 0.9405 - val_acc: 0.5961\n",
      "Epoch 204/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3921 - acc: 0.7903 - val_loss: 1.0590 - val_acc: 0.5925\n",
      "Epoch 205/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3921 - acc: 0.7803 - val_loss: 1.0042 - val_acc: 0.6176\n",
      "Epoch 206/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3692 - acc: 0.7972 - val_loss: 1.0157 - val_acc: 0.6194\n",
      "Epoch 207/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3601 - acc: 0.8096 - val_loss: 1.1449 - val_acc: 0.6104\n",
      "Epoch 208/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3874 - acc: 0.7903 - val_loss: 1.0992 - val_acc: 0.5799\n",
      "Epoch 209/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3463 - acc: 0.8126 - val_loss: 1.1139 - val_acc: 0.5889\n",
      "Epoch 210/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3205 - acc: 0.8450 - val_loss: 1.1322 - val_acc: 0.6050\n",
      "Epoch 211/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3269 - acc: 0.8427 - val_loss: 1.2262 - val_acc: 0.5853\n",
      "Epoch 212/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3504 - acc: 0.8281 - val_loss: 1.2381 - val_acc: 0.5925\n",
      "Epoch 213/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2948 - acc: 0.8574 - val_loss: 1.3402 - val_acc: 0.5871\n",
      "Epoch 214/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3332 - acc: 0.8265 - val_loss: 1.3612 - val_acc: 0.5817\n",
      "Epoch 215/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3291 - acc: 0.8373 - val_loss: 1.3514 - val_acc: 0.6122\n",
      "Epoch 216/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2992 - acc: 0.8558 - val_loss: 1.8753 - val_acc: 0.5512\n",
      "Epoch 217/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6753 - acc: 0.7610 - val_loss: 1.5914 - val_acc: 0.5763\n",
      "Epoch 218/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5485 - acc: 0.7579 - val_loss: 1.4053 - val_acc: 0.4955\n",
      "Epoch 219/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6284 - acc: 0.6538 - val_loss: 1.0941 - val_acc: 0.5350\n",
      "Epoch 220/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6100 - acc: 0.6446 - val_loss: 1.0905 - val_acc: 0.5386\n",
      "Epoch 221/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5407 - acc: 0.6793 - val_loss: 0.9328 - val_acc: 0.5727\n",
      "Epoch 222/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5430 - acc: 0.7155 - val_loss: 0.8167 - val_acc: 0.5925\n",
      "Epoch 223/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4770 - acc: 0.7409 - val_loss: 0.9914 - val_acc: 0.6086\n",
      "Epoch 224/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4626 - acc: 0.7679 - val_loss: 0.8461 - val_acc: 0.5925\n",
      "Epoch 225/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4182 - acc: 0.7934 - val_loss: 0.8694 - val_acc: 0.5907\n",
      "Epoch 226/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4261 - acc: 0.7833 - val_loss: 0.8688 - val_acc: 0.5871\n",
      "Epoch 227/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3985 - acc: 0.8026 - val_loss: 0.9667 - val_acc: 0.5655\n",
      "Epoch 228/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3662 - acc: 0.8211 - val_loss: 0.9760 - val_acc: 0.5673\n",
      "Epoch 229/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3556 - acc: 0.8165 - val_loss: 0.9953 - val_acc: 0.5781\n",
      "Epoch 230/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3140 - acc: 0.8489 - val_loss: 1.1470 - val_acc: 0.5781\n",
      "Epoch 231/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3244 - acc: 0.8350 - val_loss: 1.1520 - val_acc: 0.5745\n",
      "Epoch 232/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2886 - acc: 0.8512 - val_loss: 1.2570 - val_acc: 0.5745\n",
      "Epoch 233/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2706 - acc: 0.8643 - val_loss: 1.2350 - val_acc: 0.5871\n",
      "Epoch 234/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2535 - acc: 0.8790 - val_loss: 1.2689 - val_acc: 0.5996\n",
      "Epoch 235/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2471 - acc: 0.8843 - val_loss: 1.2006 - val_acc: 0.6212\n",
      "Epoch 236/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2344 - acc: 0.8836 - val_loss: 1.3181 - val_acc: 0.5943\n",
      "Epoch 237/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2120 - acc: 0.9036 - val_loss: 1.5752 - val_acc: 0.5925\n",
      "Epoch 238/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2639 - acc: 0.8697 - val_loss: 1.2269 - val_acc: 0.6014\n",
      "Epoch 239/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2309 - acc: 0.8936 - val_loss: 1.4679 - val_acc: 0.5961\n",
      "Epoch 240/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2866 - acc: 0.8612 - val_loss: 1.7126 - val_acc: 0.5583\n",
      "Epoch 241/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2241 - acc: 0.8959 - val_loss: 1.6355 - val_acc: 0.5691\n",
      "Epoch 242/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2067 - acc: 0.8998 - val_loss: 1.4646 - val_acc: 0.5943\n",
      "Epoch 243/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2211 - acc: 0.9052 - val_loss: 1.5364 - val_acc: 0.5996\n",
      "Epoch 244/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2497 - acc: 0.8790 - val_loss: 1.6253 - val_acc: 0.5709\n",
      "Epoch 245/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2116 - acc: 0.8967 - val_loss: 1.7757 - val_acc: 0.5817\n",
      "Epoch 246/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1962 - acc: 0.9052 - val_loss: 1.7159 - val_acc: 0.5673\n",
      "Epoch 247/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2486 - acc: 0.8658 - val_loss: 1.9656 - val_acc: 0.5853\n",
      "Epoch 248/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1831 - acc: 0.9152 - val_loss: 1.6112 - val_acc: 0.6104\n",
      "Epoch 249/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1845 - acc: 0.9075 - val_loss: 1.6263 - val_acc: 0.5978\n",
      "Epoch 250/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1877 - acc: 0.9067 - val_loss: 1.8985 - val_acc: 0.5727\n",
      "Epoch 251/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.2856 - acc: 0.8743 - val_loss: 2.0303 - val_acc: 0.5961\n",
      "Epoch 252/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.2530 - acc: 0.8620 - val_loss: 2.0256 - val_acc: 0.5745\n",
      "Epoch 253/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2398 - acc: 0.8897 - val_loss: 1.6587 - val_acc: 0.5978\n",
      "Epoch 254/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2255 - acc: 0.8990 - val_loss: 1.5017 - val_acc: 0.6050\n",
      "Epoch 255/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1968 - acc: 0.9252 - val_loss: 1.2476 - val_acc: 0.6158\n",
      "Epoch 256/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1729 - acc: 0.9237 - val_loss: 1.2484 - val_acc: 0.6032\n",
      "Epoch 257/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1696 - acc: 0.9298 - val_loss: 1.3393 - val_acc: 0.6086\n",
      "Epoch 258/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1634 - acc: 0.9352 - val_loss: 1.4527 - val_acc: 0.6122\n",
      "Epoch 259/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1381 - acc: 0.9437 - val_loss: 1.5620 - val_acc: 0.6068\n",
      "Epoch 260/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1258 - acc: 0.9437 - val_loss: 1.8566 - val_acc: 0.5907\n",
      "Epoch 261/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1652 - acc: 0.9214 - val_loss: 1.8804 - val_acc: 0.5996\n",
      "Epoch 262/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1350 - acc: 0.9352 - val_loss: 1.9543 - val_acc: 0.5871\n",
      "Epoch 263/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1319 - acc: 0.9383 - val_loss: 1.9519 - val_acc: 0.5907\n",
      "Epoch 264/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1260 - acc: 0.9414 - val_loss: 2.0502 - val_acc: 0.5889\n",
      "Epoch 265/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1135 - acc: 0.9476 - val_loss: 2.1904 - val_acc: 0.5925\n",
      "Epoch 266/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0965 - acc: 0.9599 - val_loss: 2.2569 - val_acc: 0.6176\n",
      "Epoch 267/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1001 - acc: 0.9561 - val_loss: 2.1491 - val_acc: 0.6284\n",
      "Epoch 268/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0816 - acc: 0.9707 - val_loss: 2.1622 - val_acc: 0.6086\n",
      "Epoch 269/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1019 - acc: 0.9591 - val_loss: 2.2082 - val_acc: 0.6068\n",
      "Epoch 270/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1012 - acc: 0.9514 - val_loss: 2.4045 - val_acc: 0.5889\n",
      "Epoch 271/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0759 - acc: 0.9715 - val_loss: 2.7246 - val_acc: 0.5835\n",
      "Epoch 272/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1084 - acc: 0.9514 - val_loss: 2.2899 - val_acc: 0.5871\n",
      "Epoch 273/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1435 - acc: 0.9268 - val_loss: 2.2978 - val_acc: 0.6176\n",
      "Epoch 274/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0821 - acc: 0.9676 - val_loss: 2.2112 - val_acc: 0.6158\n",
      "Epoch 275/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0743 - acc: 0.9661 - val_loss: 2.1866 - val_acc: 0.6068\n",
      "Epoch 276/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0738 - acc: 0.9676 - val_loss: 2.2304 - val_acc: 0.6104\n",
      "Epoch 277/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0727 - acc: 0.9753 - val_loss: 2.2746 - val_acc: 0.6068\n",
      "Epoch 278/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1279 - acc: 0.9453 - val_loss: 2.2676 - val_acc: 0.5943\n",
      "Epoch 279/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2467 - acc: 0.8851 - val_loss: 2.3202 - val_acc: 0.5871\n",
      "Epoch 280/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1440 - acc: 0.9345 - val_loss: 2.5481 - val_acc: 0.5709\n",
      "Epoch 281/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1948 - acc: 0.9268 - val_loss: 2.7063 - val_acc: 0.5314\n",
      "Epoch 282/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3188 - acc: 0.9052 - val_loss: 1.9840 - val_acc: 0.6014\n",
      "Epoch 283/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2608 - acc: 0.9082 - val_loss: 2.2334 - val_acc: 0.5799\n",
      "Epoch 284/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2580 - acc: 0.8936 - val_loss: 2.0817 - val_acc: 0.5835\n",
      "Epoch 285/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2375 - acc: 0.9059 - val_loss: 1.9796 - val_acc: 0.5889\n",
      "Epoch 286/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2016 - acc: 0.9183 - val_loss: 1.8102 - val_acc: 0.6014\n",
      "Epoch 287/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1673 - acc: 0.9383 - val_loss: 1.7205 - val_acc: 0.6086\n",
      "Epoch 288/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2733 - acc: 0.8450 - val_loss: 1.8164 - val_acc: 0.5835\n",
      "Epoch 289/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1677 - acc: 0.9329 - val_loss: 1.6593 - val_acc: 0.5925\n",
      "Epoch 290/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1795 - acc: 0.9237 - val_loss: 1.6538 - val_acc: 0.5871\n",
      "Epoch 291/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1950 - acc: 0.9237 - val_loss: 1.5867 - val_acc: 0.5781\n",
      "Epoch 292/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1458 - acc: 0.9453 - val_loss: 1.8636 - val_acc: 0.5548\n",
      "Epoch 293/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1892 - acc: 0.9291 - val_loss: 1.6537 - val_acc: 0.5978\n",
      "Epoch 294/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1341 - acc: 0.9630 - val_loss: 1.7975 - val_acc: 0.6032\n",
      "Epoch 295/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0956 - acc: 0.9645 - val_loss: 2.1801 - val_acc: 0.5583\n",
      "Epoch 296/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1558 - acc: 0.9445 - val_loss: 1.8849 - val_acc: 0.5871\n",
      "Epoch 297/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1157 - acc: 0.9514 - val_loss: 1.9011 - val_acc: 0.6014\n",
      "Epoch 298/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1133 - acc: 0.9553 - val_loss: 1.9452 - val_acc: 0.6050\n",
      "Epoch 299/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0945 - acc: 0.9622 - val_loss: 1.9635 - val_acc: 0.6266\n",
      "Epoch 300/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0739 - acc: 0.9715 - val_loss: 2.1343 - val_acc: 0.5961\n",
      "Epoch 301/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0791 - acc: 0.9692 - val_loss: 2.2256 - val_acc: 0.6230\n",
      "Epoch 302/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0589 - acc: 0.9838 - val_loss: 2.2750 - val_acc: 0.6050\n",
      "Epoch 303/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0603 - acc: 0.9792 - val_loss: 2.3447 - val_acc: 0.5996\n",
      "Epoch 304/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0470 - acc: 0.9861 - val_loss: 2.3597 - val_acc: 0.6086\n",
      "Epoch 305/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0508 - acc: 0.9846 - val_loss: 2.4378 - val_acc: 0.6032\n",
      "Epoch 306/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0423 - acc: 0.9892 - val_loss: 2.6108 - val_acc: 0.5996\n",
      "Epoch 307/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0427 - acc: 0.9854 - val_loss: 2.6501 - val_acc: 0.6032\n",
      "Epoch 308/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0338 - acc: 0.9900 - val_loss: 2.6023 - val_acc: 0.6032\n",
      "Epoch 309/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0350 - acc: 0.9900 - val_loss: 2.6759 - val_acc: 0.6086\n",
      "Epoch 310/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0317 - acc: 0.9907 - val_loss: 2.7611 - val_acc: 0.6104\n",
      "Epoch 311/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0270 - acc: 0.9931 - val_loss: 2.8271 - val_acc: 0.5978\n",
      "Epoch 312/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0244 - acc: 0.9923 - val_loss: 2.8969 - val_acc: 0.6014\n",
      "Epoch 313/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0230 - acc: 0.9938 - val_loss: 2.9543 - val_acc: 0.6032\n",
      "Epoch 314/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0210 - acc: 0.9938 - val_loss: 3.0822 - val_acc: 0.6014\n",
      "Epoch 315/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0185 - acc: 0.9931 - val_loss: 3.1822 - val_acc: 0.6068\n",
      "Epoch 316/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0240 - acc: 0.9900 - val_loss: 3.0804 - val_acc: 0.6032\n",
      "Epoch 317/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0317 - acc: 0.9854 - val_loss: 3.0910 - val_acc: 0.6032\n",
      "Epoch 318/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0517 - acc: 0.9746 - val_loss: 3.2433 - val_acc: 0.5996\n",
      "Epoch 319/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0204 - acc: 0.9877 - val_loss: 3.4246 - val_acc: 0.5961\n",
      "Epoch 320/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0223 - acc: 0.9892 - val_loss: 3.2271 - val_acc: 0.6104\n",
      "Epoch 321/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0179 - acc: 0.9938 - val_loss: 3.1102 - val_acc: 0.6212\n",
      "Epoch 322/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0253 - acc: 0.9915 - val_loss: 3.1555 - val_acc: 0.6248\n",
      "Epoch 323/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0144 - acc: 0.9961 - val_loss: 3.3225 - val_acc: 0.6104\n",
      "Epoch 324/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0187 - acc: 0.9938 - val_loss: 3.3463 - val_acc: 0.6068\n",
      "Epoch 325/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0117 - acc: 0.9969 - val_loss: 3.2908 - val_acc: 0.6032\n",
      "Epoch 326/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0120 - acc: 0.9954 - val_loss: 3.2814 - val_acc: 0.6014\n",
      "Epoch 327/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0125 - acc: 0.9954 - val_loss: 3.3649 - val_acc: 0.5996\n",
      "Epoch 328/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0108 - acc: 0.9954 - val_loss: 3.4647 - val_acc: 0.5996\n",
      "Epoch 329/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0113 - acc: 0.9954 - val_loss: 3.4077 - val_acc: 0.6104\n",
      "Epoch 330/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0088 - acc: 0.9961 - val_loss: 3.2976 - val_acc: 0.6194\n",
      "Epoch 331/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0135 - acc: 0.9954 - val_loss: 3.4759 - val_acc: 0.6158\n",
      "Epoch 332/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0100 - acc: 0.9969 - val_loss: 3.5038 - val_acc: 0.6212\n",
      "Epoch 333/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0077 - acc: 0.9985 - val_loss: 3.4340 - val_acc: 0.6122\n",
      "Epoch 334/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0104 - acc: 0.9969 - val_loss: 3.4776 - val_acc: 0.6068\n",
      "Epoch 335/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0094 - acc: 0.9977 - val_loss: 3.6156 - val_acc: 0.6140\n",
      "Epoch 336/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0080 - acc: 0.9977 - val_loss: 3.7258 - val_acc: 0.6122\n",
      "Epoch 337/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0085 - acc: 0.9985 - val_loss: 3.7417 - val_acc: 0.6104\n",
      "Epoch 338/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0066 - acc: 0.9985 - val_loss: 3.6720 - val_acc: 0.6068\n",
      "Epoch 339/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0069 - acc: 0.9977 - val_loss: 3.6802 - val_acc: 0.6104\n",
      "Epoch 340/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0074 - acc: 0.9985 - val_loss: 3.8385 - val_acc: 0.6068\n",
      "Epoch 341/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0046 - acc: 0.9992 - val_loss: 3.9744 - val_acc: 0.5978\n",
      "Epoch 342/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0139 - acc: 0.9938 - val_loss: 3.7051 - val_acc: 0.6014\n",
      "Epoch 343/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0515 - acc: 0.9823 - val_loss: 3.7857 - val_acc: 0.6104\n",
      "Epoch 344/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0115 - acc: 0.9961 - val_loss: 4.2445 - val_acc: 0.5943\n",
      "Epoch 345/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3005 - acc: 0.9283 - val_loss: 2.9735 - val_acc: 0.5853\n",
      "Epoch 346/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6573 - acc: 0.7934 - val_loss: 2.5562 - val_acc: 0.5583\n",
      "Epoch 347/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6329 - acc: 0.7641 - val_loss: 2.3579 - val_acc: 0.5709\n",
      "Epoch 348/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4604 - acc: 0.7749 - val_loss: 2.0105 - val_acc: 0.5853\n",
      "Epoch 349/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3844 - acc: 0.7988 - val_loss: 1.7432 - val_acc: 0.5889\n",
      "Epoch 350/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3429 - acc: 0.8134 - val_loss: 1.5477 - val_acc: 0.6194\n",
      "Epoch 351/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3373 - acc: 0.8072 - val_loss: 1.5197 - val_acc: 0.6032\n",
      "Epoch 352/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2976 - acc: 0.8281 - val_loss: 1.7213 - val_acc: 0.5943\n",
      "Epoch 353/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3168 - acc: 0.8311 - val_loss: 1.5637 - val_acc: 0.5889\n",
      "Epoch 354/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2799 - acc: 0.8265 - val_loss: 1.3834 - val_acc: 0.6014\n",
      "Epoch 355/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2562 - acc: 0.8520 - val_loss: 1.3930 - val_acc: 0.5961\n",
      "Epoch 356/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2512 - acc: 0.8497 - val_loss: 1.4278 - val_acc: 0.5925\n",
      "Epoch 357/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2235 - acc: 0.8820 - val_loss: 1.6903 - val_acc: 0.5691\n",
      "Epoch 358/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1726 - acc: 0.9237 - val_loss: 1.9540 - val_acc: 0.5835\n",
      "Epoch 359/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1894 - acc: 0.9275 - val_loss: 1.6859 - val_acc: 0.5996\n",
      "Epoch 360/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1145 - acc: 0.9607 - val_loss: 1.5604 - val_acc: 0.5978\n",
      "Epoch 361/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1035 - acc: 0.9607 - val_loss: 1.5255 - val_acc: 0.5943\n",
      "Epoch 362/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0976 - acc: 0.9638 - val_loss: 1.5975 - val_acc: 0.6014\n",
      "Epoch 363/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0922 - acc: 0.9668 - val_loss: 1.8101 - val_acc: 0.5943\n",
      "Epoch 364/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0739 - acc: 0.9776 - val_loss: 2.0093 - val_acc: 0.5853\n",
      "Epoch 365/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0559 - acc: 0.9815 - val_loss: 2.0945 - val_acc: 0.5978\n",
      "Epoch 366/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0494 - acc: 0.9823 - val_loss: 2.1535 - val_acc: 0.5889\n",
      "Epoch 367/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0385 - acc: 0.9884 - val_loss: 2.2433 - val_acc: 0.5961\n",
      "Epoch 368/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0351 - acc: 0.9892 - val_loss: 2.3344 - val_acc: 0.5853\n",
      "Epoch 369/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0335 - acc: 0.9861 - val_loss: 2.3927 - val_acc: 0.5853\n",
      "Epoch 370/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0279 - acc: 0.9877 - val_loss: 2.4016 - val_acc: 0.6032\n",
      "Epoch 371/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0262 - acc: 0.9892 - val_loss: 2.4796 - val_acc: 0.6104\n",
      "Epoch 372/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0237 - acc: 0.9900 - val_loss: 2.6138 - val_acc: 0.6032\n",
      "Epoch 373/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0204 - acc: 0.9907 - val_loss: 2.6824 - val_acc: 0.5978\n",
      "Epoch 374/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0183 - acc: 0.9931 - val_loss: 2.7412 - val_acc: 0.6032\n",
      "Epoch 375/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0177 - acc: 0.9977 - val_loss: 2.8880 - val_acc: 0.6086\n",
      "Epoch 376/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0163 - acc: 0.9969 - val_loss: 2.9684 - val_acc: 0.6050\n",
      "Epoch 377/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0150 - acc: 0.9977 - val_loss: 3.0030 - val_acc: 0.6086\n",
      "Epoch 378/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0133 - acc: 0.9985 - val_loss: 3.0928 - val_acc: 0.6068\n",
      "Epoch 379/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0122 - acc: 0.9977 - val_loss: 3.1474 - val_acc: 0.6104\n",
      "Epoch 380/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0112 - acc: 0.9977 - val_loss: 3.1761 - val_acc: 0.6014\n",
      "Epoch 381/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0103 - acc: 0.9985 - val_loss: 3.2220 - val_acc: 0.6050\n",
      "Epoch 382/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0095 - acc: 0.9992 - val_loss: 3.2761 - val_acc: 0.6086\n",
      "Epoch 383/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0088 - acc: 0.9992 - val_loss: 3.3192 - val_acc: 0.6104\n",
      "Epoch 384/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0081 - acc: 0.9992 - val_loss: 3.3514 - val_acc: 0.6068\n",
      "Epoch 385/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0076 - acc: 0.9992 - val_loss: 3.3829 - val_acc: 0.6032\n",
      "Epoch 386/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0072 - acc: 0.9992 - val_loss: 3.4251 - val_acc: 0.6032\n",
      "Epoch 387/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0066 - acc: 0.9992 - val_loss: 3.4630 - val_acc: 0.6014\n",
      "Epoch 388/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0062 - acc: 0.9992 - val_loss: 3.5058 - val_acc: 0.6050\n",
      "Epoch 389/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0059 - acc: 0.9992 - val_loss: 3.5484 - val_acc: 0.6068\n",
      "Epoch 390/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0055 - acc: 0.9992 - val_loss: 3.5861 - val_acc: 0.6032\n",
      "Epoch 391/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0052 - acc: 0.9992 - val_loss: 3.6250 - val_acc: 0.6050\n",
      "Epoch 392/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0050 - acc: 0.9992 - val_loss: 3.6671 - val_acc: 0.6050\n",
      "Epoch 393/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0047 - acc: 0.9992 - val_loss: 3.7076 - val_acc: 0.6050\n",
      "Epoch 394/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0045 - acc: 0.9992 - val_loss: 3.7331 - val_acc: 0.6050\n",
      "Epoch 395/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0043 - acc: 0.9992 - val_loss: 3.7574 - val_acc: 0.6068\n",
      "Epoch 396/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0042 - acc: 0.9992 - val_loss: 3.7966 - val_acc: 0.6050\n",
      "Epoch 397/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.9992 - val_loss: 3.8403 - val_acc: 0.6014\n",
      "Epoch 398/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.9992 - val_loss: 3.8760 - val_acc: 0.6032\n",
      "Epoch 399/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0038 - acc: 0.9992 - val_loss: 3.8884 - val_acc: 0.6032\n",
      "Epoch 400/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0036 - acc: 0.9992 - val_loss: 3.8972 - val_acc: 0.5978\n",
      "Epoch 401/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.9992 - val_loss: 3.9249 - val_acc: 0.5996\n",
      "Epoch 402/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0038 - acc: 0.9992 - val_loss: 3.9684 - val_acc: 0.5996\n",
      "Epoch 403/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.9992 - val_loss: 4.0208 - val_acc: 0.5996\n",
      "Epoch 404/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.9992 - val_loss: 4.0406 - val_acc: 0.5961\n",
      "Epoch 405/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0032 - acc: 0.9992 - val_loss: 4.0470 - val_acc: 0.5978\n",
      "Epoch 406/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 4.0743 - val_acc: 0.5961\n",
      "Epoch 407/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0029 - acc: 0.9992 - val_loss: 4.1090 - val_acc: 0.5996\n",
      "Epoch 408/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0029 - acc: 0.9992 - val_loss: 4.1133 - val_acc: 0.5996\n",
      "Epoch 409/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0027 - acc: 0.9992 - val_loss: 4.1097 - val_acc: 0.6014\n",
      "Epoch 410/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 4.1202 - val_acc: 0.5978\n",
      "Epoch 411/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 4.1571 - val_acc: 0.5996\n",
      "Epoch 412/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0026 - acc: 0.9992 - val_loss: 4.2332 - val_acc: 0.6068\n",
      "Epoch 413/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0054 - acc: 0.9977 - val_loss: 3.8941 - val_acc: 0.5871\n",
      "Epoch 414/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0242 - acc: 0.9892 - val_loss: 4.0647 - val_acc: 0.6050\n",
      "Epoch 415/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0063 - acc: 0.9985 - val_loss: 4.2368 - val_acc: 0.5907\n",
      "Epoch 416/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0111 - acc: 0.9954 - val_loss: 4.1286 - val_acc: 0.6014\n",
      "Epoch 417/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0052 - acc: 0.9985 - val_loss: 3.9992 - val_acc: 0.5907\n",
      "Epoch 418/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0119 - acc: 0.9961 - val_loss: 4.4288 - val_acc: 0.6032\n",
      "Epoch 419/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0146 - acc: 0.9931 - val_loss: 3.9313 - val_acc: 0.5996\n",
      "Epoch 420/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0525 - acc: 0.9800 - val_loss: 3.7939 - val_acc: 0.5943\n",
      "Epoch 421/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1008 - acc: 0.9653 - val_loss: 4.5643 - val_acc: 0.5637\n",
      "Epoch 422/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1617 - acc: 0.9453 - val_loss: 3.9273 - val_acc: 0.5601\n",
      "Epoch 423/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.8617 - acc: 0.8404 - val_loss: 3.7575 - val_acc: 0.5548\n",
      "Epoch 424/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4159 - acc: 0.8905 - val_loss: 4.4385 - val_acc: 0.5781\n",
      "Epoch 425/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 1.6662 - acc: 0.7556 - val_loss: 2.2351 - val_acc: 0.5422\n",
      "Epoch 426/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7188 - acc: 0.7194 - val_loss: 1.7801 - val_acc: 0.5350\n",
      "Epoch 427/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6730 - acc: 0.7548 - val_loss: 1.5294 - val_acc: 0.5943\n",
      "Epoch 428/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5532 - acc: 0.7756 - val_loss: 1.5260 - val_acc: 0.5655\n",
      "Epoch 429/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5272 - acc: 0.7402 - val_loss: 1.5659 - val_acc: 0.5548\n",
      "Epoch 430/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4550 - acc: 0.7610 - val_loss: 1.3265 - val_acc: 0.6140\n",
      "Epoch 431/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4235 - acc: 0.8019 - val_loss: 1.2675 - val_acc: 0.6158\n",
      "Epoch 432/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4082 - acc: 0.8188 - val_loss: 1.3259 - val_acc: 0.5601\n",
      "Epoch 433/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3751 - acc: 0.7918 - val_loss: 1.3332 - val_acc: 0.5601\n",
      "Epoch 434/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3554 - acc: 0.8042 - val_loss: 1.4205 - val_acc: 0.5404\n",
      "Epoch 435/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3499 - acc: 0.8088 - val_loss: 1.3010 - val_acc: 0.5835\n",
      "Epoch 436/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3144 - acc: 0.8327 - val_loss: 1.2957 - val_acc: 0.5745\n",
      "Epoch 437/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3240 - acc: 0.8520 - val_loss: 1.2388 - val_acc: 0.6122\n",
      "Epoch 438/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.3283 - acc: 0.8481 - val_loss: 1.3343 - val_acc: 0.5925\n",
      "Epoch 439/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2838 - acc: 0.8620 - val_loss: 1.4355 - val_acc: 0.5368\n",
      "Epoch 440/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2832 - acc: 0.8458 - val_loss: 1.5231 - val_acc: 0.5278\n",
      "Epoch 441/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2680 - acc: 0.8512 - val_loss: 1.5204 - val_acc: 0.5619\n",
      "Epoch 442/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2483 - acc: 0.8666 - val_loss: 1.3947 - val_acc: 0.5943\n",
      "Epoch 443/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2319 - acc: 0.8782 - val_loss: 1.4662 - val_acc: 0.5925\n",
      "Epoch 444/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2337 - acc: 0.8843 - val_loss: 1.4099 - val_acc: 0.6230\n",
      "Epoch 445/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2084 - acc: 0.8921 - val_loss: 1.5132 - val_acc: 0.6032\n",
      "Epoch 446/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2093 - acc: 0.9013 - val_loss: 1.5189 - val_acc: 0.6140\n",
      "Epoch 447/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2047 - acc: 0.8975 - val_loss: 1.4717 - val_acc: 0.5925\n",
      "Epoch 448/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1800 - acc: 0.9129 - val_loss: 1.4180 - val_acc: 0.6248\n",
      "Epoch 449/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2041 - acc: 0.9036 - val_loss: 1.5289 - val_acc: 0.5853\n",
      "Epoch 450/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1911 - acc: 0.9044 - val_loss: 1.5572 - val_acc: 0.6050\n",
      "Epoch 451/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1837 - acc: 0.9082 - val_loss: 1.5898 - val_acc: 0.5853\n",
      "Epoch 452/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1526 - acc: 0.9352 - val_loss: 1.6194 - val_acc: 0.5943\n",
      "Epoch 453/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1439 - acc: 0.9414 - val_loss: 1.6425 - val_acc: 0.5889\n",
      "Epoch 454/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1349 - acc: 0.9460 - val_loss: 1.7019 - val_acc: 0.6158\n",
      "Epoch 455/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1317 - acc: 0.9460 - val_loss: 1.7582 - val_acc: 0.5925\n",
      "Epoch 456/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1246 - acc: 0.9476 - val_loss: 1.8364 - val_acc: 0.6176\n",
      "Epoch 457/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1195 - acc: 0.9507 - val_loss: 1.8826 - val_acc: 0.6086\n",
      "Epoch 458/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1113 - acc: 0.9553 - val_loss: 1.9992 - val_acc: 0.6302\n",
      "Epoch 459/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1222 - acc: 0.9445 - val_loss: 1.9624 - val_acc: 0.5961\n",
      "Epoch 460/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1045 - acc: 0.9622 - val_loss: 1.9934 - val_acc: 0.6338\n",
      "Epoch 461/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1300 - acc: 0.9468 - val_loss: 1.9720 - val_acc: 0.6050\n",
      "Epoch 462/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1215 - acc: 0.9476 - val_loss: 2.0600 - val_acc: 0.6068\n",
      "Epoch 463/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0998 - acc: 0.9568 - val_loss: 2.0862 - val_acc: 0.6032\n",
      "Epoch 464/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0842 - acc: 0.9684 - val_loss: 2.1952 - val_acc: 0.6104\n",
      "Epoch 465/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0788 - acc: 0.9692 - val_loss: 2.1873 - val_acc: 0.6068\n",
      "Epoch 466/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0797 - acc: 0.9715 - val_loss: 2.2557 - val_acc: 0.6140\n",
      "Epoch 467/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0801 - acc: 0.9692 - val_loss: 2.2154 - val_acc: 0.6050\n",
      "Epoch 468/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1741 - acc: 0.9136 - val_loss: 2.1466 - val_acc: 0.6014\n",
      "Epoch 469/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1298 - acc: 0.9530 - val_loss: 2.1067 - val_acc: 0.6284\n",
      "Epoch 470/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1234 - acc: 0.9476 - val_loss: 2.1040 - val_acc: 0.5889\n",
      "Epoch 471/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2223 - acc: 0.8774 - val_loss: 2.2144 - val_acc: 0.5853\n",
      "Epoch 472/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1463 - acc: 0.9291 - val_loss: 2.5916 - val_acc: 0.5709\n",
      "Epoch 473/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2913 - acc: 0.8951 - val_loss: 1.9459 - val_acc: 0.5835\n",
      "Epoch 474/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2412 - acc: 0.8759 - val_loss: 1.8899 - val_acc: 0.5763\n",
      "Epoch 475/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.2885 - acc: 0.8258 - val_loss: 2.0029 - val_acc: 0.5799\n",
      "Epoch 476/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1804 - acc: 0.9082 - val_loss: 2.2268 - val_acc: 0.5907\n",
      "Epoch 477/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1947 - acc: 0.9183 - val_loss: 2.0295 - val_acc: 0.5907\n",
      "Epoch 478/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0963 - acc: 0.9622 - val_loss: 2.0949 - val_acc: 0.5745\n",
      "Epoch 479/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1600 - acc: 0.9167 - val_loss: 2.1204 - val_acc: 0.5961\n",
      "Epoch 480/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0998 - acc: 0.9630 - val_loss: 2.2602 - val_acc: 0.5781\n",
      "Epoch 481/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.1425 - acc: 0.9422 - val_loss: 2.2678 - val_acc: 0.5727\n",
      "Epoch 482/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.1178 - acc: 0.9429 - val_loss: 2.3773 - val_acc: 0.5781\n",
      "Epoch 483/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0937 - acc: 0.9607 - val_loss: 2.4497 - val_acc: 0.5907\n",
      "Epoch 484/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0843 - acc: 0.9653 - val_loss: 2.3388 - val_acc: 0.6086\n",
      "Epoch 485/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0654 - acc: 0.9738 - val_loss: 2.2299 - val_acc: 0.5943\n",
      "Epoch 486/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0946 - acc: 0.9599 - val_loss: 2.2965 - val_acc: 0.6248\n",
      "Epoch 487/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0650 - acc: 0.9746 - val_loss: 2.3126 - val_acc: 0.6122\n",
      "Epoch 488/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0682 - acc: 0.9684 - val_loss: 2.3993 - val_acc: 0.5871\n",
      "Epoch 489/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0657 - acc: 0.9722 - val_loss: 2.3378 - val_acc: 0.5961\n",
      "Epoch 490/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0573 - acc: 0.9776 - val_loss: 2.3137 - val_acc: 0.6068\n",
      "Epoch 491/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0475 - acc: 0.9776 - val_loss: 2.3483 - val_acc: 0.6086\n",
      "Epoch 492/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0466 - acc: 0.9792 - val_loss: 2.4202 - val_acc: 0.5943\n",
      "Epoch 493/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0412 - acc: 0.9838 - val_loss: 2.5110 - val_acc: 0.5996\n",
      "Epoch 494/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0389 - acc: 0.9907 - val_loss: 2.6080 - val_acc: 0.5996\n",
      "Epoch 495/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0433 - acc: 0.9854 - val_loss: 2.5947 - val_acc: 0.5925\n",
      "Epoch 496/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0377 - acc: 0.9861 - val_loss: 2.6021 - val_acc: 0.5907\n",
      "Epoch 497/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0421 - acc: 0.9846 - val_loss: 2.6375 - val_acc: 0.5996\n",
      "Epoch 498/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0336 - acc: 0.9907 - val_loss: 2.7018 - val_acc: 0.6014\n",
      "Epoch 499/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0312 - acc: 0.9907 - val_loss: 2.6936 - val_acc: 0.5871\n",
      "Epoch 500/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0314 - acc: 0.9892 - val_loss: 2.7260 - val_acc: 0.5799\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_8 = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=500,\n",
    "                    batch_size=1000,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=10, kernel_size=10, strides=2, activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "model.add(layers.MaxPooling2D((10, 10)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=20, kernel_size=5, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((4, 4)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=40, kernel_size=3, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=80, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=160, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "# model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "# model.add(layers.Dense(200, activation='relu'))\n",
    "# model.add(layers.Dense(200, activation='relu'))\n",
    "# model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/500\n",
      "1297/1297 [==============================] - 4s 3ms/step - loss: 2.0157 - acc: 0.5459 - val_loss: 0.9693 - val_acc: 0.5799\n",
      "Epoch 2/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 1.2062 - acc: 0.5397 - val_loss: 3.1802 - val_acc: 0.5566\n",
      "Epoch 3/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 3.1099 - acc: 0.5628 - val_loss: 1.9689 - val_acc: 0.4829\n",
      "Epoch 4/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 1.8456 - acc: 0.4857 - val_loss: 2.2087 - val_acc: 0.5566\n",
      "Epoch 5/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 2.1104 - acc: 0.5613 - val_loss: 2.8110 - val_acc: 0.4434\n",
      "Epoch 6/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 2.7085 - acc: 0.4379 - val_loss: 2.1127 - val_acc: 0.5566\n",
      "Epoch 7/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 2.4319 - acc: 0.5628 - val_loss: 1.8428 - val_acc: 0.5566\n",
      "Epoch 8/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 1.7782 - acc: 0.5189 - val_loss: 1.2070 - val_acc: 0.4542\n",
      "Epoch 9/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 1.1986 - acc: 0.4641 - val_loss: 0.9376 - val_acc: 0.5566\n",
      "Epoch 10/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.8864 - acc: 0.5366 - val_loss: 0.8018 - val_acc: 0.4740\n",
      "Epoch 11/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.8052 - acc: 0.4965 - val_loss: 0.7637 - val_acc: 0.5566\n",
      "Epoch 12/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7446 - acc: 0.5528 - val_loss: 0.7606 - val_acc: 0.4704\n",
      "Epoch 13/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7730 - acc: 0.4665 - val_loss: 0.7034 - val_acc: 0.5512\n",
      "Epoch 14/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6971 - acc: 0.5582 - val_loss: 0.7020 - val_acc: 0.4776\n",
      "Epoch 15/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6977 - acc: 0.5081 - val_loss: 0.7042 - val_acc: 0.5512\n",
      "Epoch 16/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7013 - acc: 0.5551 - val_loss: 0.6875 - val_acc: 0.5476\n",
      "Epoch 17/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6849 - acc: 0.5528 - val_loss: 0.6864 - val_acc: 0.5368\n",
      "Epoch 18/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6859 - acc: 0.5412 - val_loss: 0.6874 - val_acc: 0.5386\n",
      "Epoch 19/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6855 - acc: 0.5173 - val_loss: 0.6856 - val_acc: 0.5494\n",
      "Epoch 20/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6817 - acc: 0.5590 - val_loss: 0.6858 - val_acc: 0.5530\n",
      "Epoch 21/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6807 - acc: 0.5497 - val_loss: 0.6850 - val_acc: 0.5583\n",
      "Epoch 22/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6787 - acc: 0.5497 - val_loss: 0.6881 - val_acc: 0.5619\n",
      "Epoch 23/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6795 - acc: 0.5590 - val_loss: 0.6910 - val_acc: 0.5278\n",
      "Epoch 24/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6834 - acc: 0.5451 - val_loss: 0.6952 - val_acc: 0.5619\n",
      "Epoch 25/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6838 - acc: 0.5613 - val_loss: 0.7230 - val_acc: 0.4758\n",
      "Epoch 26/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7172 - acc: 0.4942 - val_loss: 0.6910 - val_acc: 0.5601\n",
      "Epoch 27/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6837 - acc: 0.5436 - val_loss: 0.6825 - val_acc: 0.5583\n",
      "Epoch 28/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6731 - acc: 0.5559 - val_loss: 0.6820 - val_acc: 0.5566\n",
      "Epoch 29/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6745 - acc: 0.5451 - val_loss: 0.6823 - val_acc: 0.5619\n",
      "Epoch 30/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6746 - acc: 0.5605 - val_loss: 0.6862 - val_acc: 0.5224\n",
      "Epoch 31/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6771 - acc: 0.5258 - val_loss: 0.6792 - val_acc: 0.5637\n",
      "Epoch 32/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6688 - acc: 0.5605 - val_loss: 0.6758 - val_acc: 0.5583\n",
      "Epoch 33/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6658 - acc: 0.5497 - val_loss: 0.6748 - val_acc: 0.5601\n",
      "Epoch 34/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6640 - acc: 0.5490 - val_loss: 0.6750 - val_acc: 0.5619\n",
      "Epoch 35/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6624 - acc: 0.5605 - val_loss: 0.6747 - val_acc: 0.5548\n",
      "Epoch 36/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6608 - acc: 0.5567 - val_loss: 0.6865 - val_acc: 0.5566\n",
      "Epoch 37/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6743 - acc: 0.5652 - val_loss: 0.6708 - val_acc: 0.5655\n",
      "Epoch 38/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6594 - acc: 0.5698 - val_loss: 0.7143 - val_acc: 0.5009\n",
      "Epoch 39/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6929 - acc: 0.5405 - val_loss: 0.7385 - val_acc: 0.5548\n",
      "Epoch 40/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7097 - acc: 0.5667 - val_loss: 0.7610 - val_acc: 0.4865\n",
      "Epoch 41/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7355 - acc: 0.5150 - val_loss: 0.7556 - val_acc: 0.5548\n",
      "Epoch 42/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7255 - acc: 0.5628 - val_loss: 0.7594 - val_acc: 0.4865\n",
      "Epoch 43/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7412 - acc: 0.5135 - val_loss: 0.7074 - val_acc: 0.5601\n",
      "Epoch 44/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6778 - acc: 0.5736 - val_loss: 0.7404 - val_acc: 0.4847\n",
      "Epoch 45/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7071 - acc: 0.5343 - val_loss: 0.7597 - val_acc: 0.5583\n",
      "Epoch 46/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7249 - acc: 0.5883 - val_loss: 0.7768 - val_acc: 0.4865\n",
      "Epoch 47/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7364 - acc: 0.5289 - val_loss: 0.7661 - val_acc: 0.5619\n",
      "Epoch 48/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7327 - acc: 0.5867 - val_loss: 0.7690 - val_acc: 0.4901\n",
      "Epoch 49/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7383 - acc: 0.5197 - val_loss: 0.7157 - val_acc: 0.5727\n",
      "Epoch 50/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6904 - acc: 0.6014 - val_loss: 0.6761 - val_acc: 0.5117\n",
      "Epoch 51/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6516 - acc: 0.5482 - val_loss: 0.6790 - val_acc: 0.6122\n",
      "Epoch 52/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6576 - acc: 0.6291 - val_loss: 0.6571 - val_acc: 0.6086\n",
      "Epoch 53/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6351 - acc: 0.6130 - val_loss: 0.6565 - val_acc: 0.6158\n",
      "Epoch 54/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6292 - acc: 0.6307 - val_loss: 0.6560 - val_acc: 0.5871\n",
      "Epoch 55/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6270 - acc: 0.5937 - val_loss: 0.6471 - val_acc: 0.6104\n",
      "Epoch 56/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6131 - acc: 0.6361 - val_loss: 0.6500 - val_acc: 0.6194\n",
      "Epoch 57/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6143 - acc: 0.6423 - val_loss: 0.7269 - val_acc: 0.4955\n",
      "Epoch 58/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6710 - acc: 0.5443 - val_loss: 0.7360 - val_acc: 0.5709\n",
      "Epoch 59/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6893 - acc: 0.6130 - val_loss: 0.7627 - val_acc: 0.4919\n",
      "Epoch 60/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.7137 - acc: 0.5189 - val_loss: 0.7225 - val_acc: 0.5925\n",
      "Epoch 61/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6822 - acc: 0.6338 - val_loss: 0.6827 - val_acc: 0.5996\n",
      "Epoch 62/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6351 - acc: 0.6199 - val_loss: 0.6503 - val_acc: 0.6266\n",
      "Epoch 63/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6106 - acc: 0.6423 - val_loss: 0.6460 - val_acc: 0.6158\n",
      "Epoch 64/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6001 - acc: 0.6384 - val_loss: 0.6479 - val_acc: 0.6176\n",
      "Epoch 65/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5973 - acc: 0.6623 - val_loss: 0.6438 - val_acc: 0.6122\n",
      "Epoch 66/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5900 - acc: 0.6523 - val_loss: 0.6474 - val_acc: 0.6212\n",
      "Epoch 67/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5902 - acc: 0.6461 - val_loss: 0.6459 - val_acc: 0.6230\n",
      "Epoch 68/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5818 - acc: 0.6584 - val_loss: 0.6509 - val_acc: 0.6266\n",
      "Epoch 69/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5792 - acc: 0.6739 - val_loss: 0.6599 - val_acc: 0.6284\n",
      "Epoch 70/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5855 - acc: 0.6615 - val_loss: 0.6958 - val_acc: 0.5637\n",
      "Epoch 71/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5965 - acc: 0.6045 - val_loss: 0.6767 - val_acc: 0.6284\n",
      "Epoch 72/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6004 - acc: 0.6677 - val_loss: 0.6532 - val_acc: 0.6194\n",
      "Epoch 73/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5737 - acc: 0.6608 - val_loss: 0.6637 - val_acc: 0.6230\n",
      "Epoch 74/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5778 - acc: 0.6777 - val_loss: 0.6682 - val_acc: 0.6176\n",
      "Epoch 75/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5676 - acc: 0.6739 - val_loss: 0.6681 - val_acc: 0.6248\n",
      "Epoch 76/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5716 - acc: 0.6662 - val_loss: 0.6893 - val_acc: 0.6194\n",
      "Epoch 77/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5638 - acc: 0.6662 - val_loss: 0.6643 - val_acc: 0.6409\n",
      "Epoch 78/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5603 - acc: 0.6731 - val_loss: 0.6642 - val_acc: 0.6373\n",
      "Epoch 79/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5513 - acc: 0.6793 - val_loss: 0.6853 - val_acc: 0.6176\n",
      "Epoch 80/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.5613 - acc: 0.6708 - val_loss: 0.6681 - val_acc: 0.6427\n",
      "Epoch 81/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.5454 - acc: 0.6877 - val_loss: 0.6888 - val_acc: 0.5889\n",
      "Epoch 82/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5590 - acc: 0.6461 - val_loss: 0.6747 - val_acc: 0.6284\n",
      "Epoch 83/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5487 - acc: 0.6901 - val_loss: 0.6914 - val_acc: 0.6122\n",
      "Epoch 84/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5491 - acc: 0.6839 - val_loss: 0.6745 - val_acc: 0.6320\n",
      "Epoch 85/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5308 - acc: 0.6947 - val_loss: 0.6836 - val_acc: 0.6230\n",
      "Epoch 86/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5284 - acc: 0.7001 - val_loss: 0.6734 - val_acc: 0.6302\n",
      "Epoch 87/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5248 - acc: 0.6947 - val_loss: 0.6751 - val_acc: 0.6302\n",
      "Epoch 88/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5205 - acc: 0.6931 - val_loss: 0.6934 - val_acc: 0.6158\n",
      "Epoch 89/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5203 - acc: 0.6908 - val_loss: 0.7088 - val_acc: 0.6176\n",
      "Epoch 90/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5260 - acc: 0.7024 - val_loss: 0.7039 - val_acc: 0.6248\n",
      "Epoch 91/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5201 - acc: 0.6962 - val_loss: 0.7216 - val_acc: 0.6373\n",
      "Epoch 92/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5223 - acc: 0.6978 - val_loss: 0.7210 - val_acc: 0.6284\n",
      "Epoch 93/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5138 - acc: 0.7055 - val_loss: 0.7169 - val_acc: 0.5943\n",
      "Epoch 94/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5257 - acc: 0.7047 - val_loss: 0.8276 - val_acc: 0.6230\n",
      "Epoch 95/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6132 - acc: 0.6476 - val_loss: 0.7732 - val_acc: 0.5799\n",
      "Epoch 96/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5755 - acc: 0.6507 - val_loss: 0.8614 - val_acc: 0.5332\n",
      "Epoch 97/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6148 - acc: 0.5968 - val_loss: 0.7727 - val_acc: 0.5817\n",
      "Epoch 98/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.6108 - acc: 0.6700 - val_loss: 0.7447 - val_acc: 0.5260\n",
      "Epoch 99/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5912 - acc: 0.5968 - val_loss: 0.7004 - val_acc: 0.6266\n",
      "Epoch 100/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5535 - acc: 0.7078 - val_loss: 0.6899 - val_acc: 0.6302\n",
      "Epoch 101/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5373 - acc: 0.6739 - val_loss: 0.7052 - val_acc: 0.6032\n",
      "Epoch 102/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5345 - acc: 0.6962 - val_loss: 0.7372 - val_acc: 0.5907\n",
      "Epoch 103/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5432 - acc: 0.6885 - val_loss: 0.8625 - val_acc: 0.5745\n",
      "Epoch 104/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5766 - acc: 0.6546 - val_loss: 0.7359 - val_acc: 0.6032\n",
      "Epoch 105/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5305 - acc: 0.6970 - val_loss: 0.6983 - val_acc: 0.6338\n",
      "Epoch 106/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5031 - acc: 0.7101 - val_loss: 0.7047 - val_acc: 0.6373\n",
      "Epoch 107/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5014 - acc: 0.7055 - val_loss: 0.7247 - val_acc: 0.6338\n",
      "Epoch 108/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4920 - acc: 0.7201 - val_loss: 0.7439 - val_acc: 0.6176\n",
      "Epoch 109/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4877 - acc: 0.7186 - val_loss: 0.7632 - val_acc: 0.5996\n",
      "Epoch 110/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.5016 - acc: 0.7132 - val_loss: 0.7711 - val_acc: 0.6140\n",
      "Epoch 111/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4918 - acc: 0.7163 - val_loss: 0.7312 - val_acc: 0.6212\n",
      "Epoch 112/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4848 - acc: 0.7325 - val_loss: 0.7371 - val_acc: 0.6230\n",
      "Epoch 113/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4866 - acc: 0.7224 - val_loss: 0.7329 - val_acc: 0.6212\n",
      "Epoch 114/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4788 - acc: 0.7325 - val_loss: 0.7457 - val_acc: 0.6230\n",
      "Epoch 115/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4759 - acc: 0.7325 - val_loss: 0.7758 - val_acc: 0.6194\n",
      "Epoch 116/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4825 - acc: 0.7163 - val_loss: 0.7623 - val_acc: 0.6104\n",
      "Epoch 117/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4717 - acc: 0.7332 - val_loss: 0.7774 - val_acc: 0.6194\n",
      "Epoch 118/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4826 - acc: 0.7178 - val_loss: 0.7839 - val_acc: 0.6158\n",
      "Epoch 119/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4776 - acc: 0.7278 - val_loss: 0.7813 - val_acc: 0.6194\n",
      "Epoch 120/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4700 - acc: 0.7325 - val_loss: 0.7614 - val_acc: 0.6176\n",
      "Epoch 121/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4617 - acc: 0.7448 - val_loss: 0.7908 - val_acc: 0.6086\n",
      "Epoch 122/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4716 - acc: 0.7317 - val_loss: 0.7827 - val_acc: 0.6194\n",
      "Epoch 123/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4700 - acc: 0.7209 - val_loss: 0.7789 - val_acc: 0.6104\n",
      "Epoch 124/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4660 - acc: 0.7332 - val_loss: 0.7808 - val_acc: 0.6122\n",
      "Epoch 125/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4562 - acc: 0.7456 - val_loss: 0.8051 - val_acc: 0.6320\n",
      "Epoch 126/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4544 - acc: 0.7440 - val_loss: 0.8496 - val_acc: 0.6014\n",
      "Epoch 127/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4623 - acc: 0.7325 - val_loss: 0.8453 - val_acc: 0.6373\n",
      "Epoch 128/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4568 - acc: 0.7433 - val_loss: 0.8435 - val_acc: 0.6104\n",
      "Epoch 129/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4500 - acc: 0.7417 - val_loss: 0.8365 - val_acc: 0.6338\n",
      "Epoch 130/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4510 - acc: 0.7425 - val_loss: 0.8210 - val_acc: 0.6122\n",
      "Epoch 131/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4433 - acc: 0.7540 - val_loss: 0.8307 - val_acc: 0.6176\n",
      "Epoch 132/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4397 - acc: 0.7417 - val_loss: 0.8664 - val_acc: 0.5996\n",
      "Epoch 133/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4441 - acc: 0.7456 - val_loss: 0.8624 - val_acc: 0.6140\n",
      "Epoch 134/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4373 - acc: 0.7471 - val_loss: 0.8877 - val_acc: 0.5817\n",
      "Epoch 135/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4390 - acc: 0.7571 - val_loss: 0.8932 - val_acc: 0.6104\n",
      "Epoch 136/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4378 - acc: 0.7502 - val_loss: 0.8845 - val_acc: 0.6032\n",
      "Epoch 137/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4367 - acc: 0.7502 - val_loss: 0.8542 - val_acc: 0.6212\n",
      "Epoch 138/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4351 - acc: 0.7479 - val_loss: 0.8250 - val_acc: 0.6122\n",
      "Epoch 139/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4346 - acc: 0.7502 - val_loss: 0.8136 - val_acc: 0.6176\n",
      "Epoch 140/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4355 - acc: 0.7540 - val_loss: 0.8281 - val_acc: 0.6104\n",
      "Epoch 141/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4240 - acc: 0.7633 - val_loss: 0.8822 - val_acc: 0.6104\n",
      "Epoch 142/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4275 - acc: 0.7571 - val_loss: 0.8988 - val_acc: 0.6176\n",
      "Epoch 143/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4303 - acc: 0.7525 - val_loss: 0.8992 - val_acc: 0.5871\n",
      "Epoch 144/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.4232 - acc: 0.7733 - val_loss: 0.9029 - val_acc: 0.6176\n",
      "Epoch 145/500\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_9 = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=500,\n",
    "                    batch_size=1000,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
