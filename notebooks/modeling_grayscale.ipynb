{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 17.6MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from keras) (1.14.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from keras) (1.11.0)\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/56/4bcec5a8d9503a87e58e814c4e32ac2b32c37c685672c30bc8c54c6e478a/Keras_Applications-1.0.8.tar.gz (289kB)\n",
      "\u001b[K    100% |████████████████████████████████| 296kB 26.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from keras) (2.8.0)\n",
      "Building wheels for collected packages: keras-applications\n",
      "  Running setup.py bdist_wheel for keras-applications ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/dd/f2/5d/2689b5547f32c4e258c3b7ccbe7f1d0f2afbb84fb01e830792\n",
      "Successfully built keras-applications\n",
      "\u001b[31mtyping-extensions 3.7.4.1 has requirement typing>=3.7.4; python_version < \"3.5\", but you'll have typing 3.6.4 which is incompatible.\u001b[0m\n",
      "Installing collected packages: keras-preprocessing, keras-applications, keras\n",
      "Successfully installed keras-2.3.1 keras-applications-1.0.8 keras-preprocessing-1.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting pip\n",
      "  Using cached https://files.pythonhosted.org/packages/00/b6/9cfa56b4081ad13874b0c6f96af8ce16cfbc1cb06bedf8e9164ce5551ec1/pip-19.3.1-py2.py3-none-any.whl\n",
      "\u001b[31mtyping-extensions 3.7.4.1 has requirement typing>=3.7.4; python_version < \"3.5\", but you'll have typing 3.6.4 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pip\n",
      "  Found existing installation: pip 10.0.1\n",
      "    Uninstalling pip-10.0.1:\n",
      "      Successfully uninstalled pip-10.0.1\n",
      "Successfully installed pip-19.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 463 images belonging to 2 classes.\n",
      "Found 1839 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Directory path\n",
    "train_data_dir = '../data/Cracks/train'\n",
    "test_data_dir = '../data/Cracks/test'\n",
    "\n",
    "# Get all the data in the directory data/validation, and reshape them\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "        test_data_dir, \n",
    "        target_size=(256, 256), batch_size=463, color_mode='grayscale')\n",
    "\n",
    "# Get all the data in the directory data/train, and reshape them\n",
    "train_generator = ImageDataGenerator().flow_from_directory(\n",
    "        train_data_dir, \n",
    "        target_size=(256, 256), batch_size=1839, color_mode='grayscale')\n",
    "\n",
    "# Create the datasets\n",
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256 at 0x7FBC8E155F60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previewing an image\n",
    "array_to_img(train_images[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1839, 256, 256, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (1839, 256, 256, 1) (1839, 2)\n",
      "test data shape: (463, 256, 256, 1) (463, 2)\n"
     ]
    }
   ],
   "source": [
    "# Checking shape of data\n",
    "print('train data shape:', np.shape(train_images), np.shape(train_labels))\n",
    "print('test data shape:', np.shape(test_images), np.shape(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_img: (1839, 65536)\n",
      "test_img: (463, 65536)\n"
     ]
    }
   ],
   "source": [
    "# Unrowing/reshaping\n",
    "train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "print('train_img:', np.shape(train_img))\n",
    "\n",
    "test_img = test_images.reshape(test_images.shape[0], -1)\n",
    "print('test_img:', np.shape(test_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the labels\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dutch': 0, 'Flemish': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels final: (1839, 1)\n",
      "test labels final: (463, 1)\n"
     ]
    }
   ],
   "source": [
    "# Transposing the labels\n",
    "train_y = np.reshape(train_labels[:,0], (1839,1))\n",
    "print('train labels final:', np.shape(train_y))\n",
    "\n",
    "test_y = np.reshape(test_labels[:,0], (463,1))\n",
    "print('test labels final:', np.shape(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(20, activation='relu', input_shape=(65536,))) #2 hidden layers\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1287 samples, validate on 552 samples\n",
      "Epoch 1/5\n",
      "1287/1287 [==============================] - 2s 1ms/step - loss: 8.9437 - acc: 0.4359 - val_loss: 9.1102 - val_acc: 0.4348\n",
      "Epoch 2/5\n",
      "1287/1287 [==============================] - 1s 439us/step - loss: 9.0923 - acc: 0.4359 - val_loss: 9.1102 - val_acc: 0.4348\n",
      "Epoch 3/5\n",
      "1287/1287 [==============================] - 1s 431us/step - loss: 9.0923 - acc: 0.4359 - val_loss: 9.1102 - val_acc: 0.4348\n",
      "Epoch 4/5\n",
      "1287/1287 [==============================] - 1s 432us/step - loss: 9.0923 - acc: 0.4359 - val_loss: 9.1102 - val_acc: 0.4348\n",
      "Epoch 5/5\n",
      "1287/1287 [==============================] - 1s 436us/step - loss: 9.0923 - acc: 0.4359 - val_loss: 9.1102 - val_acc: 0.4348\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "histoire = model.fit(train_img,\n",
    "                    train_y,\n",
    "                    epochs=5,\n",
    "                    batch_size=100,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer conv2d_3: expected ndim=4, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e755a5e199f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.add(layers.Conv2D(20, (3, 3), activation='relu',\n\u001b[0;32m----> 4\u001b[0;31m                         input_shape=(256, 256,  1)))\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer conv2d_3: expected ndim=4, found ndim=2"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(20, (3, 3), activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(70, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(150, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(200, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(250, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/5\n",
      "1297/1297 [==============================] - 133s 103ms/step - loss: 0.7820 - accuracy: 0.4965 - val_loss: 0.6893 - val_accuracy: 0.5727\n",
      "Epoch 2/5\n",
      "1297/1297 [==============================] - 134s 103ms/step - loss: 0.6865 - accuracy: 0.5559 - val_loss: 0.6751 - val_accuracy: 0.5727\n",
      "Epoch 3/5\n",
      "1297/1297 [==============================] - 133s 102ms/step - loss: 0.6688 - accuracy: 0.5559 - val_loss: 0.6672 - val_accuracy: 0.5727\n",
      "Epoch 4/5\n",
      "1297/1297 [==============================] - 133s 103ms/step - loss: 0.6643 - accuracy: 0.5659 - val_loss: 0.6906 - val_accuracy: 0.5566\n",
      "Epoch 5/5\n",
      "1297/1297 [==============================] - 132s 102ms/step - loss: 0.6893 - accuracy: 0.5389 - val_loss: 0.6850 - val_accuracy: 0.5727\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_3 = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=5,\n",
    "                    batch_size=50,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(20, (3, 3), activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(70, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(150, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(200, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(250, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/10\n",
      "1297/1297 [==============================] - 135s 104ms/step - loss: 0.7362 - accuracy: 0.4811 - val_loss: 0.6913 - val_accuracy: 0.5637\n",
      "Epoch 2/10\n",
      "1297/1297 [==============================] - 135s 104ms/step - loss: 0.6900 - accuracy: 0.5598 - val_loss: 0.6872 - val_accuracy: 0.5637\n",
      "Epoch 3/10\n",
      "1297/1297 [==============================] - 135s 104ms/step - loss: 0.6883 - accuracy: 0.5598 - val_loss: 0.6815 - val_accuracy: 0.5637\n",
      "Epoch 4/10\n",
      "1297/1297 [==============================] - 134s 103ms/step - loss: 0.6872 - accuracy: 0.5598 - val_loss: 0.6840 - val_accuracy: 0.5637\n",
      "Epoch 5/10\n",
      "1297/1297 [==============================] - 135s 104ms/step - loss: 0.6876 - accuracy: 0.5598 - val_loss: 0.6818 - val_accuracy: 0.5637\n",
      "Epoch 6/10\n",
      "1297/1297 [==============================] - 134s 104ms/step - loss: 0.6865 - accuracy: 0.5598 - val_loss: 0.6857 - val_accuracy: 0.5637\n",
      "Epoch 7/10\n",
      "1297/1297 [==============================] - 135s 104ms/step - loss: 0.6863 - accuracy: 0.5598 - val_loss: 0.6840 - val_accuracy: 0.5637\n",
      "Epoch 8/10\n",
      "1297/1297 [==============================] - 134s 103ms/step - loss: 0.6825 - accuracy: 0.5598 - val_loss: 0.6756 - val_accuracy: 0.5637\n",
      "Epoch 9/10\n",
      "1297/1297 [==============================] - 135s 104ms/step - loss: 0.6717 - accuracy: 0.5598 - val_loss: 0.6869 - val_accuracy: 0.5637\n",
      "Epoch 10/10\n",
      "1297/1297 [==============================] - 134s 104ms/step - loss: 0.6845 - accuracy: 0.5598 - val_loss: 0.6769 - val_accuracy: 0.5637\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_3 = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=10,\n",
    "                    batch_size=100,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(20, (3, 3), activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(250, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(100, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(50, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(250, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/10\n",
      "1297/1297 [==============================] - 231s 178ms/step - loss: 0.8093 - accuracy: 0.5544 - val_loss: 0.6850 - val_accuracy: 0.5566\n",
      "Epoch 2/10\n",
      "1297/1297 [==============================] - 229s 176ms/step - loss: 0.6949 - accuracy: 0.5628 - val_loss: 0.6839 - val_accuracy: 0.5566\n",
      "Epoch 3/10\n",
      "1297/1297 [==============================] - 228s 176ms/step - loss: 0.6953 - accuracy: 0.5305 - val_loss: 0.6844 - val_accuracy: 0.5566\n",
      "Epoch 4/10\n",
      "1297/1297 [==============================] - 228s 176ms/step - loss: 0.6849 - accuracy: 0.5628 - val_loss: 0.6793 - val_accuracy: 0.5566\n",
      "Epoch 5/10\n",
      "1297/1297 [==============================] - 229s 177ms/step - loss: 0.6785 - accuracy: 0.5628 - val_loss: 0.6784 - val_accuracy: 0.5566\n",
      "Epoch 6/10\n",
      "1297/1297 [==============================] - 228s 175ms/step - loss: 0.6769 - accuracy: 0.5628 - val_loss: 0.6636 - val_accuracy: 0.5566\n",
      "Epoch 7/10\n",
      " 100/1297 [=>............................] - ETA: 3:04 - loss: 0.6396 - accuracy: 0.6500"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_4 = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=10,\n",
    "                    batch_size=100,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=3, kernel_size=128, strides=(3, 3), activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=3, kernel_size=128, strides=(3, 3), activation='relu'))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=3, kernel_size=64, strides=(3, 3), activation='relu'))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=3, kernel_size=32, strides=(3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=3, kernel_size=1, strides=(3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/10\n",
      "1297/1297 [==============================] - 241s 186ms/step - loss: 0.9748 - accuracy: 0.4796 - val_loss: 0.6925 - val_accuracy: 0.5530\n",
      "Epoch 2/10\n",
      "1297/1297 [==============================] - 239s 184ms/step - loss: 0.6914 - accuracy: 0.5644 - val_loss: 0.6905 - val_accuracy: 0.5530\n",
      "Epoch 3/10\n",
      "1297/1297 [==============================] - 239s 185ms/step - loss: 0.6888 - accuracy: 0.5644 - val_loss: 0.6886 - val_accuracy: 0.5530\n",
      "Epoch 4/10\n",
      "1297/1297 [==============================] - 240s 185ms/step - loss: 0.6870 - accuracy: 0.5644 - val_loss: 0.6876 - val_accuracy: 0.5530\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-1956eb95c245>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                     validation_split=0.3)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_5 = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=10,\n",
    "                    batch_size=100,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=10, kernel_size=10, strides=(3, 3), activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=20, kernel_size=5, strides=(3, 3), activation='relu'))\n",
    "model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=40, kernel_size=3, strides=(3, 3), activation='relu'))\n",
    "model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=80, kernel_size=3, strides=(3, 3), activation='relu'))\n",
    "model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=160, kernel_size=1, strides=(3, 3), activation='relu'))\n",
    "model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/50\n",
      "1297/1297 [==============================] - 13s 10ms/step - loss: 0.6833 - accuracy: 0.5659 - val_loss: 0.6885 - val_accuracy: 0.5530\n",
      "Epoch 2/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6822 - accuracy: 0.5652 - val_loss: 0.7032 - val_accuracy: 0.5530\n",
      "Epoch 3/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6802 - accuracy: 0.5644 - val_loss: 0.6854 - val_accuracy: 0.5530\n",
      "Epoch 4/50\n",
      "1297/1297 [==============================] - 13s 10ms/step - loss: 0.6641 - accuracy: 0.5644 - val_loss: 0.6800 - val_accuracy: 0.5530\n",
      "Epoch 5/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6601 - accuracy: 0.5590 - val_loss: 0.6719 - val_accuracy: 0.5314\n",
      "Epoch 6/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6547 - accuracy: 0.5520 - val_loss: 0.6764 - val_accuracy: 0.5278\n",
      "Epoch 7/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6471 - accuracy: 0.5821 - val_loss: 0.6755 - val_accuracy: 0.5206\n",
      "Epoch 8/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6351 - accuracy: 0.5682 - val_loss: 0.6731 - val_accuracy: 0.5566\n",
      "Epoch 9/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6454 - accuracy: 0.6060 - val_loss: 0.6812 - val_accuracy: 0.5278\n",
      "Epoch 10/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6696 - accuracy: 0.5891 - val_loss: 0.6932 - val_accuracy: 0.5296\n",
      "Epoch 11/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6849 - accuracy: 0.5428 - val_loss: 0.6910 - val_accuracy: 0.4847\n",
      "Epoch 12/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6832 - accuracy: 0.5405 - val_loss: 0.6900 - val_accuracy: 0.5332\n",
      "Epoch 13/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6808 - accuracy: 0.5636 - val_loss: 0.6890 - val_accuracy: 0.5224\n",
      "Epoch 14/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6806 - accuracy: 0.5636 - val_loss: 0.6870 - val_accuracy: 0.5530\n",
      "Epoch 15/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6696 - accuracy: 0.5644 - val_loss: 0.6723 - val_accuracy: 0.5422\n",
      "Epoch 16/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6554 - accuracy: 0.5736 - val_loss: 0.6744 - val_accuracy: 0.5135\n",
      "Epoch 17/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6589 - accuracy: 0.5883 - val_loss: 0.6828 - val_accuracy: 0.5458\n",
      "Epoch 18/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6478 - accuracy: 0.5790 - val_loss: 0.6735 - val_accuracy: 0.5206\n",
      "Epoch 19/50\n",
      "1297/1297 [==============================] - 13s 10ms/step - loss: 0.6805 - accuracy: 0.5297 - val_loss: 0.6908 - val_accuracy: 0.4973\n",
      "Epoch 20/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6865 - accuracy: 0.5289 - val_loss: 0.6896 - val_accuracy: 0.5278\n",
      "Epoch 21/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6820 - accuracy: 0.5690 - val_loss: 0.6742 - val_accuracy: 0.5332\n",
      "Epoch 22/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6791 - accuracy: 0.5374 - val_loss: 0.7106 - val_accuracy: 0.5458\n",
      "Epoch 23/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6858 - accuracy: 0.5536 - val_loss: 0.6886 - val_accuracy: 0.5530\n",
      "Epoch 24/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6809 - accuracy: 0.5644 - val_loss: 0.6955 - val_accuracy: 0.5530\n",
      "Epoch 25/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6703 - accuracy: 0.5644 - val_loss: 0.6765 - val_accuracy: 0.5530\n",
      "Epoch 26/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6600 - accuracy: 0.5644 - val_loss: 0.6871 - val_accuracy: 0.5530\n",
      "Epoch 27/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6826 - accuracy: 0.5644 - val_loss: 0.6864 - val_accuracy: 0.5530\n",
      "Epoch 28/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6617 - accuracy: 0.5644 - val_loss: 0.6846 - val_accuracy: 0.5530\n",
      "Epoch 29/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6748 - accuracy: 0.5644 - val_loss: 0.6838 - val_accuracy: 0.5530\n",
      "Epoch 30/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6666 - accuracy: 0.5644 - val_loss: 0.6986 - val_accuracy: 0.5530\n",
      "Epoch 31/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6651 - accuracy: 0.5644 - val_loss: 0.6797 - val_accuracy: 0.5530\n",
      "Epoch 32/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6397 - accuracy: 0.5744 - val_loss: 0.6988 - val_accuracy: 0.5458\n",
      "Epoch 33/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6391 - accuracy: 0.5983 - val_loss: 0.6813 - val_accuracy: 0.5673\n",
      "Epoch 34/50\n",
      "1297/1297 [==============================] - 12s 10ms/step - loss: 0.6170 - accuracy: 0.6191 - val_loss: 0.7396 - val_accuracy: 0.5332\n",
      "Epoch 35/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6071 - accuracy: 0.6345 - val_loss: 0.7588 - val_accuracy: 0.5476\n",
      "Epoch 36/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6010 - accuracy: 0.6523 - val_loss: 0.9549 - val_accuracy: 0.5583\n",
      "Epoch 37/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6282 - accuracy: 0.6176 - val_loss: 0.8407 - val_accuracy: 0.5512\n",
      "Epoch 38/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6126 - accuracy: 0.6415 - val_loss: 0.7106 - val_accuracy: 0.5637\n",
      "Epoch 39/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6049 - accuracy: 0.6554 - val_loss: 0.7641 - val_accuracy: 0.5512\n",
      "Epoch 40/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.5889 - accuracy: 0.6484 - val_loss: 1.1228 - val_accuracy: 0.5242\n",
      "Epoch 41/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6858 - accuracy: 0.5837 - val_loss: 0.6947 - val_accuracy: 0.5117\n",
      "Epoch 42/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6683 - accuracy: 0.5821 - val_loss: 0.6876 - val_accuracy: 0.5224\n",
      "Epoch 43/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6292 - accuracy: 0.6160 - val_loss: 0.8359 - val_accuracy: 0.5458\n",
      "Epoch 44/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6168 - accuracy: 0.6307 - val_loss: 0.8044 - val_accuracy: 0.5171\n",
      "Epoch 45/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6060 - accuracy: 0.6430 - val_loss: 0.8263 - val_accuracy: 0.5260\n",
      "Epoch 46/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6013 - accuracy: 0.6469 - val_loss: 0.7822 - val_accuracy: 0.5673\n",
      "Epoch 47/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.5962 - accuracy: 0.6638 - val_loss: 0.8260 - val_accuracy: 0.5512\n",
      "Epoch 48/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6029 - accuracy: 0.6500 - val_loss: 0.8352 - val_accuracy: 0.5619\n",
      "Epoch 49/50\n",
      "1297/1297 [==============================] - 12s 9ms/step - loss: 0.6063 - accuracy: 0.6500 - val_loss: 0.7191 - val_accuracy: 0.5781\n",
      "Epoch 50/50\n",
      "1297/1297 [==============================] - 12s 10ms/step - loss: 0.5967 - accuracy: 0.6631 - val_loss: 0.7523 - val_accuracy: 0.5619\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_6 = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    batch_size=100,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=10, kernel_size=10, strides=2, activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "model.add(layers.AveragePooling2D((10, 10)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=20, kernel_size=5, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((6, 6)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=40, kernel_size=3, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=80, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=160, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 1.2859 - accuracy: 0.4603 - val_loss: 0.7118 - val_accuracy: 0.4057\n",
      "Epoch 2/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.7160 - accuracy: 0.4534 - val_loss: 1.0765 - val_accuracy: 0.5907\n",
      "Epoch 3/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 1.0607 - accuracy: 0.5359 - val_loss: 0.8590 - val_accuracy: 0.4093\n",
      "Epoch 4/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.7960 - accuracy: 0.4518 - val_loss: 0.6941 - val_accuracy: 0.4811\n",
      "Epoch 5/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6934 - accuracy: 0.4904 - val_loss: 0.6943 - val_accuracy: 0.4758\n",
      "Epoch 6/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6921 - accuracy: 0.4927 - val_loss: 0.6756 - val_accuracy: 0.5907\n",
      "Epoch 7/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6833 - accuracy: 0.5482 - val_loss: 0.6894 - val_accuracy: 0.5099\n",
      "Epoch 8/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6841 - accuracy: 0.5582 - val_loss: 0.6727 - val_accuracy: 0.5907\n",
      "Epoch 9/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6838 - accuracy: 0.5482 - val_loss: 0.6749 - val_accuracy: 0.5907\n",
      "Epoch 10/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6858 - accuracy: 0.5482 - val_loss: 0.6761 - val_accuracy: 0.5907\n",
      "Epoch 11/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6866 - accuracy: 0.5482 - val_loss: 0.6766 - val_accuracy: 0.5907\n",
      "Epoch 12/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.6868 - accuracy: 0.5482 - val_loss: 0.6771 - val_accuracy: 0.5907\n",
      "Epoch 13/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6859 - accuracy: 0.5482 - val_loss: 0.6761 - val_accuracy: 0.5907\n",
      "Epoch 14/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6847 - accuracy: 0.5482 - val_loss: 0.6744 - val_accuracy: 0.5907\n",
      "Epoch 15/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6829 - accuracy: 0.5482 - val_loss: 0.6716 - val_accuracy: 0.5907\n",
      "Epoch 16/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6807 - accuracy: 0.5482 - val_loss: 0.6690 - val_accuracy: 0.5907\n",
      "Epoch 17/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6780 - accuracy: 0.5482 - val_loss: 0.6659 - val_accuracy: 0.5907\n",
      "Epoch 18/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6749 - accuracy: 0.5482 - val_loss: 0.6629 - val_accuracy: 0.5907\n",
      "Epoch 19/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6716 - accuracy: 0.5482 - val_loss: 0.6634 - val_accuracy: 0.5907\n",
      "Epoch 20/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.6703 - accuracy: 0.5482 - val_loss: 0.6609 - val_accuracy: 0.5889\n",
      "Epoch 21/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6683 - accuracy: 0.5490 - val_loss: 0.6499 - val_accuracy: 0.5907\n",
      "Epoch 22/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6638 - accuracy: 0.5490 - val_loss: 0.6509 - val_accuracy: 0.6140\n",
      "Epoch 23/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6652 - accuracy: 0.5251 - val_loss: 0.6436 - val_accuracy: 0.6086\n",
      "Epoch 24/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6635 - accuracy: 0.5520 - val_loss: 0.6392 - val_accuracy: 0.5943\n",
      "Epoch 25/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6604 - accuracy: 0.5536 - val_loss: 0.6513 - val_accuracy: 0.6014\n",
      "Epoch 26/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6596 - accuracy: 0.5628 - val_loss: 0.6539 - val_accuracy: 0.5763\n",
      "Epoch 27/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6603 - accuracy: 0.5790 - val_loss: 0.6477 - val_accuracy: 0.5943\n",
      "Epoch 28/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.6572 - accuracy: 0.5644 - val_loss: 0.6409 - val_accuracy: 0.5853\n",
      "Epoch 29/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6552 - accuracy: 0.5667 - val_loss: 0.6370 - val_accuracy: 0.6230\n",
      "Epoch 30/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6553 - accuracy: 0.5821 - val_loss: 0.6361 - val_accuracy: 0.5996\n",
      "Epoch 31/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6526 - accuracy: 0.5705 - val_loss: 0.6355 - val_accuracy: 0.6104\n",
      "Epoch 32/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6499 - accuracy: 0.5806 - val_loss: 0.6353 - val_accuracy: 0.6140\n",
      "Epoch 33/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6476 - accuracy: 0.5883 - val_loss: 0.6302 - val_accuracy: 0.6050\n",
      "Epoch 34/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6452 - accuracy: 0.5906 - val_loss: 0.6398 - val_accuracy: 0.6176\n",
      "Epoch 35/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6456 - accuracy: 0.6191 - val_loss: 0.6208 - val_accuracy: 0.6230\n",
      "Epoch 36/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.6412 - accuracy: 0.5921 - val_loss: 0.6208 - val_accuracy: 0.6320\n",
      "Epoch 37/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6354 - accuracy: 0.6122 - val_loss: 0.6372 - val_accuracy: 0.5691\n",
      "Epoch 38/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6397 - accuracy: 0.5952 - val_loss: 0.6204 - val_accuracy: 0.6320\n",
      "Epoch 39/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6319 - accuracy: 0.6137 - val_loss: 0.6205 - val_accuracy: 0.6355\n",
      "Epoch 40/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6275 - accuracy: 0.6153 - val_loss: 0.6112 - val_accuracy: 0.6427\n",
      "Epoch 41/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6199 - accuracy: 0.6207 - val_loss: 0.6143 - val_accuracy: 0.6409\n",
      "Epoch 42/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6158 - accuracy: 0.6145 - val_loss: 0.6235 - val_accuracy: 0.5907\n",
      "Epoch 43/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.6155 - accuracy: 0.6307 - val_loss: 0.6262 - val_accuracy: 0.6320\n",
      "Epoch 44/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.6316 - accuracy: 0.6222 - val_loss: 0.7848 - val_accuracy: 0.4722\n",
      "Epoch 45/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.7072 - accuracy: 0.5420 - val_loss: 0.6337 - val_accuracy: 0.6517\n",
      "Epoch 46/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6332 - accuracy: 0.6130 - val_loss: 0.6596 - val_accuracy: 0.5889\n",
      "Epoch 47/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6627 - accuracy: 0.5490 - val_loss: 0.6579 - val_accuracy: 0.5907\n",
      "Epoch 48/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6575 - accuracy: 0.5482 - val_loss: 0.6586 - val_accuracy: 0.5907\n",
      "Epoch 49/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6558 - accuracy: 0.5482 - val_loss: 0.6560 - val_accuracy: 0.5907\n",
      "Epoch 50/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6533 - accuracy: 0.5482 - val_loss: 0.6495 - val_accuracy: 0.5907\n",
      "Epoch 51/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.6485 - accuracy: 0.5482 - val_loss: 0.6437 - val_accuracy: 0.5907\n",
      "Epoch 52/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.6439 - accuracy: 0.5497 - val_loss: 0.6386 - val_accuracy: 0.6014\n",
      "Epoch 53/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6370 - accuracy: 0.5613 - val_loss: 0.6350 - val_accuracy: 0.6463\n",
      "Epoch 54/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6302 - accuracy: 0.6145 - val_loss: 0.6258 - val_accuracy: 0.6697\n",
      "Epoch 55/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6217 - accuracy: 0.6438 - val_loss: 0.6234 - val_accuracy: 0.6840\n",
      "Epoch 56/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6103 - accuracy: 0.6777 - val_loss: 0.6171 - val_accuracy: 0.6750\n",
      "Epoch 57/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5983 - accuracy: 0.6692 - val_loss: 0.6173 - val_accuracy: 0.6373\n",
      "Epoch 58/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5822 - accuracy: 0.6731 - val_loss: 0.6380 - val_accuracy: 0.6014\n",
      "Epoch 59/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.5952 - accuracy: 0.6739 - val_loss: 0.7106 - val_accuracy: 0.4955\n",
      "Epoch 60/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6650 - accuracy: 0.5906 - val_loss: 0.6576 - val_accuracy: 0.6553\n",
      "Epoch 61/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6596 - accuracy: 0.6507 - val_loss: 0.6570 - val_accuracy: 0.6445\n",
      "Epoch 62/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.6430 - accuracy: 0.6315 - val_loss: 0.6276 - val_accuracy: 0.6194\n",
      "Epoch 63/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6075 - accuracy: 0.6369 - val_loss: 0.6378 - val_accuracy: 0.6068\n",
      "Epoch 64/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6107 - accuracy: 0.6261 - val_loss: 0.6196 - val_accuracy: 0.6499\n",
      "Epoch 65/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6041 - accuracy: 0.6554 - val_loss: 0.6248 - val_accuracy: 0.6535\n",
      "Epoch 66/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6072 - accuracy: 0.6500 - val_loss: 0.6215 - val_accuracy: 0.6176\n",
      "Epoch 67/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.5951 - accuracy: 0.6677 - val_loss: 0.6263 - val_accuracy: 0.6176\n",
      "Epoch 68/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5890 - accuracy: 0.6677 - val_loss: 0.5969 - val_accuracy: 0.6697\n",
      "Epoch 69/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5739 - accuracy: 0.6746 - val_loss: 0.6498 - val_accuracy: 0.5871\n",
      "Epoch 70/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.5736 - accuracy: 0.6847 - val_loss: 0.6041 - val_accuracy: 0.6804\n",
      "Epoch 71/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5836 - accuracy: 0.6608 - val_loss: 0.5980 - val_accuracy: 0.6894\n",
      "Epoch 72/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5632 - accuracy: 0.6901 - val_loss: 0.6024 - val_accuracy: 0.6517\n",
      "Epoch 73/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5423 - accuracy: 0.7101 - val_loss: 0.6210 - val_accuracy: 0.6553\n",
      "Epoch 74/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5698 - accuracy: 0.6715 - val_loss: 0.5848 - val_accuracy: 0.6804\n",
      "Epoch 75/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.5447 - accuracy: 0.6893 - val_loss: 0.6066 - val_accuracy: 0.6715\n",
      "Epoch 76/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.5553 - accuracy: 0.6862 - val_loss: 0.6095 - val_accuracy: 0.6732\n",
      "Epoch 77/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5486 - accuracy: 0.6931 - val_loss: 0.6334 - val_accuracy: 0.6750\n",
      "Epoch 78/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.5488 - accuracy: 0.6870 - val_loss: 0.6385 - val_accuracy: 0.6427\n",
      "Epoch 79/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.5333 - accuracy: 0.6870 - val_loss: 0.6291 - val_accuracy: 0.6697\n",
      "Epoch 80/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5164 - accuracy: 0.7093 - val_loss: 0.6236 - val_accuracy: 0.6553\n",
      "Epoch 81/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5032 - accuracy: 0.7155 - val_loss: 0.6364 - val_accuracy: 0.6445\n",
      "Epoch 82/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4938 - accuracy: 0.7247 - val_loss: 0.6467 - val_accuracy: 0.6463\n",
      "Epoch 83/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.4965 - accuracy: 0.7294 - val_loss: 0.6474 - val_accuracy: 0.6050\n",
      "Epoch 84/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4819 - accuracy: 0.7294 - val_loss: 0.6352 - val_accuracy: 0.6589\n",
      "Epoch 85/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.4732 - accuracy: 0.7386 - val_loss: 0.6344 - val_accuracy: 0.6786\n",
      "Epoch 86/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4626 - accuracy: 0.7448 - val_loss: 0.6572 - val_accuracy: 0.6661\n",
      "Epoch 87/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4486 - accuracy: 0.7625 - val_loss: 0.6715 - val_accuracy: 0.6643\n",
      "Epoch 88/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.4360 - accuracy: 0.7641 - val_loss: 0.7322 - val_accuracy: 0.6571\n",
      "Epoch 89/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.4441 - accuracy: 0.7679 - val_loss: 0.6906 - val_accuracy: 0.6535\n",
      "Epoch 90/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4262 - accuracy: 0.7918 - val_loss: 0.6937 - val_accuracy: 0.6284\n",
      "Epoch 91/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.4567 - accuracy: 0.7641 - val_loss: 0.6842 - val_accuracy: 0.6571\n",
      "Epoch 92/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4473 - accuracy: 0.7502 - val_loss: 0.6850 - val_accuracy: 0.6248\n",
      "Epoch 93/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4393 - accuracy: 0.7564 - val_loss: 0.6552 - val_accuracy: 0.6625\n",
      "Epoch 94/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4129 - accuracy: 0.7918 - val_loss: 0.6953 - val_accuracy: 0.6481\n",
      "Epoch 95/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.4211 - accuracy: 0.7795 - val_loss: 0.6442 - val_accuracy: 0.6517\n",
      "Epoch 96/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4090 - accuracy: 0.7926 - val_loss: 0.7113 - val_accuracy: 0.6230\n",
      "Epoch 97/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4137 - accuracy: 0.7857 - val_loss: 0.8536 - val_accuracy: 0.6230\n",
      "Epoch 98/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3917 - accuracy: 0.7826 - val_loss: 0.6926 - val_accuracy: 0.6535\n",
      "Epoch 99/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.4027 - accuracy: 0.8011 - val_loss: 0.7286 - val_accuracy: 0.6553\n",
      "Epoch 100/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3983 - accuracy: 0.8026 - val_loss: 0.7429 - val_accuracy: 0.6230\n",
      "Epoch 101/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.3960 - accuracy: 0.8134 - val_loss: 0.7510 - val_accuracy: 0.6122\n",
      "Epoch 102/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4043 - accuracy: 0.7911 - val_loss: 0.7057 - val_accuracy: 0.6643\n",
      "Epoch 103/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3677 - accuracy: 0.8126 - val_loss: 0.7889 - val_accuracy: 0.6355\n",
      "Epoch 104/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3734 - accuracy: 0.8196 - val_loss: 0.7482 - val_accuracy: 0.6427\n",
      "Epoch 105/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3632 - accuracy: 0.8227 - val_loss: 0.7024 - val_accuracy: 0.6661\n",
      "Epoch 106/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3462 - accuracy: 0.8443 - val_loss: 0.7406 - val_accuracy: 0.6661\n",
      "Epoch 107/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.3321 - accuracy: 0.8396 - val_loss: 0.7909 - val_accuracy: 0.6607\n",
      "Epoch 108/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3103 - accuracy: 0.8527 - val_loss: 0.8074 - val_accuracy: 0.6571\n",
      "Epoch 109/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3073 - accuracy: 0.8589 - val_loss: 0.8451 - val_accuracy: 0.6535\n",
      "Epoch 110/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3063 - accuracy: 0.8581 - val_loss: 0.9177 - val_accuracy: 0.6589\n",
      "Epoch 111/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2832 - accuracy: 0.8720 - val_loss: 1.1454 - val_accuracy: 0.6427\n",
      "Epoch 112/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5090 - accuracy: 0.7926 - val_loss: 1.1000 - val_accuracy: 0.6409\n",
      "Epoch 113/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.6093 - accuracy: 0.7556 - val_loss: 0.7613 - val_accuracy: 0.6463\n",
      "Epoch 114/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4474 - accuracy: 0.7764 - val_loss: 0.7598 - val_accuracy: 0.6302\n",
      "Epoch 115/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.4723 - accuracy: 0.7672 - val_loss: 0.7355 - val_accuracy: 0.5763\n",
      "Epoch 116/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.5355 - accuracy: 0.7224 - val_loss: 0.7415 - val_accuracy: 0.6050\n",
      "Epoch 117/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4771 - accuracy: 0.7672 - val_loss: 0.7024 - val_accuracy: 0.6194\n",
      "Epoch 118/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.4131 - accuracy: 0.8150 - val_loss: 0.6951 - val_accuracy: 0.6409\n",
      "Epoch 119/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3914 - accuracy: 0.8165 - val_loss: 0.6674 - val_accuracy: 0.6320\n",
      "Epoch 120/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3733 - accuracy: 0.8304 - val_loss: 0.6813 - val_accuracy: 0.6373\n",
      "Epoch 121/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3541 - accuracy: 0.8489 - val_loss: 0.7130 - val_accuracy: 0.6463\n",
      "Epoch 122/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.3334 - accuracy: 0.8358 - val_loss: 0.7456 - val_accuracy: 0.6517\n",
      "Epoch 123/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.3003 - accuracy: 0.8674 - val_loss: 0.8520 - val_accuracy: 0.6607\n",
      "Epoch 124/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2935 - accuracy: 0.8736 - val_loss: 0.9049 - val_accuracy: 0.6409\n",
      "Epoch 125/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.2728 - accuracy: 0.8813 - val_loss: 0.9159 - val_accuracy: 0.6409\n",
      "Epoch 126/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2628 - accuracy: 0.8805 - val_loss: 0.9145 - val_accuracy: 0.6373\n",
      "Epoch 127/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2610 - accuracy: 0.8890 - val_loss: 0.8690 - val_accuracy: 0.6571\n",
      "Epoch 128/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2620 - accuracy: 0.8843 - val_loss: 0.8782 - val_accuracy: 0.6571\n",
      "Epoch 129/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2462 - accuracy: 0.8913 - val_loss: 0.9131 - val_accuracy: 0.6571\n",
      "Epoch 130/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2302 - accuracy: 0.9052 - val_loss: 0.9133 - val_accuracy: 0.6643\n",
      "Epoch 131/200\n",
      "1297/1297 [==============================] - 23s 18ms/step - loss: 0.2284 - accuracy: 0.8998 - val_loss: 0.9936 - val_accuracy: 0.6625\n",
      "Epoch 132/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2246 - accuracy: 0.9082 - val_loss: 1.0289 - val_accuracy: 0.6553\n",
      "Epoch 133/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2070 - accuracy: 0.9129 - val_loss: 1.0721 - val_accuracy: 0.6553\n",
      "Epoch 134/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.2064 - accuracy: 0.9152 - val_loss: 1.0562 - val_accuracy: 0.6732\n",
      "Epoch 135/200\n",
      "1297/1297 [==============================] - 23s 17ms/step - loss: 0.1948 - accuracy: 0.9229 - val_loss: 0.9783 - val_accuracy: 0.6607\n",
      "Epoch 136/200\n",
      "1297/1297 [==============================] - 22s 17ms/step - loss: 0.2023 - accuracy: 0.9183 - val_loss: 0.9592 - val_accuracy: 0.6535\n",
      "Epoch 137/200\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_7 = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=200,\n",
    "                    batch_size=1000,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=10, kernel_size=10, strides=2, activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "model.add(layers.MaxPooling2D((10, 10)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=20, kernel_size=5, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((4, 4)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=40, kernel_size=3, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=80, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=160, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "# model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1297 samples, validate on 557 samples\n",
      "Epoch 1/500\n",
      "1297/1297 [==============================] - 5s 4ms/step - loss: 0.0542 - acc: 0.9800 - val_loss: 4.8865 - val_acc: 0.5673\n",
      "Epoch 2/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0526 - acc: 0.9830 - val_loss: 3.8052 - val_acc: 0.6122\n",
      "Epoch 3/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0112 - acc: 0.9931 - val_loss: 4.2008 - val_acc: 0.5978\n",
      "Epoch 4/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0153 - acc: 0.9946 - val_loss: 4.0361 - val_acc: 0.6302\n",
      "Epoch 5/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0066 - acc: 0.9977 - val_loss: 4.1366 - val_acc: 0.6230\n",
      "Epoch 6/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0841 - acc: 0.9738 - val_loss: 4.3061 - val_acc: 0.5763\n",
      "Epoch 7/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.1218 - acc: 0.9653 - val_loss: 3.3528 - val_acc: 0.6104\n",
      "Epoch 8/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0365 - acc: 0.9884 - val_loss: 3.3635 - val_acc: 0.6032\n",
      "Epoch 9/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0114 - acc: 0.9954 - val_loss: 3.3668 - val_acc: 0.6122\n",
      "Epoch 10/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0083 - acc: 0.9969 - val_loss: 3.4501 - val_acc: 0.6140\n",
      "Epoch 11/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0100 - acc: 0.9946 - val_loss: 3.5490 - val_acc: 0.6122\n",
      "Epoch 12/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0294 - acc: 0.9861 - val_loss: 3.3413 - val_acc: 0.6140\n",
      "Epoch 13/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0271 - acc: 0.9830 - val_loss: 3.4524 - val_acc: 0.5925\n",
      "Epoch 14/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0129 - acc: 0.9931 - val_loss: 3.6805 - val_acc: 0.6014\n",
      "Epoch 15/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0100 - acc: 0.9954 - val_loss: 3.5620 - val_acc: 0.6050\n",
      "Epoch 16/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0070 - acc: 0.9977 - val_loss: 3.5593 - val_acc: 0.6104\n",
      "Epoch 17/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0063 - acc: 0.9977 - val_loss: 3.7795 - val_acc: 0.6194\n",
      "Epoch 18/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0032 - acc: 0.9992 - val_loss: 3.8217 - val_acc: 0.6086\n",
      "Epoch 19/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.2594 - acc: 0.9044 - val_loss: 1.3718 - val_acc: 0.5440\n",
      "Epoch 20/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.1091 - acc: 0.9491 - val_loss: 2.5084 - val_acc: 0.5925\n",
      "Epoch 21/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0124 - acc: 0.9961 - val_loss: 3.1336 - val_acc: 0.5961\n",
      "Epoch 22/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0090 - acc: 0.9961 - val_loss: 3.3260 - val_acc: 0.5996\n",
      "Epoch 23/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0109 - acc: 0.9954 - val_loss: 3.4304 - val_acc: 0.5978\n",
      "Epoch 24/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0098 - acc: 0.9961 - val_loss: 3.6061 - val_acc: 0.6050\n",
      "Epoch 25/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0038 - acc: 0.9992 - val_loss: 3.6194 - val_acc: 0.6014\n",
      "Epoch 26/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0952 - acc: 0.9692 - val_loss: 2.7855 - val_acc: 0.6050\n",
      "Epoch 27/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0720 - acc: 0.9776 - val_loss: 2.5212 - val_acc: 0.6230\n",
      "Epoch 28/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0153 - acc: 0.9907 - val_loss: 2.8383 - val_acc: 0.6176\n",
      "Epoch 29/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0052 - acc: 0.9992 - val_loss: 3.0252 - val_acc: 0.6158\n",
      "Epoch 30/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0039 - acc: 0.9992 - val_loss: 3.1664 - val_acc: 0.6068\n",
      "Epoch 31/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0033 - acc: 0.9992 - val_loss: 3.2672 - val_acc: 0.6032\n",
      "Epoch 32/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.9992 - val_loss: 3.3670 - val_acc: 0.6050\n",
      "Epoch 33/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 3.4478 - val_acc: 0.6032\n",
      "Epoch 34/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0468 - acc: 0.9900 - val_loss: 3.4187 - val_acc: 0.5961\n",
      "Epoch 35/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.3658 - acc: 0.9067 - val_loss: 2.0230 - val_acc: 0.5925\n",
      "Epoch 36/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0486 - acc: 0.9823 - val_loss: 2.8499 - val_acc: 0.5799\n",
      "Epoch 37/500\n",
      "1297/1297 [==============================] - 4s 3ms/step - loss: 0.0105 - acc: 0.9961 - val_loss: 3.0220 - val_acc: 0.6122\n",
      "Epoch 38/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0088 - acc: 0.9969 - val_loss: 3.1904 - val_acc: 0.5925\n",
      "Epoch 39/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.3246 - acc: 0.8975 - val_loss: 2.0007 - val_acc: 0.5943\n",
      "Epoch 40/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0669 - acc: 0.9815 - val_loss: 2.5389 - val_acc: 0.5817\n",
      "Epoch 41/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0384 - acc: 0.9846 - val_loss: 2.4448 - val_acc: 0.6086\n",
      "Epoch 42/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0288 - acc: 0.9892 - val_loss: 2.6544 - val_acc: 0.5925\n",
      "Epoch 43/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0257 - acc: 0.9900 - val_loss: 2.8005 - val_acc: 0.5978\n",
      "Epoch 44/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0178 - acc: 0.9884 - val_loss: 2.9891 - val_acc: 0.5943\n",
      "Epoch 45/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0097 - acc: 0.9931 - val_loss: 3.1611 - val_acc: 0.5925\n",
      "Epoch 46/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0059 - acc: 0.9992 - val_loss: 3.2269 - val_acc: 0.5996\n",
      "Epoch 47/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0042 - acc: 0.9992 - val_loss: 3.2916 - val_acc: 0.5996\n",
      "Epoch 48/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0102 - acc: 0.9938 - val_loss: 3.1307 - val_acc: 0.5907\n",
      "Epoch 49/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0987 - acc: 0.9522 - val_loss: 2.6610 - val_acc: 0.5601\n",
      "Epoch 50/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0424 - acc: 0.9815 - val_loss: 2.7463 - val_acc: 0.5907\n",
      "Epoch 51/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0141 - acc: 0.9907 - val_loss: 3.0980 - val_acc: 0.5943\n",
      "Epoch 52/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0097 - acc: 0.9946 - val_loss: 3.2145 - val_acc: 0.6068\n",
      "Epoch 53/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0576 - acc: 0.9738 - val_loss: 3.0086 - val_acc: 0.5548\n",
      "Epoch 54/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0582 - acc: 0.9738 - val_loss: 3.0022 - val_acc: 0.6068\n",
      "Epoch 55/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0068 - acc: 0.9977 - val_loss: 3.0723 - val_acc: 0.6140\n",
      "Epoch 56/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0122 - acc: 0.9977 - val_loss: 3.0839 - val_acc: 0.6104\n",
      "Epoch 57/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0101 - acc: 0.9961 - val_loss: 3.2881 - val_acc: 0.6212\n",
      "Epoch 58/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0450 - acc: 0.9807 - val_loss: 2.6459 - val_acc: 0.5727\n",
      "Epoch 59/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0814 - acc: 0.9630 - val_loss: 2.8003 - val_acc: 0.6068\n",
      "Epoch 60/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0084 - acc: 0.9961 - val_loss: 3.0562 - val_acc: 0.6068\n",
      "Epoch 61/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0150 - acc: 0.9900 - val_loss: 3.0624 - val_acc: 0.6104\n",
      "Epoch 62/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0166 - acc: 0.9884 - val_loss: 3.3788 - val_acc: 0.6104\n",
      "Epoch 63/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0086 - acc: 0.9977 - val_loss: 3.3620 - val_acc: 0.6284\n",
      "Epoch 64/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0077 - acc: 0.9961 - val_loss: 3.4234 - val_acc: 0.6302\n",
      "Epoch 65/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0209 - acc: 0.9938 - val_loss: 3.2064 - val_acc: 0.5907\n",
      "Epoch 66/500\n",
      "1297/1297 [==============================] - 2s 2ms/step - loss: 0.0218 - acc: 0.9869 - val_loss: 3.2074 - val_acc: 0.6086\n",
      "Epoch 67/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0869 - acc: 0.9668 - val_loss: 2.8276 - val_acc: 0.5619\n",
      "Epoch 68/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0555 - acc: 0.9807 - val_loss: 2.6582 - val_acc: 0.5853\n",
      "Epoch 69/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0174 - acc: 0.9900 - val_loss: 2.9639 - val_acc: 0.6104\n",
      "Epoch 70/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0115 - acc: 0.9931 - val_loss: 2.8999 - val_acc: 0.6194\n",
      "Epoch 71/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0052 - acc: 0.9992 - val_loss: 2.9881 - val_acc: 0.6176\n",
      "Epoch 72/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0628 - acc: 0.9707 - val_loss: 2.9394 - val_acc: 0.5619\n",
      "Epoch 73/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0514 - acc: 0.9800 - val_loss: 3.0135 - val_acc: 0.5889\n",
      "Epoch 74/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0300 - acc: 0.9854 - val_loss: 2.9444 - val_acc: 0.6212\n",
      "Epoch 75/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0067 - acc: 0.9985 - val_loss: 3.1245 - val_acc: 0.6266\n",
      "Epoch 76/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0108 - acc: 0.9954 - val_loss: 3.2596 - val_acc: 0.6122\n",
      "Epoch 77/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0693 - acc: 0.9638 - val_loss: 2.8438 - val_acc: 0.5889\n",
      "Epoch 78/500\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 0.0296 - acc: 0.9869 - val_loss: 3.0837 - val_acc: 0.5853\n",
      "Epoch 79/500\n",
      "1024/1297 [======================>.......] - ETA: 0s - loss: 0.1132 - acc: 0.9443"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-dc19821f9d5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                     validation_split=0.3)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_8 = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=500,\n",
    "                    batch_size=16,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model1 = models.Sequential()\n",
    "model1.add(layers.Conv2D(filters=10, kernel_size=10, strides=2, activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "model1.add(layers.MaxPooling2D((10, 10)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=10, kernel_size=5, strides=2,activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((4, 4)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=10, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=10, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=10, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "\n",
    "model1.add(layers.Flatten())\n",
    "# model.add(layers.Dropout(0.2))\n",
    "model1.add(layers.Dense(20, activation='relu'))\n",
    "model1.add(layers.Dense(100, activation='relu'))\n",
    "model1.add(layers.Dense(200, activation='relu'))\n",
    "# model.add(layers.Dense(200, activation='relu'))\n",
    "# model.add(layers.Dense(200, activation='relu'))\n",
    "# model.add(layers.Dense(200, activation='relu'))\n",
    "model1.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 1287 samples, validate on 552 samples\n",
      "Epoch 1/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.8998 - acc: 0.5237 - val_loss: 0.7195 - val_acc: 0.5996\n",
      "Epoch 2/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.7514 - acc: 0.5346 - val_loss: 0.7020 - val_acc: 0.5725\n",
      "Epoch 3/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.7234 - acc: 0.5509 - val_loss: 0.6960 - val_acc: 0.5652\n",
      "Epoch 4/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6999 - acc: 0.5470 - val_loss: 0.6923 - val_acc: 0.5888\n",
      "Epoch 5/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6910 - acc: 0.5548 - val_loss: 0.6902 - val_acc: 0.5634\n",
      "Epoch 6/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6849 - acc: 0.5641 - val_loss: 0.6871 - val_acc: 0.5652\n",
      "Epoch 7/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6770 - acc: 0.5750 - val_loss: 0.6993 - val_acc: 0.5580\n",
      "Epoch 8/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6743 - acc: 0.5851 - val_loss: 0.6924 - val_acc: 0.5543\n",
      "Epoch 9/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6680 - acc: 0.5828 - val_loss: 0.7028 - val_acc: 0.5362\n",
      "Epoch 10/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6641 - acc: 0.5913 - val_loss: 0.6760 - val_acc: 0.5562\n",
      "Epoch 11/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6662 - acc: 0.5851 - val_loss: 0.6756 - val_acc: 0.5743\n",
      "Epoch 12/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6573 - acc: 0.5952 - val_loss: 0.6808 - val_acc: 0.5598\n",
      "Epoch 13/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6568 - acc: 0.5967 - val_loss: 0.6776 - val_acc: 0.5598\n",
      "Epoch 14/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6537 - acc: 0.6037 - val_loss: 0.6999 - val_acc: 0.5399\n",
      "Epoch 15/1200\n",
      "1287/1287 [==============================] - 3s 3ms/step - loss: 0.6490 - acc: 0.6006 - val_loss: 0.6677 - val_acc: 0.5779\n",
      "Epoch 16/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.6503 - acc: 0.6099 - val_loss: 0.6649 - val_acc: 0.5779\n",
      "Epoch 17/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6469 - acc: 0.6154 - val_loss: 0.6647 - val_acc: 0.5743\n",
      "Epoch 18/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6486 - acc: 0.6061 - val_loss: 0.6622 - val_acc: 0.5833\n",
      "Epoch 19/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6421 - acc: 0.6092 - val_loss: 0.6672 - val_acc: 0.5870\n",
      "Epoch 20/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6405 - acc: 0.6068 - val_loss: 0.6643 - val_acc: 0.5797\n",
      "Epoch 21/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6381 - acc: 0.6239 - val_loss: 0.6675 - val_acc: 0.5906\n",
      "Epoch 22/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6403 - acc: 0.6146 - val_loss: 0.6631 - val_acc: 0.5707\n",
      "Epoch 23/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6384 - acc: 0.6216 - val_loss: 0.6615 - val_acc: 0.5707\n",
      "Epoch 24/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6337 - acc: 0.6255 - val_loss: 0.6630 - val_acc: 0.5996\n",
      "Epoch 25/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6315 - acc: 0.6247 - val_loss: 0.6621 - val_acc: 0.5924\n",
      "Epoch 26/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6312 - acc: 0.6364 - val_loss: 0.6603 - val_acc: 0.5815\n",
      "Epoch 27/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6295 - acc: 0.6224 - val_loss: 0.6662 - val_acc: 0.5906\n",
      "Epoch 28/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6303 - acc: 0.6232 - val_loss: 0.6597 - val_acc: 0.5870\n",
      "Epoch 29/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6257 - acc: 0.6333 - val_loss: 0.6690 - val_acc: 0.5779\n",
      "Epoch 30/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6275 - acc: 0.6364 - val_loss: 0.6616 - val_acc: 0.5924\n",
      "Epoch 31/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6245 - acc: 0.6270 - val_loss: 0.6594 - val_acc: 0.5815\n",
      "Epoch 32/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6256 - acc: 0.6317 - val_loss: 0.6625 - val_acc: 0.5978\n",
      "Epoch 33/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6228 - acc: 0.6426 - val_loss: 0.6584 - val_acc: 0.5942\n",
      "Epoch 34/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6208 - acc: 0.6348 - val_loss: 0.6797 - val_acc: 0.5580\n",
      "Epoch 35/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6197 - acc: 0.6387 - val_loss: 0.6585 - val_acc: 0.6014\n",
      "Epoch 36/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6167 - acc: 0.6418 - val_loss: 0.6576 - val_acc: 0.5960\n",
      "Epoch 37/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6225 - acc: 0.6371 - val_loss: 0.6562 - val_acc: 0.5960\n",
      "Epoch 38/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6162 - acc: 0.6434 - val_loss: 0.6591 - val_acc: 0.5906\n",
      "Epoch 39/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6120 - acc: 0.6402 - val_loss: 0.6580 - val_acc: 0.5870\n",
      "Epoch 40/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6199 - acc: 0.6418 - val_loss: 0.6617 - val_acc: 0.6014\n",
      "Epoch 41/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6113 - acc: 0.6480 - val_loss: 0.6568 - val_acc: 0.6014\n",
      "Epoch 42/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6097 - acc: 0.6519 - val_loss: 0.6576 - val_acc: 0.6051\n",
      "Epoch 43/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6117 - acc: 0.6371 - val_loss: 0.6563 - val_acc: 0.5960\n",
      "Epoch 44/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6092 - acc: 0.6511 - val_loss: 0.6599 - val_acc: 0.6069\n",
      "Epoch 45/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6096 - acc: 0.6465 - val_loss: 0.6572 - val_acc: 0.5996\n",
      "Epoch 46/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6087 - acc: 0.6434 - val_loss: 0.6624 - val_acc: 0.6014\n",
      "Epoch 47/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6056 - acc: 0.6503 - val_loss: 0.6562 - val_acc: 0.6087\n",
      "Epoch 48/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6087 - acc: 0.6395 - val_loss: 0.6542 - val_acc: 0.6087\n",
      "Epoch 49/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6064 - acc: 0.6449 - val_loss: 0.6576 - val_acc: 0.6033\n",
      "Epoch 50/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6015 - acc: 0.6542 - val_loss: 0.6609 - val_acc: 0.5996\n",
      "Epoch 51/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6025 - acc: 0.6589 - val_loss: 0.6565 - val_acc: 0.6051\n",
      "Epoch 52/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6001 - acc: 0.6636 - val_loss: 0.6567 - val_acc: 0.6033\n",
      "Epoch 53/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5988 - acc: 0.6628 - val_loss: 0.6564 - val_acc: 0.6141\n",
      "Epoch 54/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5979 - acc: 0.6667 - val_loss: 0.6544 - val_acc: 0.6087\n",
      "Epoch 55/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5985 - acc: 0.6566 - val_loss: 0.6542 - val_acc: 0.6087\n",
      "Epoch 56/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5997 - acc: 0.6597 - val_loss: 0.6538 - val_acc: 0.6051\n",
      "Epoch 57/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.6011 - acc: 0.6550 - val_loss: 0.6593 - val_acc: 0.5996\n",
      "Epoch 58/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5949 - acc: 0.6713 - val_loss: 0.6550 - val_acc: 0.6033\n",
      "Epoch 59/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5949 - acc: 0.6581 - val_loss: 0.6596 - val_acc: 0.5906\n",
      "Epoch 60/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5944 - acc: 0.6636 - val_loss: 0.6529 - val_acc: 0.6141\n",
      "Epoch 61/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5949 - acc: 0.6597 - val_loss: 0.6523 - val_acc: 0.6123\n",
      "Epoch 62/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5894 - acc: 0.6791 - val_loss: 0.6549 - val_acc: 0.6141\n",
      "Epoch 63/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5927 - acc: 0.6667 - val_loss: 0.6526 - val_acc: 0.6214\n",
      "Epoch 64/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5948 - acc: 0.6550 - val_loss: 0.6522 - val_acc: 0.6141\n",
      "Epoch 65/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5861 - acc: 0.6744 - val_loss: 0.6661 - val_acc: 0.5688\n",
      "Epoch 66/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5872 - acc: 0.6752 - val_loss: 0.6579 - val_acc: 0.5978\n",
      "Epoch 67/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5856 - acc: 0.6791 - val_loss: 0.6545 - val_acc: 0.6159\n",
      "Epoch 68/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5847 - acc: 0.6768 - val_loss: 0.6570 - val_acc: 0.6069\n",
      "Epoch 69/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5844 - acc: 0.6791 - val_loss: 0.6560 - val_acc: 0.6141\n",
      "Epoch 70/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5832 - acc: 0.6760 - val_loss: 0.6506 - val_acc: 0.6159\n",
      "Epoch 71/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5828 - acc: 0.6690 - val_loss: 0.6603 - val_acc: 0.5942\n",
      "Epoch 72/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5877 - acc: 0.6729 - val_loss: 0.6619 - val_acc: 0.5924\n",
      "Epoch 73/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5813 - acc: 0.6775 - val_loss: 0.6546 - val_acc: 0.6087\n",
      "Epoch 74/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5796 - acc: 0.6884 - val_loss: 0.6678 - val_acc: 0.5725\n",
      "Epoch 75/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5785 - acc: 0.6876 - val_loss: 0.6539 - val_acc: 0.6123\n",
      "Epoch 76/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5774 - acc: 0.6807 - val_loss: 0.6562 - val_acc: 0.6014\n",
      "Epoch 77/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5748 - acc: 0.6869 - val_loss: 0.6554 - val_acc: 0.6141\n",
      "Epoch 78/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5742 - acc: 0.6931 - val_loss: 0.6709 - val_acc: 0.5580\n",
      "Epoch 79/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5749 - acc: 0.6876 - val_loss: 0.6545 - val_acc: 0.6123\n",
      "Epoch 80/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5721 - acc: 0.6908 - val_loss: 0.6522 - val_acc: 0.6178\n",
      "Epoch 81/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5736 - acc: 0.6954 - val_loss: 0.6539 - val_acc: 0.6196\n",
      "Epoch 82/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.5705 - acc: 0.6923 - val_loss: 0.6617 - val_acc: 0.5888\n",
      "Epoch 83/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5697 - acc: 0.6892 - val_loss: 0.6537 - val_acc: 0.6178\n",
      "Epoch 84/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5684 - acc: 0.6985 - val_loss: 0.6587 - val_acc: 0.5960\n",
      "Epoch 85/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5693 - acc: 0.6830 - val_loss: 0.6640 - val_acc: 0.5833\n",
      "Epoch 86/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5673 - acc: 0.6853 - val_loss: 0.6669 - val_acc: 0.5815\n",
      "Epoch 87/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5668 - acc: 0.6946 - val_loss: 0.6499 - val_acc: 0.6196\n",
      "Epoch 88/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5700 - acc: 0.6923 - val_loss: 0.6504 - val_acc: 0.6232\n",
      "Epoch 89/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5707 - acc: 0.6869 - val_loss: 0.6523 - val_acc: 0.6178\n",
      "Epoch 90/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5624 - acc: 0.6939 - val_loss: 0.6549 - val_acc: 0.6105\n",
      "Epoch 91/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5620 - acc: 0.6970 - val_loss: 0.6593 - val_acc: 0.5996\n",
      "Epoch 92/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5609 - acc: 0.6923 - val_loss: 0.6529 - val_acc: 0.6196\n",
      "Epoch 93/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5615 - acc: 0.6931 - val_loss: 0.6544 - val_acc: 0.6105\n",
      "Epoch 94/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5560 - acc: 0.7024 - val_loss: 0.6552 - val_acc: 0.6250\n",
      "Epoch 95/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5565 - acc: 0.6993 - val_loss: 0.6528 - val_acc: 0.6196\n",
      "Epoch 96/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5559 - acc: 0.6993 - val_loss: 0.6526 - val_acc: 0.6395\n",
      "Epoch 97/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5554 - acc: 0.6993 - val_loss: 0.6594 - val_acc: 0.5996\n",
      "Epoch 98/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5536 - acc: 0.7071 - val_loss: 0.6548 - val_acc: 0.6159\n",
      "Epoch 99/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5542 - acc: 0.7078 - val_loss: 0.6603 - val_acc: 0.5960\n",
      "Epoch 100/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5515 - acc: 0.7078 - val_loss: 0.6543 - val_acc: 0.6123\n",
      "Epoch 101/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5542 - acc: 0.6962 - val_loss: 0.6579 - val_acc: 0.6087\n",
      "Epoch 102/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5483 - acc: 0.7211 - val_loss: 0.6592 - val_acc: 0.5942\n",
      "Epoch 103/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5516 - acc: 0.6970 - val_loss: 0.6691 - val_acc: 0.5833\n",
      "Epoch 104/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5472 - acc: 0.7148 - val_loss: 0.6544 - val_acc: 0.6159\n",
      "Epoch 105/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5487 - acc: 0.7071 - val_loss: 0.6577 - val_acc: 0.6033\n",
      "Epoch 106/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5465 - acc: 0.7078 - val_loss: 0.6583 - val_acc: 0.6069\n",
      "Epoch 107/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5447 - acc: 0.7063 - val_loss: 0.6584 - val_acc: 0.6087\n",
      "Epoch 108/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5423 - acc: 0.7172 - val_loss: 0.6574 - val_acc: 0.6141\n",
      "Epoch 109/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5492 - acc: 0.7032 - val_loss: 0.6654 - val_acc: 0.5833\n",
      "Epoch 110/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5399 - acc: 0.7125 - val_loss: 0.6585 - val_acc: 0.6123\n",
      "Epoch 111/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5409 - acc: 0.7133 - val_loss: 0.6597 - val_acc: 0.6087\n",
      "Epoch 112/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5373 - acc: 0.7187 - val_loss: 0.6662 - val_acc: 0.5797\n",
      "Epoch 113/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5375 - acc: 0.7211 - val_loss: 0.6646 - val_acc: 0.5851\n",
      "Epoch 114/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5424 - acc: 0.7055 - val_loss: 0.6608 - val_acc: 0.6322\n",
      "Epoch 115/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5400 - acc: 0.6977 - val_loss: 0.6691 - val_acc: 0.5725\n",
      "Epoch 116/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5358 - acc: 0.7164 - val_loss: 0.6580 - val_acc: 0.6250\n",
      "Epoch 117/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5344 - acc: 0.7164 - val_loss: 0.6598 - val_acc: 0.6286\n",
      "Epoch 118/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5360 - acc: 0.7187 - val_loss: 0.6700 - val_acc: 0.5851\n",
      "Epoch 119/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5296 - acc: 0.7304 - val_loss: 0.6654 - val_acc: 0.6123\n",
      "Epoch 120/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5293 - acc: 0.7288 - val_loss: 0.6651 - val_acc: 0.6341\n",
      "Epoch 121/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5287 - acc: 0.7257 - val_loss: 0.6644 - val_acc: 0.6069\n",
      "Epoch 122/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5341 - acc: 0.7156 - val_loss: 0.6740 - val_acc: 0.5761\n",
      "Epoch 123/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5259 - acc: 0.7257 - val_loss: 0.6797 - val_acc: 0.5743\n",
      "Epoch 124/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5244 - acc: 0.7296 - val_loss: 0.6663 - val_acc: 0.6069\n",
      "Epoch 125/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5244 - acc: 0.7350 - val_loss: 0.6855 - val_acc: 0.5725\n",
      "Epoch 126/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5211 - acc: 0.7405 - val_loss: 0.6716 - val_acc: 0.5924\n",
      "Epoch 127/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5222 - acc: 0.7296 - val_loss: 0.6675 - val_acc: 0.6105\n",
      "Epoch 128/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5213 - acc: 0.7335 - val_loss: 0.6674 - val_acc: 0.6178\n",
      "Epoch 129/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5182 - acc: 0.7420 - val_loss: 0.6796 - val_acc: 0.5815\n",
      "Epoch 130/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5192 - acc: 0.7374 - val_loss: 0.6713 - val_acc: 0.5960\n",
      "Epoch 131/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5193 - acc: 0.7366 - val_loss: 0.6706 - val_acc: 0.5996\n",
      "Epoch 132/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5155 - acc: 0.7389 - val_loss: 0.6786 - val_acc: 0.5851\n",
      "Epoch 133/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5163 - acc: 0.7436 - val_loss: 0.6694 - val_acc: 0.6014\n",
      "Epoch 134/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5129 - acc: 0.7420 - val_loss: 0.6745 - val_acc: 0.5906\n",
      "Epoch 135/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5112 - acc: 0.7498 - val_loss: 0.6689 - val_acc: 0.6051\n",
      "Epoch 136/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5142 - acc: 0.7249 - val_loss: 0.6665 - val_acc: 0.6250\n",
      "Epoch 137/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5104 - acc: 0.7436 - val_loss: 0.6707 - val_acc: 0.5996\n",
      "Epoch 138/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5096 - acc: 0.7514 - val_loss: 0.6757 - val_acc: 0.5833\n",
      "Epoch 139/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5163 - acc: 0.7350 - val_loss: 0.6684 - val_acc: 0.6359\n",
      "Epoch 140/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5073 - acc: 0.7568 - val_loss: 0.6682 - val_acc: 0.6141\n",
      "Epoch 141/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5090 - acc: 0.7382 - val_loss: 0.6700 - val_acc: 0.6322\n",
      "Epoch 142/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5018 - acc: 0.7599 - val_loss: 0.6763 - val_acc: 0.6033\n",
      "Epoch 143/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5019 - acc: 0.7529 - val_loss: 0.6729 - val_acc: 0.5978\n",
      "Epoch 144/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5010 - acc: 0.7599 - val_loss: 0.6761 - val_acc: 0.6105\n",
      "Epoch 145/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4982 - acc: 0.7584 - val_loss: 0.6805 - val_acc: 0.5779\n",
      "Epoch 146/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4982 - acc: 0.7607 - val_loss: 0.6810 - val_acc: 0.5870\n",
      "Epoch 147/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4959 - acc: 0.7591 - val_loss: 0.6755 - val_acc: 0.6014\n",
      "Epoch 148/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.4939 - acc: 0.7630 - val_loss: 0.6730 - val_acc: 0.6214\n",
      "Epoch 149/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.4975 - acc: 0.7490 - val_loss: 0.6833 - val_acc: 0.5815\n",
      "Epoch 150/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4958 - acc: 0.7529 - val_loss: 0.6764 - val_acc: 0.6123\n",
      "Epoch 151/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4914 - acc: 0.7638 - val_loss: 0.6806 - val_acc: 0.6232\n",
      "Epoch 152/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4906 - acc: 0.7622 - val_loss: 0.6796 - val_acc: 0.6105\n",
      "Epoch 153/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4868 - acc: 0.7677 - val_loss: 0.6856 - val_acc: 0.6069\n",
      "Epoch 154/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4889 - acc: 0.7669 - val_loss: 0.6822 - val_acc: 0.6268\n",
      "Epoch 155/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4921 - acc: 0.7646 - val_loss: 0.6865 - val_acc: 0.6395\n",
      "Epoch 156/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4971 - acc: 0.7490 - val_loss: 0.6759 - val_acc: 0.6087\n",
      "Epoch 157/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4849 - acc: 0.7638 - val_loss: 0.6804 - val_acc: 0.5960\n",
      "Epoch 158/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4828 - acc: 0.7692 - val_loss: 0.6808 - val_acc: 0.5960\n",
      "Epoch 159/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4815 - acc: 0.7723 - val_loss: 0.6870 - val_acc: 0.5942\n",
      "Epoch 160/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4803 - acc: 0.7669 - val_loss: 0.6887 - val_acc: 0.6413\n",
      "Epoch 161/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4827 - acc: 0.7646 - val_loss: 0.6924 - val_acc: 0.5870\n",
      "Epoch 162/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4792 - acc: 0.7677 - val_loss: 0.6910 - val_acc: 0.5888\n",
      "Epoch 163/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4762 - acc: 0.7716 - val_loss: 0.6801 - val_acc: 0.6069\n",
      "Epoch 164/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4788 - acc: 0.7692 - val_loss: 0.6928 - val_acc: 0.6304\n",
      "Epoch 165/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4742 - acc: 0.7669 - val_loss: 0.6903 - val_acc: 0.5924\n",
      "Epoch 166/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4739 - acc: 0.7677 - val_loss: 0.7002 - val_acc: 0.5960\n",
      "Epoch 167/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4709 - acc: 0.7778 - val_loss: 0.6865 - val_acc: 0.5942\n",
      "Epoch 168/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4690 - acc: 0.7863 - val_loss: 0.6957 - val_acc: 0.6413\n",
      "Epoch 169/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4704 - acc: 0.7661 - val_loss: 0.6910 - val_acc: 0.6069\n",
      "Epoch 170/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4669 - acc: 0.7824 - val_loss: 0.6918 - val_acc: 0.6214\n",
      "Epoch 171/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4691 - acc: 0.7793 - val_loss: 0.6890 - val_acc: 0.6141\n",
      "Epoch 172/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4668 - acc: 0.7754 - val_loss: 0.6950 - val_acc: 0.6250\n",
      "Epoch 173/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4707 - acc: 0.7739 - val_loss: 0.7004 - val_acc: 0.5870\n",
      "Epoch 174/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4640 - acc: 0.7770 - val_loss: 0.6900 - val_acc: 0.6214\n",
      "Epoch 175/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4624 - acc: 0.7793 - val_loss: 0.6972 - val_acc: 0.6051\n",
      "Epoch 176/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4598 - acc: 0.7910 - val_loss: 0.7008 - val_acc: 0.5942\n",
      "Epoch 177/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4589 - acc: 0.7817 - val_loss: 0.6968 - val_acc: 0.5924\n",
      "Epoch 178/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4604 - acc: 0.7824 - val_loss: 0.7025 - val_acc: 0.5851\n",
      "Epoch 179/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4586 - acc: 0.7817 - val_loss: 0.7042 - val_acc: 0.5978\n",
      "Epoch 180/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4548 - acc: 0.7918 - val_loss: 0.7029 - val_acc: 0.5960\n",
      "Epoch 181/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4585 - acc: 0.7855 - val_loss: 0.7109 - val_acc: 0.5833\n",
      "Epoch 182/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4586 - acc: 0.7832 - val_loss: 0.6944 - val_acc: 0.6431\n",
      "Epoch 183/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4567 - acc: 0.7786 - val_loss: 0.6989 - val_acc: 0.5978\n",
      "Epoch 184/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4526 - acc: 0.7809 - val_loss: 0.7009 - val_acc: 0.5888\n",
      "Epoch 185/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4546 - acc: 0.7824 - val_loss: 0.7079 - val_acc: 0.5960\n",
      "Epoch 186/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4528 - acc: 0.7840 - val_loss: 0.7044 - val_acc: 0.5833\n",
      "Epoch 187/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4487 - acc: 0.7910 - val_loss: 0.7003 - val_acc: 0.6105\n",
      "Epoch 188/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4491 - acc: 0.7918 - val_loss: 0.7079 - val_acc: 0.6395\n",
      "Epoch 189/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4475 - acc: 0.7925 - val_loss: 0.7009 - val_acc: 0.5996\n",
      "Epoch 190/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4445 - acc: 0.7941 - val_loss: 0.7054 - val_acc: 0.6214\n",
      "Epoch 191/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4419 - acc: 0.7918 - val_loss: 0.7158 - val_acc: 0.5815\n",
      "Epoch 192/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4470 - acc: 0.7956 - val_loss: 0.7085 - val_acc: 0.5978\n",
      "Epoch 193/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4438 - acc: 0.7894 - val_loss: 0.7097 - val_acc: 0.6105\n",
      "Epoch 194/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4393 - acc: 0.7949 - val_loss: 0.7074 - val_acc: 0.6105\n",
      "Epoch 195/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4380 - acc: 0.7980 - val_loss: 0.7162 - val_acc: 0.6087\n",
      "Epoch 196/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4368 - acc: 0.7988 - val_loss: 0.7144 - val_acc: 0.5906\n",
      "Epoch 197/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4347 - acc: 0.8003 - val_loss: 0.7220 - val_acc: 0.6069\n",
      "Epoch 198/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4337 - acc: 0.8026 - val_loss: 0.7138 - val_acc: 0.5996\n",
      "Epoch 199/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4322 - acc: 0.8050 - val_loss: 0.7210 - val_acc: 0.6304\n",
      "Epoch 200/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4329 - acc: 0.7949 - val_loss: 0.7169 - val_acc: 0.5978\n",
      "Epoch 201/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4291 - acc: 0.8034 - val_loss: 0.7173 - val_acc: 0.6159\n",
      "Epoch 202/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4364 - acc: 0.7995 - val_loss: 0.7319 - val_acc: 0.5797\n",
      "Epoch 203/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4294 - acc: 0.8019 - val_loss: 0.7180 - val_acc: 0.5924\n",
      "Epoch 204/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4270 - acc: 0.8159 - val_loss: 0.7259 - val_acc: 0.6196\n",
      "Epoch 205/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4296 - acc: 0.7995 - val_loss: 0.7166 - val_acc: 0.6123\n",
      "Epoch 206/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4351 - acc: 0.7894 - val_loss: 0.7338 - val_acc: 0.5870\n",
      "Epoch 207/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4314 - acc: 0.7995 - val_loss: 0.7255 - val_acc: 0.5924\n",
      "Epoch 208/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4265 - acc: 0.8034 - val_loss: 0.7239 - val_acc: 0.5942\n",
      "Epoch 209/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4230 - acc: 0.8042 - val_loss: 0.7219 - val_acc: 0.6014\n",
      "Epoch 210/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4223 - acc: 0.8135 - val_loss: 0.7305 - val_acc: 0.5797\n",
      "Epoch 211/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4212 - acc: 0.8089 - val_loss: 0.7207 - val_acc: 0.5942\n",
      "Epoch 212/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4179 - acc: 0.8104 - val_loss: 0.7445 - val_acc: 0.5833\n",
      "Epoch 213/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4170 - acc: 0.8104 - val_loss: 0.7372 - val_acc: 0.6141\n",
      "Epoch 214/1200\n",
      "1287/1287 [==============================] - 3s 3ms/step - loss: 0.4171 - acc: 0.8057 - val_loss: 0.7392 - val_acc: 0.5779\n",
      "Epoch 215/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.4169 - acc: 0.8127 - val_loss: 0.7354 - val_acc: 0.6051\n",
      "Epoch 216/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4123 - acc: 0.8135 - val_loss: 0.7431 - val_acc: 0.5978\n",
      "Epoch 217/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4110 - acc: 0.8197 - val_loss: 0.7277 - val_acc: 0.6069\n",
      "Epoch 218/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4121 - acc: 0.8112 - val_loss: 0.7361 - val_acc: 0.5888\n",
      "Epoch 219/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4105 - acc: 0.8182 - val_loss: 0.7406 - val_acc: 0.5906\n",
      "Epoch 220/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4118 - acc: 0.8182 - val_loss: 0.7414 - val_acc: 0.6268\n",
      "Epoch 221/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4110 - acc: 0.8120 - val_loss: 0.7513 - val_acc: 0.5833\n",
      "Epoch 222/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4122 - acc: 0.8065 - val_loss: 0.7443 - val_acc: 0.6159\n",
      "Epoch 223/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4073 - acc: 0.8174 - val_loss: 0.7220 - val_acc: 0.6087\n",
      "Epoch 224/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4040 - acc: 0.8197 - val_loss: 0.7413 - val_acc: 0.5978\n",
      "Epoch 225/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4026 - acc: 0.8221 - val_loss: 0.7359 - val_acc: 0.5942\n",
      "Epoch 226/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4021 - acc: 0.8275 - val_loss: 0.7381 - val_acc: 0.6033\n",
      "Epoch 227/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4019 - acc: 0.8151 - val_loss: 0.7431 - val_acc: 0.6069\n",
      "Epoch 228/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4011 - acc: 0.8127 - val_loss: 0.7487 - val_acc: 0.5797\n",
      "Epoch 229/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4011 - acc: 0.8190 - val_loss: 0.7459 - val_acc: 0.5960\n",
      "Epoch 230/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4065 - acc: 0.7980 - val_loss: 0.7625 - val_acc: 0.5815\n",
      "Epoch 231/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.4019 - acc: 0.8096 - val_loss: 0.7470 - val_acc: 0.6159\n",
      "Epoch 232/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3991 - acc: 0.8127 - val_loss: 0.7484 - val_acc: 0.5888\n",
      "Epoch 233/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3960 - acc: 0.8244 - val_loss: 0.7570 - val_acc: 0.5888\n",
      "Epoch 234/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3923 - acc: 0.8221 - val_loss: 0.7510 - val_acc: 0.6105\n",
      "Epoch 235/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3938 - acc: 0.8283 - val_loss: 0.7667 - val_acc: 0.5906\n",
      "Epoch 236/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3883 - acc: 0.8322 - val_loss: 0.7582 - val_acc: 0.5924\n",
      "Epoch 237/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3918 - acc: 0.8197 - val_loss: 0.7587 - val_acc: 0.5870\n",
      "Epoch 238/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3860 - acc: 0.8329 - val_loss: 0.7666 - val_acc: 0.6051\n",
      "Epoch 239/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3856 - acc: 0.8291 - val_loss: 0.7590 - val_acc: 0.6105\n",
      "Epoch 240/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3849 - acc: 0.8236 - val_loss: 0.7671 - val_acc: 0.6051\n",
      "Epoch 241/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3858 - acc: 0.8314 - val_loss: 0.7679 - val_acc: 0.5942\n",
      "Epoch 242/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3826 - acc: 0.8368 - val_loss: 0.7805 - val_acc: 0.6014\n",
      "Epoch 243/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3822 - acc: 0.8329 - val_loss: 0.7730 - val_acc: 0.6123\n",
      "Epoch 244/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3787 - acc: 0.8353 - val_loss: 0.7759 - val_acc: 0.5996\n",
      "Epoch 245/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3820 - acc: 0.8314 - val_loss: 0.7787 - val_acc: 0.5906\n",
      "Epoch 246/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3767 - acc: 0.8368 - val_loss: 0.7870 - val_acc: 0.6141\n",
      "Epoch 247/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3773 - acc: 0.8314 - val_loss: 0.7695 - val_acc: 0.5870\n",
      "Epoch 248/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3803 - acc: 0.8275 - val_loss: 0.7913 - val_acc: 0.5888\n",
      "Epoch 249/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3800 - acc: 0.8291 - val_loss: 0.7684 - val_acc: 0.6033\n",
      "Epoch 250/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3751 - acc: 0.8306 - val_loss: 0.7734 - val_acc: 0.6286\n",
      "Epoch 251/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3730 - acc: 0.8329 - val_loss: 0.7875 - val_acc: 0.6033\n",
      "Epoch 252/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3721 - acc: 0.8384 - val_loss: 0.7838 - val_acc: 0.5888\n",
      "Epoch 253/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3708 - acc: 0.8430 - val_loss: 0.7796 - val_acc: 0.6123\n",
      "Epoch 254/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3702 - acc: 0.8353 - val_loss: 0.7880 - val_acc: 0.5924\n",
      "Epoch 255/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3693 - acc: 0.8415 - val_loss: 0.7904 - val_acc: 0.6105\n",
      "Epoch 256/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3689 - acc: 0.8298 - val_loss: 0.7960 - val_acc: 0.5888\n",
      "Epoch 257/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3686 - acc: 0.8430 - val_loss: 0.7932 - val_acc: 0.5833\n",
      "Epoch 258/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3678 - acc: 0.8345 - val_loss: 0.7949 - val_acc: 0.6214\n",
      "Epoch 259/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3680 - acc: 0.8329 - val_loss: 0.7895 - val_acc: 0.5960\n",
      "Epoch 260/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3630 - acc: 0.8446 - val_loss: 0.8009 - val_acc: 0.5960\n",
      "Epoch 261/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3618 - acc: 0.8407 - val_loss: 0.7864 - val_acc: 0.6123\n",
      "Epoch 262/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3614 - acc: 0.8392 - val_loss: 0.8021 - val_acc: 0.6051\n",
      "Epoch 263/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3593 - acc: 0.8516 - val_loss: 0.8035 - val_acc: 0.6069\n",
      "Epoch 264/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3617 - acc: 0.8376 - val_loss: 0.7896 - val_acc: 0.5906\n",
      "Epoch 265/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3545 - acc: 0.8485 - val_loss: 0.8084 - val_acc: 0.6123\n",
      "Epoch 266/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3554 - acc: 0.8500 - val_loss: 0.7940 - val_acc: 0.6250\n",
      "Epoch 267/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3578 - acc: 0.8407 - val_loss: 0.7942 - val_acc: 0.6069\n",
      "Epoch 268/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3543 - acc: 0.8477 - val_loss: 0.8041 - val_acc: 0.6123\n",
      "Epoch 269/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3531 - acc: 0.8415 - val_loss: 0.8089 - val_acc: 0.5870\n",
      "Epoch 270/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3506 - acc: 0.8469 - val_loss: 0.8034 - val_acc: 0.5924\n",
      "Epoch 271/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3574 - acc: 0.8423 - val_loss: 0.7977 - val_acc: 0.5924\n",
      "Epoch 272/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3502 - acc: 0.8500 - val_loss: 0.8177 - val_acc: 0.5851\n",
      "Epoch 273/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3523 - acc: 0.8430 - val_loss: 0.8161 - val_acc: 0.5924\n",
      "Epoch 274/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3472 - acc: 0.8563 - val_loss: 0.8200 - val_acc: 0.6105\n",
      "Epoch 275/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3469 - acc: 0.8516 - val_loss: 0.8192 - val_acc: 0.5942\n",
      "Epoch 276/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3436 - acc: 0.8485 - val_loss: 0.8240 - val_acc: 0.6105\n",
      "Epoch 277/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3433 - acc: 0.8446 - val_loss: 0.8276 - val_acc: 0.6105\n",
      "Epoch 278/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3498 - acc: 0.8353 - val_loss: 0.8093 - val_acc: 0.5888\n",
      "Epoch 279/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3419 - acc: 0.8578 - val_loss: 0.8355 - val_acc: 0.5851\n",
      "Epoch 280/1200\n",
      "1287/1287 [==============================] - 3s 3ms/step - loss: 0.3417 - acc: 0.8531 - val_loss: 0.8327 - val_acc: 0.6141\n",
      "Epoch 281/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.3403 - acc: 0.8469 - val_loss: 0.8272 - val_acc: 0.6123\n",
      "Epoch 282/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3418 - acc: 0.8493 - val_loss: 0.8292 - val_acc: 0.6105\n",
      "Epoch 283/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3373 - acc: 0.8539 - val_loss: 0.8377 - val_acc: 0.5942\n",
      "Epoch 284/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3364 - acc: 0.8563 - val_loss: 0.8292 - val_acc: 0.5996\n",
      "Epoch 285/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3354 - acc: 0.8539 - val_loss: 0.8530 - val_acc: 0.5978\n",
      "Epoch 286/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3371 - acc: 0.8594 - val_loss: 0.8423 - val_acc: 0.6159\n",
      "Epoch 287/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3334 - acc: 0.8586 - val_loss: 0.8590 - val_acc: 0.6141\n",
      "Epoch 288/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3341 - acc: 0.8555 - val_loss: 0.8456 - val_acc: 0.6123\n",
      "Epoch 289/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3301 - acc: 0.8632 - val_loss: 0.8553 - val_acc: 0.5924\n",
      "Epoch 290/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3304 - acc: 0.8570 - val_loss: 0.8569 - val_acc: 0.6105\n",
      "Epoch 291/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3286 - acc: 0.8648 - val_loss: 0.8511 - val_acc: 0.5978\n",
      "Epoch 292/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3262 - acc: 0.8687 - val_loss: 0.8570 - val_acc: 0.5924\n",
      "Epoch 293/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3256 - acc: 0.8594 - val_loss: 0.8614 - val_acc: 0.5906\n",
      "Epoch 294/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3234 - acc: 0.8578 - val_loss: 0.8740 - val_acc: 0.6304\n",
      "Epoch 295/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3254 - acc: 0.8640 - val_loss: 0.8518 - val_acc: 0.6087\n",
      "Epoch 296/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3267 - acc: 0.8555 - val_loss: 0.8673 - val_acc: 0.5906\n",
      "Epoch 297/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3237 - acc: 0.8656 - val_loss: 0.8768 - val_acc: 0.5960\n",
      "Epoch 298/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3242 - acc: 0.8656 - val_loss: 0.8701 - val_acc: 0.5924\n",
      "Epoch 299/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3191 - acc: 0.8617 - val_loss: 0.8681 - val_acc: 0.5942\n",
      "Epoch 300/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3184 - acc: 0.8671 - val_loss: 0.8693 - val_acc: 0.6069\n",
      "Epoch 301/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3178 - acc: 0.8671 - val_loss: 0.8668 - val_acc: 0.5996\n",
      "Epoch 302/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3156 - acc: 0.8687 - val_loss: 0.8739 - val_acc: 0.5888\n",
      "Epoch 303/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3200 - acc: 0.8594 - val_loss: 0.8676 - val_acc: 0.6105\n",
      "Epoch 304/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3129 - acc: 0.8640 - val_loss: 0.8815 - val_acc: 0.6159\n",
      "Epoch 305/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3130 - acc: 0.8679 - val_loss: 0.8969 - val_acc: 0.5996\n",
      "Epoch 306/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3153 - acc: 0.8695 - val_loss: 0.8942 - val_acc: 0.5924\n",
      "Epoch 307/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3127 - acc: 0.8687 - val_loss: 0.8806 - val_acc: 0.6033\n",
      "Epoch 308/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3100 - acc: 0.8648 - val_loss: 0.8697 - val_acc: 0.6014\n",
      "Epoch 309/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3092 - acc: 0.8710 - val_loss: 0.8722 - val_acc: 0.5960\n",
      "Epoch 310/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3080 - acc: 0.8679 - val_loss: 0.8804 - val_acc: 0.5924\n",
      "Epoch 311/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3059 - acc: 0.8726 - val_loss: 0.8871 - val_acc: 0.6087\n",
      "Epoch 312/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3077 - acc: 0.8695 - val_loss: 0.9019 - val_acc: 0.6051\n",
      "Epoch 313/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3078 - acc: 0.8733 - val_loss: 0.8977 - val_acc: 0.5978\n",
      "Epoch 314/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3016 - acc: 0.8726 - val_loss: 0.8902 - val_acc: 0.5942\n",
      "Epoch 315/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3009 - acc: 0.8679 - val_loss: 0.9220 - val_acc: 0.5942\n",
      "Epoch 316/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3033 - acc: 0.8656 - val_loss: 0.9190 - val_acc: 0.5851\n",
      "Epoch 317/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3023 - acc: 0.8757 - val_loss: 0.8890 - val_acc: 0.6141\n",
      "Epoch 318/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3036 - acc: 0.8733 - val_loss: 0.9455 - val_acc: 0.6159\n",
      "Epoch 319/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3025 - acc: 0.8710 - val_loss: 0.9168 - val_acc: 0.5978\n",
      "Epoch 320/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.3007 - acc: 0.8733 - val_loss: 0.9166 - val_acc: 0.6051\n",
      "Epoch 321/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2929 - acc: 0.8796 - val_loss: 0.8988 - val_acc: 0.5906\n",
      "Epoch 322/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2998 - acc: 0.8733 - val_loss: 0.9371 - val_acc: 0.5851\n",
      "Epoch 323/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2922 - acc: 0.8788 - val_loss: 0.9170 - val_acc: 0.6014\n",
      "Epoch 324/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2934 - acc: 0.8827 - val_loss: 0.9129 - val_acc: 0.6087\n",
      "Epoch 325/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2900 - acc: 0.8772 - val_loss: 0.9378 - val_acc: 0.5851\n",
      "Epoch 326/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2923 - acc: 0.8780 - val_loss: 0.9175 - val_acc: 0.5924\n",
      "Epoch 327/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2891 - acc: 0.8796 - val_loss: 0.9270 - val_acc: 0.5924\n",
      "Epoch 328/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2868 - acc: 0.8796 - val_loss: 0.9395 - val_acc: 0.6069\n",
      "Epoch 329/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2846 - acc: 0.8796 - val_loss: 0.9348 - val_acc: 0.6051\n",
      "Epoch 330/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2866 - acc: 0.8834 - val_loss: 0.9351 - val_acc: 0.6033\n",
      "Epoch 331/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2899 - acc: 0.8765 - val_loss: 0.9436 - val_acc: 0.5960\n",
      "Epoch 332/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2849 - acc: 0.8873 - val_loss: 0.9510 - val_acc: 0.6105\n",
      "Epoch 333/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2816 - acc: 0.8811 - val_loss: 0.9503 - val_acc: 0.6087\n",
      "Epoch 334/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2796 - acc: 0.8881 - val_loss: 0.9363 - val_acc: 0.6051\n",
      "Epoch 335/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2843 - acc: 0.8780 - val_loss: 0.9649 - val_acc: 0.5888\n",
      "Epoch 336/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2834 - acc: 0.8765 - val_loss: 0.9422 - val_acc: 0.6087\n",
      "Epoch 337/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2828 - acc: 0.8710 - val_loss: 0.9385 - val_acc: 0.5906\n",
      "Epoch 338/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2792 - acc: 0.8780 - val_loss: 0.9728 - val_acc: 0.6051\n",
      "Epoch 339/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2761 - acc: 0.8873 - val_loss: 0.9440 - val_acc: 0.5888\n",
      "Epoch 340/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2781 - acc: 0.8811 - val_loss: 0.9833 - val_acc: 0.6014\n",
      "Epoch 341/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2808 - acc: 0.8842 - val_loss: 0.9679 - val_acc: 0.6014\n",
      "Epoch 342/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2752 - acc: 0.8928 - val_loss: 0.9724 - val_acc: 0.5996\n",
      "Epoch 343/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2700 - acc: 0.8889 - val_loss: 0.9620 - val_acc: 0.5996\n",
      "Epoch 344/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2773 - acc: 0.8765 - val_loss: 0.9747 - val_acc: 0.6105\n",
      "Epoch 345/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2776 - acc: 0.8780 - val_loss: 0.9836 - val_acc: 0.5996\n",
      "Epoch 346/1200\n",
      "1287/1287 [==============================] - 3s 3ms/step - loss: 0.2716 - acc: 0.8834 - val_loss: 0.9578 - val_acc: 0.5960\n",
      "Epoch 347/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.2689 - acc: 0.8858 - val_loss: 0.9948 - val_acc: 0.6051\n",
      "Epoch 348/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2718 - acc: 0.8842 - val_loss: 0.9751 - val_acc: 0.5960\n",
      "Epoch 349/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2686 - acc: 0.8889 - val_loss: 0.9878 - val_acc: 0.5924\n",
      "Epoch 350/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2712 - acc: 0.8858 - val_loss: 0.9806 - val_acc: 0.6123\n",
      "Epoch 351/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2655 - acc: 0.8881 - val_loss: 0.9809 - val_acc: 0.5906\n",
      "Epoch 352/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2614 - acc: 0.8998 - val_loss: 0.9868 - val_acc: 0.5942\n",
      "Epoch 353/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2634 - acc: 0.8912 - val_loss: 0.9715 - val_acc: 0.5924\n",
      "Epoch 354/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2637 - acc: 0.8904 - val_loss: 0.9683 - val_acc: 0.6014\n",
      "Epoch 355/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2604 - acc: 0.8943 - val_loss: 0.9946 - val_acc: 0.5942\n",
      "Epoch 356/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2557 - acc: 0.8974 - val_loss: 0.9992 - val_acc: 0.5978\n",
      "Epoch 357/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2577 - acc: 0.8974 - val_loss: 1.0027 - val_acc: 0.6033\n",
      "Epoch 358/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2552 - acc: 0.9005 - val_loss: 1.0137 - val_acc: 0.6051\n",
      "Epoch 359/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2572 - acc: 0.8897 - val_loss: 1.0121 - val_acc: 0.6141\n",
      "Epoch 360/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2576 - acc: 0.8920 - val_loss: 1.0125 - val_acc: 0.5924\n",
      "Epoch 361/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2533 - acc: 0.8982 - val_loss: 1.0032 - val_acc: 0.6014\n",
      "Epoch 362/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2540 - acc: 0.8951 - val_loss: 1.0109 - val_acc: 0.6141\n",
      "Epoch 363/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2535 - acc: 0.8928 - val_loss: 1.0308 - val_acc: 0.6051\n",
      "Epoch 364/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2513 - acc: 0.9005 - val_loss: 1.0284 - val_acc: 0.5888\n",
      "Epoch 365/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2505 - acc: 0.8967 - val_loss: 1.0153 - val_acc: 0.6033\n",
      "Epoch 366/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2508 - acc: 0.8990 - val_loss: 1.0211 - val_acc: 0.6033\n",
      "Epoch 367/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2551 - acc: 0.8951 - val_loss: 1.0315 - val_acc: 0.5978\n",
      "Epoch 368/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2517 - acc: 0.8951 - val_loss: 1.0205 - val_acc: 0.6033\n",
      "Epoch 369/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2492 - acc: 0.8990 - val_loss: 1.0347 - val_acc: 0.5960\n",
      "Epoch 370/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2512 - acc: 0.8990 - val_loss: 1.0348 - val_acc: 0.6051\n",
      "Epoch 371/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2503 - acc: 0.8967 - val_loss: 1.0303 - val_acc: 0.5978\n",
      "Epoch 372/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2433 - acc: 0.9013 - val_loss: 1.0464 - val_acc: 0.6051\n",
      "Epoch 373/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2465 - acc: 0.8943 - val_loss: 1.0344 - val_acc: 0.6069\n",
      "Epoch 374/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2442 - acc: 0.8928 - val_loss: 1.0398 - val_acc: 0.5942\n",
      "Epoch 375/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2424 - acc: 0.8943 - val_loss: 1.0640 - val_acc: 0.6014\n",
      "Epoch 376/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2369 - acc: 0.9013 - val_loss: 1.0429 - val_acc: 0.5978\n",
      "Epoch 377/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2374 - acc: 0.9060 - val_loss: 1.0570 - val_acc: 0.5960\n",
      "Epoch 378/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2369 - acc: 0.9044 - val_loss: 1.0381 - val_acc: 0.5960\n",
      "Epoch 379/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2360 - acc: 0.9013 - val_loss: 1.0645 - val_acc: 0.6033\n",
      "Epoch 380/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2370 - acc: 0.9013 - val_loss: 1.0473 - val_acc: 0.6051\n",
      "Epoch 381/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2368 - acc: 0.9052 - val_loss: 1.0643 - val_acc: 0.6014\n",
      "Epoch 382/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2314 - acc: 0.9044 - val_loss: 1.0757 - val_acc: 0.5960\n",
      "Epoch 383/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2338 - acc: 0.9037 - val_loss: 1.0865 - val_acc: 0.5996\n",
      "Epoch 384/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2313 - acc: 0.9052 - val_loss: 1.0728 - val_acc: 0.6159\n",
      "Epoch 385/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2366 - acc: 0.8998 - val_loss: 1.0566 - val_acc: 0.5996\n",
      "Epoch 386/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2279 - acc: 0.9122 - val_loss: 1.0721 - val_acc: 0.5960\n",
      "Epoch 387/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2286 - acc: 0.9122 - val_loss: 1.0981 - val_acc: 0.5942\n",
      "Epoch 388/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2323 - acc: 0.9029 - val_loss: 1.0823 - val_acc: 0.6014\n",
      "Epoch 389/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2292 - acc: 0.9075 - val_loss: 1.0666 - val_acc: 0.5924\n",
      "Epoch 390/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2281 - acc: 0.9106 - val_loss: 1.0667 - val_acc: 0.6014\n",
      "Epoch 391/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2314 - acc: 0.8998 - val_loss: 1.0747 - val_acc: 0.5996\n",
      "Epoch 392/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2270 - acc: 0.8974 - val_loss: 1.1125 - val_acc: 0.6069\n",
      "Epoch 393/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2253 - acc: 0.9060 - val_loss: 1.0878 - val_acc: 0.6033\n",
      "Epoch 394/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2221 - acc: 0.9138 - val_loss: 1.0767 - val_acc: 0.6178\n",
      "Epoch 395/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2223 - acc: 0.9106 - val_loss: 1.1186 - val_acc: 0.5924\n",
      "Epoch 396/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2236 - acc: 0.9099 - val_loss: 1.0882 - val_acc: 0.5996\n",
      "Epoch 397/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2188 - acc: 0.9099 - val_loss: 1.1445 - val_acc: 0.6014\n",
      "Epoch 398/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2191 - acc: 0.9138 - val_loss: 1.1012 - val_acc: 0.6159\n",
      "Epoch 399/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2190 - acc: 0.9106 - val_loss: 1.1082 - val_acc: 0.6014\n",
      "Epoch 400/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2163 - acc: 0.9130 - val_loss: 1.1321 - val_acc: 0.5870\n",
      "Epoch 401/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2144 - acc: 0.9138 - val_loss: 1.1187 - val_acc: 0.6014\n",
      "Epoch 402/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2165 - acc: 0.9060 - val_loss: 1.1083 - val_acc: 0.6014\n",
      "Epoch 403/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2137 - acc: 0.9184 - val_loss: 1.1350 - val_acc: 0.6178\n",
      "Epoch 404/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2145 - acc: 0.9122 - val_loss: 1.0916 - val_acc: 0.6033\n",
      "Epoch 405/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2133 - acc: 0.9130 - val_loss: 1.1454 - val_acc: 0.5978\n",
      "Epoch 406/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2140 - acc: 0.9083 - val_loss: 1.1263 - val_acc: 0.6178\n",
      "Epoch 407/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2158 - acc: 0.9099 - val_loss: 1.1617 - val_acc: 0.6033\n",
      "Epoch 408/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2142 - acc: 0.9130 - val_loss: 1.1530 - val_acc: 0.5906\n",
      "Epoch 409/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2091 - acc: 0.9176 - val_loss: 1.1314 - val_acc: 0.5996\n",
      "Epoch 410/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2111 - acc: 0.9184 - val_loss: 1.1329 - val_acc: 0.5996\n",
      "Epoch 411/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2102 - acc: 0.9153 - val_loss: 1.1089 - val_acc: 0.5978\n",
      "Epoch 412/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.2116 - acc: 0.9161 - val_loss: 1.1600 - val_acc: 0.6051\n",
      "Epoch 413/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.2059 - acc: 0.9153 - val_loss: 1.1554 - val_acc: 0.5870\n",
      "Epoch 414/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2061 - acc: 0.9083 - val_loss: 1.1228 - val_acc: 0.6087\n",
      "Epoch 415/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2030 - acc: 0.9192 - val_loss: 1.1812 - val_acc: 0.5924\n",
      "Epoch 416/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2064 - acc: 0.9153 - val_loss: 1.1525 - val_acc: 0.5978\n",
      "Epoch 417/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2038 - acc: 0.9169 - val_loss: 1.1817 - val_acc: 0.5906\n",
      "Epoch 418/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2046 - acc: 0.9153 - val_loss: 1.1480 - val_acc: 0.6141\n",
      "Epoch 419/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2015 - acc: 0.9246 - val_loss: 1.1754 - val_acc: 0.5870\n",
      "Epoch 420/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2004 - acc: 0.9223 - val_loss: 1.1818 - val_acc: 0.6069\n",
      "Epoch 421/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1991 - acc: 0.9277 - val_loss: 1.1445 - val_acc: 0.5996\n",
      "Epoch 422/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1983 - acc: 0.9176 - val_loss: 1.1720 - val_acc: 0.5797\n",
      "Epoch 423/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.2046 - acc: 0.9200 - val_loss: 1.1645 - val_acc: 0.5888\n",
      "Epoch 424/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1969 - acc: 0.9200 - val_loss: 1.1784 - val_acc: 0.5924\n",
      "Epoch 425/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1963 - acc: 0.9231 - val_loss: 1.1738 - val_acc: 0.5996\n",
      "Epoch 426/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1955 - acc: 0.9192 - val_loss: 1.1879 - val_acc: 0.5888\n",
      "Epoch 427/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1956 - acc: 0.9270 - val_loss: 1.2011 - val_acc: 0.5960\n",
      "Epoch 428/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1978 - acc: 0.9239 - val_loss: 1.1967 - val_acc: 0.5906\n",
      "Epoch 429/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1935 - acc: 0.9293 - val_loss: 1.1913 - val_acc: 0.5942\n",
      "Epoch 430/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1935 - acc: 0.9239 - val_loss: 1.1921 - val_acc: 0.6014\n",
      "Epoch 431/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1907 - acc: 0.9215 - val_loss: 1.2007 - val_acc: 0.5978\n",
      "Epoch 432/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1884 - acc: 0.9223 - val_loss: 1.1908 - val_acc: 0.6069\n",
      "Epoch 433/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1888 - acc: 0.9239 - val_loss: 1.2047 - val_acc: 0.5924\n",
      "Epoch 434/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1917 - acc: 0.9262 - val_loss: 1.1765 - val_acc: 0.5996\n",
      "Epoch 435/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1911 - acc: 0.9207 - val_loss: 1.1989 - val_acc: 0.5942\n",
      "Epoch 436/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1869 - acc: 0.9285 - val_loss: 1.2128 - val_acc: 0.5924\n",
      "Epoch 437/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1891 - acc: 0.9207 - val_loss: 1.2021 - val_acc: 0.6159\n",
      "Epoch 438/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1873 - acc: 0.9308 - val_loss: 1.1931 - val_acc: 0.6051\n",
      "Epoch 439/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1872 - acc: 0.9246 - val_loss: 1.2093 - val_acc: 0.5870\n",
      "Epoch 440/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1851 - acc: 0.9301 - val_loss: 1.1929 - val_acc: 0.5996\n",
      "Epoch 441/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1846 - acc: 0.9371 - val_loss: 1.2389 - val_acc: 0.5978\n",
      "Epoch 442/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1805 - acc: 0.9277 - val_loss: 1.2301 - val_acc: 0.5779\n",
      "Epoch 443/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1924 - acc: 0.9130 - val_loss: 1.2292 - val_acc: 0.5851\n",
      "Epoch 444/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1845 - acc: 0.9285 - val_loss: 1.2417 - val_acc: 0.6178\n",
      "Epoch 445/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1804 - acc: 0.9347 - val_loss: 1.2199 - val_acc: 0.5942\n",
      "Epoch 446/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1794 - acc: 0.9301 - val_loss: 1.2439 - val_acc: 0.5978\n",
      "Epoch 447/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1761 - acc: 0.9308 - val_loss: 1.2469 - val_acc: 0.5942\n",
      "Epoch 448/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1748 - acc: 0.9324 - val_loss: 1.2407 - val_acc: 0.5960\n",
      "Epoch 449/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1760 - acc: 0.9355 - val_loss: 1.2218 - val_acc: 0.6069\n",
      "Epoch 450/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1793 - acc: 0.9332 - val_loss: 1.2342 - val_acc: 0.6141\n",
      "Epoch 451/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1728 - acc: 0.9324 - val_loss: 1.2610 - val_acc: 0.6033\n",
      "Epoch 452/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1781 - acc: 0.9363 - val_loss: 1.2557 - val_acc: 0.6014\n",
      "Epoch 453/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1733 - acc: 0.9371 - val_loss: 1.2587 - val_acc: 0.5815\n",
      "Epoch 454/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1726 - acc: 0.9371 - val_loss: 1.2698 - val_acc: 0.6014\n",
      "Epoch 455/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1705 - acc: 0.9355 - val_loss: 1.2258 - val_acc: 0.5978\n",
      "Epoch 456/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1725 - acc: 0.9425 - val_loss: 1.2374 - val_acc: 0.5996\n",
      "Epoch 457/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1692 - acc: 0.9363 - val_loss: 1.2697 - val_acc: 0.6033\n",
      "Epoch 458/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1711 - acc: 0.9332 - val_loss: 1.2649 - val_acc: 0.5960\n",
      "Epoch 459/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1672 - acc: 0.9378 - val_loss: 1.2476 - val_acc: 0.5978\n",
      "Epoch 460/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1658 - acc: 0.9441 - val_loss: 1.2743 - val_acc: 0.6014\n",
      "Epoch 461/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1670 - acc: 0.9402 - val_loss: 1.2687 - val_acc: 0.6159\n",
      "Epoch 462/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1667 - acc: 0.9394 - val_loss: 1.2638 - val_acc: 0.5960\n",
      "Epoch 463/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1629 - acc: 0.9433 - val_loss: 1.2643 - val_acc: 0.6033\n",
      "Epoch 464/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1647 - acc: 0.9409 - val_loss: 1.2712 - val_acc: 0.6123\n",
      "Epoch 465/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1607 - acc: 0.9402 - val_loss: 1.3256 - val_acc: 0.5870\n",
      "Epoch 466/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1632 - acc: 0.9409 - val_loss: 1.3106 - val_acc: 0.6014\n",
      "Epoch 467/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1624 - acc: 0.9409 - val_loss: 1.3127 - val_acc: 0.6014\n",
      "Epoch 468/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1644 - acc: 0.9347 - val_loss: 1.2730 - val_acc: 0.5888\n",
      "Epoch 469/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1628 - acc: 0.9456 - val_loss: 1.2810 - val_acc: 0.6051\n",
      "Epoch 470/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1615 - acc: 0.9417 - val_loss: 1.3214 - val_acc: 0.6033\n",
      "Epoch 471/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1562 - acc: 0.9472 - val_loss: 1.3263 - val_acc: 0.6051\n",
      "Epoch 472/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1617 - acc: 0.9378 - val_loss: 1.3051 - val_acc: 0.5870\n",
      "Epoch 473/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1579 - acc: 0.9472 - val_loss: 1.3163 - val_acc: 0.5996\n",
      "Epoch 474/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1641 - acc: 0.9394 - val_loss: 1.3037 - val_acc: 0.6178\n",
      "Epoch 475/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1567 - acc: 0.9495 - val_loss: 1.3477 - val_acc: 0.6123\n",
      "Epoch 476/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1537 - acc: 0.9479 - val_loss: 1.3073 - val_acc: 0.5924\n",
      "Epoch 477/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1527 - acc: 0.9472 - val_loss: 1.3287 - val_acc: 0.6123\n",
      "Epoch 478/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.1526 - acc: 0.9487 - val_loss: 1.3317 - val_acc: 0.5978\n",
      "Epoch 479/1200\n",
      "1287/1287 [==============================] - 3s 3ms/step - loss: 0.1513 - acc: 0.9464 - val_loss: 1.3085 - val_acc: 0.5942\n",
      "Epoch 480/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1508 - acc: 0.9479 - val_loss: 1.3414 - val_acc: 0.5924\n",
      "Epoch 481/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1503 - acc: 0.9518 - val_loss: 1.3301 - val_acc: 0.6014\n",
      "Epoch 482/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1487 - acc: 0.9510 - val_loss: 1.3395 - val_acc: 0.5924\n",
      "Epoch 483/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1551 - acc: 0.9441 - val_loss: 1.3421 - val_acc: 0.5924\n",
      "Epoch 484/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1502 - acc: 0.9495 - val_loss: 1.3403 - val_acc: 0.6105\n",
      "Epoch 485/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1484 - acc: 0.9487 - val_loss: 1.3341 - val_acc: 0.6178\n",
      "Epoch 486/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1492 - acc: 0.9472 - val_loss: 1.3312 - val_acc: 0.6178\n",
      "Epoch 487/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1465 - acc: 0.9479 - val_loss: 1.3421 - val_acc: 0.6014\n",
      "Epoch 488/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1457 - acc: 0.9479 - val_loss: 1.3485 - val_acc: 0.5978\n",
      "Epoch 489/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1469 - acc: 0.9526 - val_loss: 1.3608 - val_acc: 0.5996\n",
      "Epoch 490/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1459 - acc: 0.9510 - val_loss: 1.3980 - val_acc: 0.5924\n",
      "Epoch 491/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1432 - acc: 0.9518 - val_loss: 1.3524 - val_acc: 0.6033\n",
      "Epoch 492/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1412 - acc: 0.9565 - val_loss: 1.3550 - val_acc: 0.6123\n",
      "Epoch 493/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1423 - acc: 0.9503 - val_loss: 1.3573 - val_acc: 0.5888\n",
      "Epoch 494/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1403 - acc: 0.9565 - val_loss: 1.3854 - val_acc: 0.5870\n",
      "Epoch 495/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1436 - acc: 0.9510 - val_loss: 1.3912 - val_acc: 0.6051\n",
      "Epoch 496/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1377 - acc: 0.9588 - val_loss: 1.3763 - val_acc: 0.5960\n",
      "Epoch 497/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1384 - acc: 0.9542 - val_loss: 1.3808 - val_acc: 0.6159\n",
      "Epoch 498/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1376 - acc: 0.9557 - val_loss: 1.3586 - val_acc: 0.6051\n",
      "Epoch 499/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1373 - acc: 0.9580 - val_loss: 1.3914 - val_acc: 0.6069\n",
      "Epoch 500/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1344 - acc: 0.9557 - val_loss: 1.3720 - val_acc: 0.5978\n",
      "Epoch 501/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1347 - acc: 0.9611 - val_loss: 1.3989 - val_acc: 0.6033\n",
      "Epoch 502/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1360 - acc: 0.9503 - val_loss: 1.4121 - val_acc: 0.6069\n",
      "Epoch 503/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1375 - acc: 0.9534 - val_loss: 1.4232 - val_acc: 0.5851\n",
      "Epoch 504/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1375 - acc: 0.9542 - val_loss: 1.4203 - val_acc: 0.6033\n",
      "Epoch 505/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1321 - acc: 0.9619 - val_loss: 1.3986 - val_acc: 0.6033\n",
      "Epoch 506/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1311 - acc: 0.9580 - val_loss: 1.4141 - val_acc: 0.5978\n",
      "Epoch 507/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1341 - acc: 0.9580 - val_loss: 1.3910 - val_acc: 0.6051\n",
      "Epoch 508/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1299 - acc: 0.9588 - val_loss: 1.4341 - val_acc: 0.5942\n",
      "Epoch 509/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1274 - acc: 0.9643 - val_loss: 1.4174 - val_acc: 0.5978\n",
      "Epoch 510/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1288 - acc: 0.9611 - val_loss: 1.4015 - val_acc: 0.6141\n",
      "Epoch 511/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1297 - acc: 0.9580 - val_loss: 1.3980 - val_acc: 0.6141\n",
      "Epoch 512/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1329 - acc: 0.9565 - val_loss: 1.3914 - val_acc: 0.6014\n",
      "Epoch 513/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1272 - acc: 0.9604 - val_loss: 1.4096 - val_acc: 0.6033\n",
      "Epoch 514/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1263 - acc: 0.9611 - val_loss: 1.4302 - val_acc: 0.5996\n",
      "Epoch 515/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1239 - acc: 0.9658 - val_loss: 1.4159 - val_acc: 0.6033\n",
      "Epoch 516/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1223 - acc: 0.9643 - val_loss: 1.4384 - val_acc: 0.6033\n",
      "Epoch 517/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1266 - acc: 0.9627 - val_loss: 1.4405 - val_acc: 0.6033\n",
      "Epoch 518/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1276 - acc: 0.9596 - val_loss: 1.4481 - val_acc: 0.5960\n",
      "Epoch 519/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1232 - acc: 0.9627 - val_loss: 1.4704 - val_acc: 0.6069\n",
      "Epoch 520/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1199 - acc: 0.9650 - val_loss: 1.4412 - val_acc: 0.6051\n",
      "Epoch 521/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1205 - acc: 0.9643 - val_loss: 1.4679 - val_acc: 0.6051\n",
      "Epoch 522/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1199 - acc: 0.9627 - val_loss: 1.4665 - val_acc: 0.5906\n",
      "Epoch 523/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1208 - acc: 0.9596 - val_loss: 1.4857 - val_acc: 0.6014\n",
      "Epoch 524/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1207 - acc: 0.9611 - val_loss: 1.5112 - val_acc: 0.6178\n",
      "Epoch 525/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1184 - acc: 0.9627 - val_loss: 1.4751 - val_acc: 0.5960\n",
      "Epoch 526/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1165 - acc: 0.9674 - val_loss: 1.4548 - val_acc: 0.5978\n",
      "Epoch 527/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1150 - acc: 0.9674 - val_loss: 1.4751 - val_acc: 0.6033\n",
      "Epoch 528/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1171 - acc: 0.9674 - val_loss: 1.4799 - val_acc: 0.5906\n",
      "Epoch 529/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1157 - acc: 0.9681 - val_loss: 1.4904 - val_acc: 0.6014\n",
      "Epoch 530/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1133 - acc: 0.9697 - val_loss: 1.4543 - val_acc: 0.6069\n",
      "Epoch 531/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1174 - acc: 0.9658 - val_loss: 1.4506 - val_acc: 0.6141\n",
      "Epoch 532/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1186 - acc: 0.9611 - val_loss: 1.4650 - val_acc: 0.6033\n",
      "Epoch 533/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1129 - acc: 0.9697 - val_loss: 1.4760 - val_acc: 0.5942\n",
      "Epoch 534/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1171 - acc: 0.9650 - val_loss: 1.4773 - val_acc: 0.5924\n",
      "Epoch 535/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1109 - acc: 0.9689 - val_loss: 1.5196 - val_acc: 0.6159\n",
      "Epoch 536/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1109 - acc: 0.9674 - val_loss: 1.5243 - val_acc: 0.5978\n",
      "Epoch 537/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1083 - acc: 0.9713 - val_loss: 1.4965 - val_acc: 0.6141\n",
      "Epoch 538/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1098 - acc: 0.9681 - val_loss: 1.4976 - val_acc: 0.5851\n",
      "Epoch 539/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1106 - acc: 0.9697 - val_loss: 1.5098 - val_acc: 0.5960\n",
      "Epoch 540/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1093 - acc: 0.9705 - val_loss: 1.5319 - val_acc: 0.5960\n",
      "Epoch 541/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1082 - acc: 0.9720 - val_loss: 1.5089 - val_acc: 0.5978\n",
      "Epoch 542/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1130 - acc: 0.9596 - val_loss: 1.5135 - val_acc: 0.5870\n",
      "Epoch 543/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1092 - acc: 0.9681 - val_loss: 1.5297 - val_acc: 0.6033\n",
      "Epoch 544/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.1023 - acc: 0.9751 - val_loss: 1.5247 - val_acc: 0.6159\n",
      "Epoch 545/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1033 - acc: 0.9736 - val_loss: 1.5403 - val_acc: 0.6178\n",
      "Epoch 546/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1016 - acc: 0.9744 - val_loss: 1.5147 - val_acc: 0.6087\n",
      "Epoch 547/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1025 - acc: 0.9751 - val_loss: 1.5553 - val_acc: 0.6232\n",
      "Epoch 548/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1058 - acc: 0.9697 - val_loss: 1.5266 - val_acc: 0.6105\n",
      "Epoch 549/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1005 - acc: 0.9728 - val_loss: 1.5454 - val_acc: 0.6214\n",
      "Epoch 550/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1014 - acc: 0.9705 - val_loss: 1.5517 - val_acc: 0.5906\n",
      "Epoch 551/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1001 - acc: 0.9736 - val_loss: 1.5293 - val_acc: 0.6105\n",
      "Epoch 552/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.1011 - acc: 0.9697 - val_loss: 1.5742 - val_acc: 0.5960\n",
      "Epoch 553/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0999 - acc: 0.9713 - val_loss: 1.5558 - val_acc: 0.6087\n",
      "Epoch 554/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0980 - acc: 0.9728 - val_loss: 1.5378 - val_acc: 0.5888\n",
      "Epoch 555/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0964 - acc: 0.9790 - val_loss: 1.5877 - val_acc: 0.5851\n",
      "Epoch 556/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0971 - acc: 0.9759 - val_loss: 1.5955 - val_acc: 0.5978\n",
      "Epoch 557/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0966 - acc: 0.9744 - val_loss: 1.5861 - val_acc: 0.6141\n",
      "Epoch 558/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0940 - acc: 0.9790 - val_loss: 1.5731 - val_acc: 0.5906\n",
      "Epoch 559/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0920 - acc: 0.9790 - val_loss: 1.5529 - val_acc: 0.6105\n",
      "Epoch 560/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0917 - acc: 0.9775 - val_loss: 1.5819 - val_acc: 0.5996\n",
      "Epoch 561/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0939 - acc: 0.9767 - val_loss: 1.5657 - val_acc: 0.6069\n",
      "Epoch 562/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0924 - acc: 0.9775 - val_loss: 1.5959 - val_acc: 0.5996\n",
      "Epoch 563/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0914 - acc: 0.9775 - val_loss: 1.6038 - val_acc: 0.6105\n",
      "Epoch 564/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0944 - acc: 0.9720 - val_loss: 1.6053 - val_acc: 0.5996\n",
      "Epoch 565/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0936 - acc: 0.9775 - val_loss: 1.5863 - val_acc: 0.5960\n",
      "Epoch 566/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0941 - acc: 0.9705 - val_loss: 1.6184 - val_acc: 0.5942\n",
      "Epoch 567/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0908 - acc: 0.9759 - val_loss: 1.6209 - val_acc: 0.6141\n",
      "Epoch 568/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0882 - acc: 0.9767 - val_loss: 1.6241 - val_acc: 0.6087\n",
      "Epoch 569/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0876 - acc: 0.9798 - val_loss: 1.6321 - val_acc: 0.6250\n",
      "Epoch 570/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0904 - acc: 0.9759 - val_loss: 1.6149 - val_acc: 0.6051\n",
      "Epoch 571/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0890 - acc: 0.9782 - val_loss: 1.6234 - val_acc: 0.6087\n",
      "Epoch 572/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0861 - acc: 0.9814 - val_loss: 1.6651 - val_acc: 0.6214\n",
      "Epoch 573/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0857 - acc: 0.9798 - val_loss: 1.6347 - val_acc: 0.5996\n",
      "Epoch 574/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0829 - acc: 0.9829 - val_loss: 1.6474 - val_acc: 0.6051\n",
      "Epoch 575/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0830 - acc: 0.9806 - val_loss: 1.6500 - val_acc: 0.6105\n",
      "Epoch 576/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0871 - acc: 0.9767 - val_loss: 1.6775 - val_acc: 0.5888\n",
      "Epoch 577/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0842 - acc: 0.9782 - val_loss: 1.6648 - val_acc: 0.6051\n",
      "Epoch 578/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0836 - acc: 0.9775 - val_loss: 1.6390 - val_acc: 0.5924\n",
      "Epoch 579/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0813 - acc: 0.9837 - val_loss: 1.6744 - val_acc: 0.6196\n",
      "Epoch 580/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0857 - acc: 0.9767 - val_loss: 1.6392 - val_acc: 0.6141\n",
      "Epoch 581/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0794 - acc: 0.9845 - val_loss: 1.6657 - val_acc: 0.6232\n",
      "Epoch 582/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0780 - acc: 0.9829 - val_loss: 1.6878 - val_acc: 0.6286\n",
      "Epoch 583/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0787 - acc: 0.9775 - val_loss: 1.6695 - val_acc: 0.5906\n",
      "Epoch 584/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0791 - acc: 0.9837 - val_loss: 1.6587 - val_acc: 0.6214\n",
      "Epoch 585/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0795 - acc: 0.9821 - val_loss: 1.7054 - val_acc: 0.6196\n",
      "Epoch 586/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0801 - acc: 0.9775 - val_loss: 1.6637 - val_acc: 0.6069\n",
      "Epoch 587/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0782 - acc: 0.9837 - val_loss: 1.6846 - val_acc: 0.6014\n",
      "Epoch 588/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0763 - acc: 0.9829 - val_loss: 1.7048 - val_acc: 0.6250\n",
      "Epoch 589/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0745 - acc: 0.9821 - val_loss: 1.6886 - val_acc: 0.6232\n",
      "Epoch 590/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0769 - acc: 0.9814 - val_loss: 1.6915 - val_acc: 0.6033\n",
      "Epoch 591/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0772 - acc: 0.9806 - val_loss: 1.6904 - val_acc: 0.6268\n",
      "Epoch 592/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0749 - acc: 0.9829 - val_loss: 1.6964 - val_acc: 0.6051\n",
      "Epoch 593/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0736 - acc: 0.9821 - val_loss: 1.6795 - val_acc: 0.6196\n",
      "Epoch 594/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0744 - acc: 0.9837 - val_loss: 1.6917 - val_acc: 0.6159\n",
      "Epoch 595/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0709 - acc: 0.9845 - val_loss: 1.7050 - val_acc: 0.6051\n",
      "Epoch 596/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0721 - acc: 0.9860 - val_loss: 1.7256 - val_acc: 0.6178\n",
      "Epoch 597/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0722 - acc: 0.9806 - val_loss: 1.7319 - val_acc: 0.5906\n",
      "Epoch 598/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0737 - acc: 0.9806 - val_loss: 1.6950 - val_acc: 0.6159\n",
      "Epoch 599/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0693 - acc: 0.9868 - val_loss: 1.7130 - val_acc: 0.6141\n",
      "Epoch 600/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0710 - acc: 0.9852 - val_loss: 1.7211 - val_acc: 0.6178\n",
      "Epoch 601/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0700 - acc: 0.9837 - val_loss: 1.7212 - val_acc: 0.6159\n",
      "Epoch 602/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0683 - acc: 0.9860 - val_loss: 1.7285 - val_acc: 0.6069\n",
      "Epoch 603/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0666 - acc: 0.9876 - val_loss: 1.7295 - val_acc: 0.6051\n",
      "Epoch 604/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0673 - acc: 0.9852 - val_loss: 1.7802 - val_acc: 0.5996\n",
      "Epoch 605/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0715 - acc: 0.9845 - val_loss: 1.7567 - val_acc: 0.6159\n",
      "Epoch 606/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0643 - acc: 0.9883 - val_loss: 1.7362 - val_acc: 0.6178\n",
      "Epoch 607/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0667 - acc: 0.9852 - val_loss: 1.7213 - val_acc: 0.6069\n",
      "Epoch 608/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0645 - acc: 0.9852 - val_loss: 1.7625 - val_acc: 0.6196\n",
      "Epoch 609/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0642 - acc: 0.9883 - val_loss: 1.7672 - val_acc: 0.6214\n",
      "Epoch 610/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.0629 - acc: 0.9876 - val_loss: 1.7701 - val_acc: 0.6033\n",
      "Epoch 611/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0627 - acc: 0.9868 - val_loss: 1.7908 - val_acc: 0.5942\n",
      "Epoch 612/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0635 - acc: 0.9907 - val_loss: 1.7642 - val_acc: 0.5996\n",
      "Epoch 613/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0684 - acc: 0.9790 - val_loss: 1.7700 - val_acc: 0.6033\n",
      "Epoch 614/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0609 - acc: 0.9868 - val_loss: 1.7807 - val_acc: 0.6268\n",
      "Epoch 615/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0602 - acc: 0.9899 - val_loss: 1.8022 - val_acc: 0.6159\n",
      "Epoch 616/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0624 - acc: 0.9860 - val_loss: 1.8364 - val_acc: 0.6069\n",
      "Epoch 617/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0619 - acc: 0.9876 - val_loss: 1.8041 - val_acc: 0.6033\n",
      "Epoch 618/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0580 - acc: 0.9860 - val_loss: 1.8216 - val_acc: 0.5996\n",
      "Epoch 619/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0595 - acc: 0.9868 - val_loss: 1.8127 - val_acc: 0.6014\n",
      "Epoch 620/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0581 - acc: 0.9907 - val_loss: 1.8107 - val_acc: 0.6178\n",
      "Epoch 621/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0567 - acc: 0.9899 - val_loss: 1.7923 - val_acc: 0.6014\n",
      "Epoch 622/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0592 - acc: 0.9891 - val_loss: 1.7758 - val_acc: 0.5996\n",
      "Epoch 623/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0581 - acc: 0.9891 - val_loss: 1.7945 - val_acc: 0.6105\n",
      "Epoch 624/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0559 - acc: 0.9891 - val_loss: 1.7859 - val_acc: 0.6123\n",
      "Epoch 625/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0557 - acc: 0.9883 - val_loss: 1.8316 - val_acc: 0.6232\n",
      "Epoch 626/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0565 - acc: 0.9915 - val_loss: 1.8230 - val_acc: 0.6014\n",
      "Epoch 627/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0578 - acc: 0.9876 - val_loss: 1.8323 - val_acc: 0.6051\n",
      "Epoch 628/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0534 - acc: 0.9915 - val_loss: 1.8164 - val_acc: 0.6178\n",
      "Epoch 629/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0530 - acc: 0.9907 - val_loss: 1.8307 - val_acc: 0.6141\n",
      "Epoch 630/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0507 - acc: 0.9899 - val_loss: 1.8764 - val_acc: 0.6087\n",
      "Epoch 631/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0577 - acc: 0.9868 - val_loss: 1.8473 - val_acc: 0.5960\n",
      "Epoch 632/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0531 - acc: 0.9915 - val_loss: 1.8678 - val_acc: 0.6141\n",
      "Epoch 633/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0501 - acc: 0.9915 - val_loss: 1.8428 - val_acc: 0.6105\n",
      "Epoch 634/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0515 - acc: 0.9915 - val_loss: 1.8752 - val_acc: 0.6250\n",
      "Epoch 635/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0486 - acc: 0.9907 - val_loss: 1.8765 - val_acc: 0.6178\n",
      "Epoch 636/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0507 - acc: 0.9922 - val_loss: 1.8629 - val_acc: 0.5960\n",
      "Epoch 637/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0499 - acc: 0.9899 - val_loss: 1.8899 - val_acc: 0.6159\n",
      "Epoch 638/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0481 - acc: 0.9922 - val_loss: 1.8798 - val_acc: 0.6178\n",
      "Epoch 639/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0477 - acc: 0.9915 - val_loss: 1.9133 - val_acc: 0.6341\n",
      "Epoch 640/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0499 - acc: 0.9907 - val_loss: 1.8885 - val_acc: 0.6069\n",
      "Epoch 641/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0479 - acc: 0.9915 - val_loss: 1.8997 - val_acc: 0.6178\n",
      "Epoch 642/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0477 - acc: 0.9922 - val_loss: 1.9073 - val_acc: 0.6087\n",
      "Epoch 643/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0464 - acc: 0.9922 - val_loss: 1.8873 - val_acc: 0.6087\n",
      "Epoch 644/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0444 - acc: 0.9930 - val_loss: 1.9133 - val_acc: 0.6250\n",
      "Epoch 645/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0453 - acc: 0.9907 - val_loss: 1.9270 - val_acc: 0.6051\n",
      "Epoch 646/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0460 - acc: 0.9915 - val_loss: 1.9084 - val_acc: 0.6141\n",
      "Epoch 647/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0434 - acc: 0.9930 - val_loss: 1.8884 - val_acc: 0.6178\n",
      "Epoch 648/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0461 - acc: 0.9907 - val_loss: 1.9232 - val_acc: 0.5924\n",
      "Epoch 649/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0461 - acc: 0.9922 - val_loss: 1.9060 - val_acc: 0.6159\n",
      "Epoch 650/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0475 - acc: 0.9930 - val_loss: 1.9261 - val_acc: 0.6178\n",
      "Epoch 651/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0424 - acc: 0.9946 - val_loss: 1.9380 - val_acc: 0.6250\n",
      "Epoch 652/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0434 - acc: 0.9930 - val_loss: 1.9128 - val_acc: 0.6105\n",
      "Epoch 653/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0417 - acc: 0.9938 - val_loss: 1.9601 - val_acc: 0.6105\n",
      "Epoch 654/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0429 - acc: 0.9915 - val_loss: 1.9287 - val_acc: 0.6159\n",
      "Epoch 655/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0404 - acc: 0.9938 - val_loss: 1.9538 - val_acc: 0.6196\n",
      "Epoch 656/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0417 - acc: 0.9930 - val_loss: 1.9367 - val_acc: 0.6087\n",
      "Epoch 657/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0405 - acc: 0.9938 - val_loss: 1.9500 - val_acc: 0.6178\n",
      "Epoch 658/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0399 - acc: 0.9930 - val_loss: 1.9352 - val_acc: 0.6087\n",
      "Epoch 659/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0404 - acc: 0.9938 - val_loss: 1.9281 - val_acc: 0.6250\n",
      "Epoch 660/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0424 - acc: 0.9922 - val_loss: 1.9746 - val_acc: 0.6105\n",
      "Epoch 661/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0385 - acc: 0.9946 - val_loss: 1.9615 - val_acc: 0.6178\n",
      "Epoch 662/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0375 - acc: 0.9946 - val_loss: 1.9725 - val_acc: 0.6178\n",
      "Epoch 663/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0398 - acc: 0.9946 - val_loss: 1.9584 - val_acc: 0.6232\n",
      "Epoch 664/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0390 - acc: 0.9922 - val_loss: 1.9303 - val_acc: 0.6196\n",
      "Epoch 665/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0364 - acc: 0.9953 - val_loss: 1.9739 - val_acc: 0.6214\n",
      "Epoch 666/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0365 - acc: 0.9938 - val_loss: 1.9952 - val_acc: 0.6123\n",
      "Epoch 667/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0347 - acc: 0.9946 - val_loss: 1.9758 - val_acc: 0.6178\n",
      "Epoch 668/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0356 - acc: 0.9938 - val_loss: 1.9724 - val_acc: 0.6141\n",
      "Epoch 669/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0364 - acc: 0.9953 - val_loss: 1.9918 - val_acc: 0.6087\n",
      "Epoch 670/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0351 - acc: 0.9938 - val_loss: 1.9941 - val_acc: 0.6268\n",
      "Epoch 671/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0357 - acc: 0.9953 - val_loss: 1.9983 - val_acc: 0.6123\n",
      "Epoch 672/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0331 - acc: 0.9946 - val_loss: 2.0608 - val_acc: 0.5996\n",
      "Epoch 673/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0384 - acc: 0.9930 - val_loss: 2.0013 - val_acc: 0.5942\n",
      "Epoch 674/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0351 - acc: 0.9953 - val_loss: 2.0096 - val_acc: 0.6105\n",
      "Epoch 675/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.0333 - acc: 0.9961 - val_loss: 2.0471 - val_acc: 0.6123\n",
      "Epoch 676/1200\n",
      "1287/1287 [==============================] - 3s 3ms/step - loss: 0.0327 - acc: 0.9953 - val_loss: 2.0301 - val_acc: 0.6286\n",
      "Epoch 677/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0340 - acc: 0.9953 - val_loss: 2.0070 - val_acc: 0.6159\n",
      "Epoch 678/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0316 - acc: 0.9961 - val_loss: 2.0183 - val_acc: 0.6123\n",
      "Epoch 679/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0326 - acc: 0.9953 - val_loss: 2.0088 - val_acc: 0.6214\n",
      "Epoch 680/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0315 - acc: 0.9953 - val_loss: 2.0018 - val_acc: 0.6196\n",
      "Epoch 681/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0314 - acc: 0.9953 - val_loss: 2.0445 - val_acc: 0.6069\n",
      "Epoch 682/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0302 - acc: 0.9953 - val_loss: 2.0240 - val_acc: 0.6304\n",
      "Epoch 683/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0284 - acc: 0.9969 - val_loss: 2.0651 - val_acc: 0.6087\n",
      "Epoch 684/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0300 - acc: 0.9961 - val_loss: 2.0481 - val_acc: 0.6214\n",
      "Epoch 685/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0288 - acc: 0.9953 - val_loss: 2.0848 - val_acc: 0.6159\n",
      "Epoch 686/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0284 - acc: 0.9977 - val_loss: 2.0729 - val_acc: 0.6105\n",
      "Epoch 687/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0287 - acc: 0.9961 - val_loss: 2.0696 - val_acc: 0.5996\n",
      "Epoch 688/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0291 - acc: 0.9961 - val_loss: 2.0485 - val_acc: 0.6178\n",
      "Epoch 689/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0279 - acc: 0.9961 - val_loss: 2.0632 - val_acc: 0.6141\n",
      "Epoch 690/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0273 - acc: 0.9969 - val_loss: 2.0817 - val_acc: 0.6033\n",
      "Epoch 691/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0284 - acc: 0.9969 - val_loss: 2.0671 - val_acc: 0.6214\n",
      "Epoch 692/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0285 - acc: 0.9961 - val_loss: 2.0950 - val_acc: 0.6159\n",
      "Epoch 693/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0278 - acc: 0.9946 - val_loss: 2.1360 - val_acc: 0.6196\n",
      "Epoch 694/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0299 - acc: 0.9938 - val_loss: 2.0702 - val_acc: 0.6105\n",
      "Epoch 695/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0260 - acc: 0.9977 - val_loss: 2.0928 - val_acc: 0.6051\n",
      "Epoch 696/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0286 - acc: 0.9969 - val_loss: 2.1006 - val_acc: 0.6051\n",
      "Epoch 697/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0254 - acc: 0.9961 - val_loss: 2.1101 - val_acc: 0.6159\n",
      "Epoch 698/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0252 - acc: 0.9977 - val_loss: 2.1394 - val_acc: 0.6014\n",
      "Epoch 699/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0261 - acc: 0.9984 - val_loss: 2.1050 - val_acc: 0.6359\n",
      "Epoch 700/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0285 - acc: 0.9961 - val_loss: 2.1044 - val_acc: 0.6141\n",
      "Epoch 701/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0247 - acc: 0.9969 - val_loss: 2.1164 - val_acc: 0.6232\n",
      "Epoch 702/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0238 - acc: 0.9969 - val_loss: 2.1459 - val_acc: 0.6178\n",
      "Epoch 703/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0243 - acc: 0.9969 - val_loss: 2.1183 - val_acc: 0.6159\n",
      "Epoch 704/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0234 - acc: 0.9984 - val_loss: 2.1215 - val_acc: 0.6069\n",
      "Epoch 705/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0257 - acc: 0.9969 - val_loss: 2.1035 - val_acc: 0.6250\n",
      "Epoch 706/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0237 - acc: 0.9969 - val_loss: 2.1384 - val_acc: 0.6178\n",
      "Epoch 707/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0223 - acc: 0.9969 - val_loss: 2.1546 - val_acc: 0.6105\n",
      "Epoch 708/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0219 - acc: 0.9977 - val_loss: 2.1514 - val_acc: 0.6268\n",
      "Epoch 709/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0233 - acc: 0.9969 - val_loss: 2.1340 - val_acc: 0.6196\n",
      "Epoch 710/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0251 - acc: 0.9961 - val_loss: 2.1637 - val_acc: 0.6250\n",
      "Epoch 711/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0223 - acc: 0.9977 - val_loss: 2.1571 - val_acc: 0.6178\n",
      "Epoch 712/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0209 - acc: 0.9977 - val_loss: 2.1589 - val_acc: 0.6268\n",
      "Epoch 713/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0212 - acc: 0.9984 - val_loss: 2.1698 - val_acc: 0.6105\n",
      "Epoch 714/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0201 - acc: 0.9984 - val_loss: 2.1776 - val_acc: 0.6069\n",
      "Epoch 715/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0208 - acc: 0.9984 - val_loss: 2.1728 - val_acc: 0.6250\n",
      "Epoch 716/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0204 - acc: 0.9984 - val_loss: 2.1932 - val_acc: 0.6014\n",
      "Epoch 717/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0200 - acc: 0.9984 - val_loss: 2.2026 - val_acc: 0.6159\n",
      "Epoch 718/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0200 - acc: 0.9984 - val_loss: 2.1829 - val_acc: 0.6286\n",
      "Epoch 719/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0192 - acc: 0.9992 - val_loss: 2.1945 - val_acc: 0.6087\n",
      "Epoch 720/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0183 - acc: 0.9984 - val_loss: 2.1802 - val_acc: 0.6214\n",
      "Epoch 721/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0213 - acc: 0.9969 - val_loss: 2.1842 - val_acc: 0.6069\n",
      "Epoch 722/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0181 - acc: 0.9977 - val_loss: 2.1990 - val_acc: 0.6286\n",
      "Epoch 723/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0187 - acc: 0.9984 - val_loss: 2.2058 - val_acc: 0.6178\n",
      "Epoch 724/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0184 - acc: 0.9992 - val_loss: 2.2366 - val_acc: 0.6051\n",
      "Epoch 725/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0177 - acc: 0.9984 - val_loss: 2.1841 - val_acc: 0.6105\n",
      "Epoch 726/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0176 - acc: 0.9984 - val_loss: 2.2286 - val_acc: 0.6214\n",
      "Epoch 727/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0179 - acc: 0.9977 - val_loss: 2.2101 - val_acc: 0.6123\n",
      "Epoch 728/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0181 - acc: 0.9984 - val_loss: 2.2161 - val_acc: 0.6178\n",
      "Epoch 729/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0173 - acc: 0.9984 - val_loss: 2.2443 - val_acc: 0.6178\n",
      "Epoch 730/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0177 - acc: 0.9984 - val_loss: 2.2489 - val_acc: 0.6123\n",
      "Epoch 731/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0172 - acc: 0.9984 - val_loss: 2.2703 - val_acc: 0.6087\n",
      "Epoch 732/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0163 - acc: 0.9984 - val_loss: 2.2487 - val_acc: 0.6033\n",
      "Epoch 733/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0160 - acc: 0.9992 - val_loss: 2.2371 - val_acc: 0.6214\n",
      "Epoch 734/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0155 - acc: 0.9984 - val_loss: 2.2495 - val_acc: 0.6196\n",
      "Epoch 735/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0151 - acc: 0.9984 - val_loss: 2.2676 - val_acc: 0.6123\n",
      "Epoch 736/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0160 - acc: 0.9992 - val_loss: 2.2645 - val_acc: 0.6268\n",
      "Epoch 737/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0153 - acc: 0.9992 - val_loss: 2.2678 - val_acc: 0.6123\n",
      "Epoch 738/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0151 - acc: 0.9992 - val_loss: 2.2642 - val_acc: 0.6159\n",
      "Epoch 739/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0152 - acc: 0.9984 - val_loss: 2.2844 - val_acc: 0.6178\n",
      "Epoch 740/1200\n",
      "1287/1287 [==============================] - 3s 3ms/step - loss: 0.0159 - acc: 0.9984 - val_loss: 2.2542 - val_acc: 0.6214\n",
      "Epoch 741/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.0148 - acc: 0.9984 - val_loss: 2.3060 - val_acc: 0.6196\n",
      "Epoch 742/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0140 - acc: 1.0000 - val_loss: 2.3123 - val_acc: 0.6250\n",
      "Epoch 743/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0140 - acc: 0.9992 - val_loss: 2.2868 - val_acc: 0.6250\n",
      "Epoch 744/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0139 - acc: 0.9992 - val_loss: 2.3117 - val_acc: 0.6141\n",
      "Epoch 745/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0137 - acc: 0.9984 - val_loss: 2.3156 - val_acc: 0.6214\n",
      "Epoch 746/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.3075 - val_acc: 0.6178\n",
      "Epoch 747/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0130 - acc: 0.9992 - val_loss: 2.3221 - val_acc: 0.6141\n",
      "Epoch 748/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0138 - acc: 0.9984 - val_loss: 2.3299 - val_acc: 0.6250\n",
      "Epoch 749/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.3203 - val_acc: 0.5942\n",
      "Epoch 750/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0140 - acc: 1.0000 - val_loss: 2.3313 - val_acc: 0.6123\n",
      "Epoch 751/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0139 - acc: 0.9992 - val_loss: 2.3323 - val_acc: 0.6178\n",
      "Epoch 752/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0120 - acc: 1.0000 - val_loss: 2.3418 - val_acc: 0.6159\n",
      "Epoch 753/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0121 - acc: 1.0000 - val_loss: 2.3640 - val_acc: 0.6087\n",
      "Epoch 754/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0120 - acc: 0.9984 - val_loss: 2.3651 - val_acc: 0.6159\n",
      "Epoch 755/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0110 - acc: 1.0000 - val_loss: 2.3228 - val_acc: 0.6250\n",
      "Epoch 756/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0129 - acc: 0.9992 - val_loss: 2.3669 - val_acc: 0.5996\n",
      "Epoch 757/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0154 - acc: 0.9977 - val_loss: 2.3678 - val_acc: 0.6051\n",
      "Epoch 758/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0111 - acc: 0.9992 - val_loss: 2.3606 - val_acc: 0.6087\n",
      "Epoch 759/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0109 - acc: 0.9992 - val_loss: 2.3421 - val_acc: 0.6159\n",
      "Epoch 760/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0105 - acc: 1.0000 - val_loss: 2.3721 - val_acc: 0.6123\n",
      "Epoch 761/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0106 - acc: 1.0000 - val_loss: 2.3693 - val_acc: 0.6250\n",
      "Epoch 762/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0112 - acc: 0.9992 - val_loss: 2.3925 - val_acc: 0.6141\n",
      "Epoch 763/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0097 - acc: 1.0000 - val_loss: 2.3846 - val_acc: 0.6105\n",
      "Epoch 764/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 2.4286 - val_acc: 0.5924\n",
      "Epoch 765/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0108 - acc: 0.9992 - val_loss: 2.3705 - val_acc: 0.6087\n",
      "Epoch 766/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0098 - acc: 1.0000 - val_loss: 2.4049 - val_acc: 0.6087\n",
      "Epoch 767/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0094 - acc: 1.0000 - val_loss: 2.3832 - val_acc: 0.6214\n",
      "Epoch 768/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 2.4158 - val_acc: 0.6159\n",
      "Epoch 769/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0099 - acc: 1.0000 - val_loss: 2.4048 - val_acc: 0.6141\n",
      "Epoch 770/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0091 - acc: 1.0000 - val_loss: 2.4155 - val_acc: 0.6123\n",
      "Epoch 771/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0089 - acc: 1.0000 - val_loss: 2.4184 - val_acc: 0.6141\n",
      "Epoch 772/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0088 - acc: 1.0000 - val_loss: 2.4034 - val_acc: 0.6232\n",
      "Epoch 773/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0130 - acc: 0.9992 - val_loss: 2.4401 - val_acc: 0.6123\n",
      "Epoch 774/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0085 - acc: 1.0000 - val_loss: 2.4116 - val_acc: 0.6268\n",
      "Epoch 775/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0084 - acc: 1.0000 - val_loss: 2.4423 - val_acc: 0.6123\n",
      "Epoch 776/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0100 - acc: 0.9992 - val_loss: 2.4094 - val_acc: 0.6178\n",
      "Epoch 777/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 2.4513 - val_acc: 0.6069\n",
      "Epoch 778/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0087 - acc: 1.0000 - val_loss: 2.5308 - val_acc: 0.5924\n",
      "Epoch 779/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0099 - acc: 1.0000 - val_loss: 2.4347 - val_acc: 0.6159\n",
      "Epoch 780/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0080 - acc: 1.0000 - val_loss: 2.4823 - val_acc: 0.6014\n",
      "Epoch 781/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0077 - acc: 1.0000 - val_loss: 2.4736 - val_acc: 0.6123\n",
      "Epoch 782/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0074 - acc: 1.0000 - val_loss: 2.4670 - val_acc: 0.6159\n",
      "Epoch 783/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0074 - acc: 1.0000 - val_loss: 2.4759 - val_acc: 0.6123\n",
      "Epoch 784/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 2.4950 - val_acc: 0.6033\n",
      "Epoch 785/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 2.4691 - val_acc: 0.6105\n",
      "Epoch 786/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 2.4746 - val_acc: 0.6178\n",
      "Epoch 787/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 2.4864 - val_acc: 0.6087\n",
      "Epoch 788/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0078 - acc: 1.0000 - val_loss: 2.4585 - val_acc: 0.6196\n",
      "Epoch 789/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 2.4843 - val_acc: 0.6123\n",
      "Epoch 790/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0063 - acc: 1.0000 - val_loss: 2.5117 - val_acc: 0.6159\n",
      "Epoch 791/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 2.5166 - val_acc: 0.6178\n",
      "Epoch 792/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0064 - acc: 1.0000 - val_loss: 2.5177 - val_acc: 0.6123\n",
      "Epoch 793/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 2.4975 - val_acc: 0.6141\n",
      "Epoch 794/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 2.5366 - val_acc: 0.6069\n",
      "Epoch 795/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 2.5382 - val_acc: 0.6123\n",
      "Epoch 796/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 2.5269 - val_acc: 0.6141\n",
      "Epoch 797/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 2.5397 - val_acc: 0.6141\n",
      "Epoch 798/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 2.5391 - val_acc: 0.6141\n",
      "Epoch 799/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 2.5558 - val_acc: 0.6033\n",
      "Epoch 800/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 2.5555 - val_acc: 0.6159\n",
      "Epoch 801/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 2.5505 - val_acc: 0.6105\n",
      "Epoch 802/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0058 - acc: 1.0000 - val_loss: 2.5508 - val_acc: 0.6105\n",
      "Epoch 803/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 2.5499 - val_acc: 0.6141\n",
      "Epoch 804/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 2.5899 - val_acc: 0.6105\n",
      "Epoch 805/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 2.5565 - val_acc: 0.6232\n",
      "Epoch 806/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 2.5920 - val_acc: 0.6033\n",
      "Epoch 807/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 2.5689 - val_acc: 0.6178\n",
      "Epoch 808/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 2.5651 - val_acc: 0.6178\n",
      "Epoch 809/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 2.5991 - val_acc: 0.6123\n",
      "Epoch 810/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 2.6216 - val_acc: 0.6033\n",
      "Epoch 811/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 2.5918 - val_acc: 0.6069\n",
      "Epoch 812/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 2.5975 - val_acc: 0.6178\n",
      "Epoch 813/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 2.6051 - val_acc: 0.6087\n",
      "Epoch 814/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0043 - acc: 1.0000 - val_loss: 2.6246 - val_acc: 0.6178\n",
      "Epoch 815/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0043 - acc: 1.0000 - val_loss: 2.6071 - val_acc: 0.6178\n",
      "Epoch 816/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 2.5831 - val_acc: 0.6178\n",
      "Epoch 817/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0041 - acc: 1.0000 - val_loss: 2.6308 - val_acc: 0.6141\n",
      "Epoch 818/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 2.6258 - val_acc: 0.6159\n",
      "Epoch 819/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 2.6220 - val_acc: 0.6196\n",
      "Epoch 820/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 2.6303 - val_acc: 0.6141\n",
      "Epoch 821/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 2.6280 - val_acc: 0.6268\n",
      "Epoch 822/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 2.6451 - val_acc: 0.6087\n",
      "Epoch 823/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 2.6515 - val_acc: 0.6196\n",
      "Epoch 824/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 2.6565 - val_acc: 0.6087\n",
      "Epoch 825/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 2.6426 - val_acc: 0.6196\n",
      "Epoch 826/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 2.6644 - val_acc: 0.6141\n",
      "Epoch 827/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 2.6673 - val_acc: 0.6178\n",
      "Epoch 828/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 2.6440 - val_acc: 0.6232\n",
      "Epoch 829/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 2.6771 - val_acc: 0.6196\n",
      "Epoch 830/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 2.6724 - val_acc: 0.6178\n",
      "Epoch 831/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 2.7089 - val_acc: 0.6087\n",
      "Epoch 832/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 2.6839 - val_acc: 0.6123\n",
      "Epoch 833/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 2.6893 - val_acc: 0.6178\n",
      "Epoch 834/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 2.7138 - val_acc: 0.6196\n",
      "Epoch 835/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 2.7095 - val_acc: 0.6123\n",
      "Epoch 836/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 2.7132 - val_acc: 0.6123\n",
      "Epoch 837/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 2.7328 - val_acc: 0.6087\n",
      "Epoch 838/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 2.7300 - val_acc: 0.6178\n",
      "Epoch 839/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 2.7532 - val_acc: 0.6033\n",
      "Epoch 840/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 2.7455 - val_acc: 0.6087\n",
      "Epoch 841/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 2.7614 - val_acc: 0.6051\n",
      "Epoch 842/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 2.7514 - val_acc: 0.6141\n",
      "Epoch 843/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 2.7377 - val_acc: 0.6123\n",
      "Epoch 844/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 2.7393 - val_acc: 0.6123\n",
      "Epoch 845/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 2.7625 - val_acc: 0.6033\n",
      "Epoch 846/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 2.7555 - val_acc: 0.6214\n",
      "Epoch 847/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 2.7620 - val_acc: 0.6123\n",
      "Epoch 848/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 2.7736 - val_acc: 0.6178\n",
      "Epoch 849/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 2.7921 - val_acc: 0.6087\n",
      "Epoch 850/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 2.7747 - val_acc: 0.6141\n",
      "Epoch 851/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 2.7752 - val_acc: 0.6141\n",
      "Epoch 852/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 2.7571 - val_acc: 0.6304\n",
      "Epoch 853/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 2.7745 - val_acc: 0.6087\n",
      "Epoch 854/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 2.7593 - val_acc: 0.6232\n",
      "Epoch 855/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 2.8078 - val_acc: 0.6141\n",
      "Epoch 856/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 2.8168 - val_acc: 0.6105\n",
      "Epoch 857/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 2.8083 - val_acc: 0.6178\n",
      "Epoch 858/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 2.7928 - val_acc: 0.6159\n",
      "Epoch 859/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 2.8206 - val_acc: 0.6123\n",
      "Epoch 860/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 2.8469 - val_acc: 0.6087\n",
      "Epoch 861/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 2.7949 - val_acc: 0.6196\n",
      "Epoch 862/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 2.8341 - val_acc: 0.6159\n",
      "Epoch 863/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 2.8497 - val_acc: 0.6087\n",
      "Epoch 864/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 2.8511 - val_acc: 0.6141\n",
      "Epoch 865/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 2.8281 - val_acc: 0.6141\n",
      "Epoch 866/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 2.8378 - val_acc: 0.6105\n",
      "Epoch 867/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 2.8554 - val_acc: 0.6069\n",
      "Epoch 868/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 2.8441 - val_acc: 0.6178\n",
      "Epoch 869/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 2.8442 - val_acc: 0.6196\n",
      "Epoch 870/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 2.8493 - val_acc: 0.6123\n",
      "Epoch 871/1200\n",
      "1287/1287 [==============================] - 3s 3ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 2.8664 - val_acc: 0.6141\n",
      "Epoch 872/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 2.8700 - val_acc: 0.6196\n",
      "Epoch 873/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 2.8707 - val_acc: 0.6123\n",
      "Epoch 874/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 2.8897 - val_acc: 0.6123\n",
      "Epoch 875/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 2.8867 - val_acc: 0.6105\n",
      "Epoch 876/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 2.9095 - val_acc: 0.6087\n",
      "Epoch 877/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 2.9090 - val_acc: 0.6087\n",
      "Epoch 878/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 2.8932 - val_acc: 0.6141\n",
      "Epoch 879/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 2.8922 - val_acc: 0.6069\n",
      "Epoch 880/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 2.9252 - val_acc: 0.6105\n",
      "Epoch 881/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 2.9031 - val_acc: 0.6123\n",
      "Epoch 882/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.9332 - val_acc: 0.6051\n",
      "Epoch 883/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.9300 - val_acc: 0.6105\n",
      "Epoch 884/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.9531 - val_acc: 0.6123\n",
      "Epoch 885/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.9231 - val_acc: 0.6123\n",
      "Epoch 886/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.9423 - val_acc: 0.6105\n",
      "Epoch 887/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.9222 - val_acc: 0.6123\n",
      "Epoch 888/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.9416 - val_acc: 0.6178\n",
      "Epoch 889/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.9506 - val_acc: 0.6014\n",
      "Epoch 890/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.9285 - val_acc: 0.6196\n",
      "Epoch 891/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.9732 - val_acc: 0.6105\n",
      "Epoch 892/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 2.9741 - val_acc: 0.6087\n",
      "Epoch 893/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.9548 - val_acc: 0.6159\n",
      "Epoch 894/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.9720 - val_acc: 0.6159\n",
      "Epoch 895/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 2.9803 - val_acc: 0.6105\n",
      "Epoch 896/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 3.0780 - val_acc: 0.5942\n",
      "Epoch 897/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 2.9342 - val_acc: 0.6105\n",
      "Epoch 898/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.9538 - val_acc: 0.6159\n",
      "Epoch 899/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 9.1751e-04 - acc: 1.0000 - val_loss: 2.9670 - val_acc: 0.6141\n",
      "Epoch 900/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 9.9798e-04 - acc: 1.0000 - val_loss: 2.9893 - val_acc: 0.6141\n",
      "Epoch 901/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 8.6579e-04 - acc: 1.0000 - val_loss: 2.9961 - val_acc: 0.6087\n",
      "Epoch 902/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 8.5131e-04 - acc: 1.0000 - val_loss: 2.9655 - val_acc: 0.6141\n",
      "Epoch 903/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 8.6810e-04 - acc: 1.0000 - val_loss: 3.0227 - val_acc: 0.6051\n",
      "Epoch 904/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 8.2864e-04 - acc: 1.0000 - val_loss: 2.9993 - val_acc: 0.6123\n",
      "Epoch 905/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 8.6309e-04 - acc: 1.0000 - val_loss: 3.0008 - val_acc: 0.6087\n",
      "Epoch 906/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 8.3323e-04 - acc: 1.0000 - val_loss: 3.0128 - val_acc: 0.6141\n",
      "Epoch 907/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 8.2232e-04 - acc: 1.0000 - val_loss: 3.0131 - val_acc: 0.6159\n",
      "Epoch 908/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.9224e-04 - acc: 1.0000 - val_loss: 3.0135 - val_acc: 0.6123\n",
      "Epoch 909/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.4709e-04 - acc: 1.0000 - val_loss: 3.0192 - val_acc: 0.6159\n",
      "Epoch 910/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.5075e-04 - acc: 1.0000 - val_loss: 3.0418 - val_acc: 0.6178\n",
      "Epoch 911/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.0313e-04 - acc: 1.0000 - val_loss: 3.0420 - val_acc: 0.6087\n",
      "Epoch 912/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0056 - acc: 0.9984 - val_loss: 2.9539 - val_acc: 0.5960\n",
      "Epoch 913/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0364 - acc: 0.9876 - val_loss: 2.8542 - val_acc: 0.6051\n",
      "Epoch 914/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 2.8465 - val_acc: 0.6105\n",
      "Epoch 915/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 2.8981 - val_acc: 0.6105\n",
      "Epoch 916/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.9176 - val_acc: 0.6105\n",
      "Epoch 917/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 2.9435 - val_acc: 0.6051\n",
      "Epoch 918/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 9.3914e-04 - acc: 1.0000 - val_loss: 2.9441 - val_acc: 0.6159\n",
      "Epoch 919/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 9.0723e-04 - acc: 1.0000 - val_loss: 2.9626 - val_acc: 0.6105\n",
      "Epoch 920/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 8.3917e-04 - acc: 1.0000 - val_loss: 2.9736 - val_acc: 0.6069\n",
      "Epoch 921/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 8.1981e-04 - acc: 1.0000 - val_loss: 2.9868 - val_acc: 0.6105\n",
      "Epoch 922/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.9521e-04 - acc: 1.0000 - val_loss: 2.9849 - val_acc: 0.6123\n",
      "Epoch 923/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.7703e-04 - acc: 1.0000 - val_loss: 2.9878 - val_acc: 0.6105\n",
      "Epoch 924/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.5825e-04 - acc: 1.0000 - val_loss: 2.9949 - val_acc: 0.6123\n",
      "Epoch 925/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.4527e-04 - acc: 1.0000 - val_loss: 2.9961 - val_acc: 0.6105\n",
      "Epoch 926/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.3108e-04 - acc: 1.0000 - val_loss: 2.9947 - val_acc: 0.6087\n",
      "Epoch 927/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.2322e-04 - acc: 1.0000 - val_loss: 2.9991 - val_acc: 0.6087\n",
      "Epoch 928/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.0889e-04 - acc: 1.0000 - val_loss: 3.0011 - val_acc: 0.6087\n",
      "Epoch 929/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.0142e-04 - acc: 1.0000 - val_loss: 3.0010 - val_acc: 0.6069\n",
      "Epoch 930/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.9502e-04 - acc: 1.0000 - val_loss: 2.9916 - val_acc: 0.6123\n",
      "Epoch 931/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.8049e-04 - acc: 1.0000 - val_loss: 3.0084 - val_acc: 0.6087\n",
      "Epoch 932/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.7730e-04 - acc: 1.0000 - val_loss: 3.0013 - val_acc: 0.6087\n",
      "Epoch 933/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.7334e-04 - acc: 1.0000 - val_loss: 2.9975 - val_acc: 0.6105\n",
      "Epoch 934/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.6694e-04 - acc: 1.0000 - val_loss: 2.9993 - val_acc: 0.6105\n",
      "Epoch 935/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.6192e-04 - acc: 1.0000 - val_loss: 2.9983 - val_acc: 0.6105\n",
      "Epoch 936/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.5088e-04 - acc: 1.0000 - val_loss: 3.0074 - val_acc: 0.6087\n",
      "Epoch 937/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 6.4623e-04 - acc: 1.0000 - val_loss: 3.0117 - val_acc: 0.6014\n",
      "Epoch 938/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.4376e-04 - acc: 1.0000 - val_loss: 3.0156 - val_acc: 0.6014\n",
      "Epoch 939/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.4336e-04 - acc: 1.0000 - val_loss: 3.0213 - val_acc: 0.5996\n",
      "Epoch 940/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.4027e-04 - acc: 1.0000 - val_loss: 3.0151 - val_acc: 0.6033\n",
      "Epoch 941/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.2681e-04 - acc: 1.0000 - val_loss: 3.0098 - val_acc: 0.6087\n",
      "Epoch 942/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.1942e-04 - acc: 1.0000 - val_loss: 3.0087 - val_acc: 0.6069\n",
      "Epoch 943/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.1632e-04 - acc: 1.0000 - val_loss: 3.0189 - val_acc: 0.6014\n",
      "Epoch 944/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.1423e-04 - acc: 1.0000 - val_loss: 3.0156 - val_acc: 0.6069\n",
      "Epoch 945/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.0649e-04 - acc: 1.0000 - val_loss: 3.0187 - val_acc: 0.6014\n",
      "Epoch 946/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.0884e-04 - acc: 1.0000 - val_loss: 3.0253 - val_acc: 0.5996\n",
      "Epoch 947/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.0711e-04 - acc: 1.0000 - val_loss: 3.0186 - val_acc: 0.6051\n",
      "Epoch 948/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.0267e-04 - acc: 1.0000 - val_loss: 3.0178 - val_acc: 0.6051\n",
      "Epoch 949/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.9576e-04 - acc: 1.0000 - val_loss: 3.0221 - val_acc: 0.6033\n",
      "Epoch 950/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.9392e-04 - acc: 1.0000 - val_loss: 3.0239 - val_acc: 0.6069\n",
      "Epoch 951/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.9553e-04 - acc: 1.0000 - val_loss: 3.0260 - val_acc: 0.6051\n",
      "Epoch 952/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.8195e-04 - acc: 1.0000 - val_loss: 3.0314 - val_acc: 0.6014\n",
      "Epoch 953/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.7958e-04 - acc: 1.0000 - val_loss: 3.0249 - val_acc: 0.6014\n",
      "Epoch 954/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.6833e-04 - acc: 1.0000 - val_loss: 3.0194 - val_acc: 0.6069\n",
      "Epoch 955/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.7081e-04 - acc: 1.0000 - val_loss: 3.0318 - val_acc: 0.6051\n",
      "Epoch 956/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.6293e-04 - acc: 1.0000 - val_loss: 3.0292 - val_acc: 0.6051\n",
      "Epoch 957/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.6949e-04 - acc: 1.0000 - val_loss: 3.0325 - val_acc: 0.6033\n",
      "Epoch 958/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.5672e-04 - acc: 1.0000 - val_loss: 3.0370 - val_acc: 0.6069\n",
      "Epoch 959/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.6281e-04 - acc: 1.0000 - val_loss: 3.0367 - val_acc: 0.6087\n",
      "Epoch 960/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.5483e-04 - acc: 1.0000 - val_loss: 3.0438 - val_acc: 0.6033\n",
      "Epoch 961/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.4667e-04 - acc: 1.0000 - val_loss: 3.0459 - val_acc: 0.6033\n",
      "Epoch 962/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.4293e-04 - acc: 1.0000 - val_loss: 3.0546 - val_acc: 0.6033\n",
      "Epoch 963/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.4917e-04 - acc: 1.0000 - val_loss: 3.0532 - val_acc: 0.6051\n",
      "Epoch 964/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.3881e-04 - acc: 1.0000 - val_loss: 3.0555 - val_acc: 0.6014\n",
      "Epoch 965/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.3568e-04 - acc: 1.0000 - val_loss: 3.0513 - val_acc: 0.6051\n",
      "Epoch 966/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.2666e-04 - acc: 1.0000 - val_loss: 3.0502 - val_acc: 0.6087\n",
      "Epoch 967/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.2163e-04 - acc: 1.0000 - val_loss: 3.0571 - val_acc: 0.6033\n",
      "Epoch 968/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.1491e-04 - acc: 1.0000 - val_loss: 3.0457 - val_acc: 0.6123\n",
      "Epoch 969/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.2283e-04 - acc: 1.0000 - val_loss: 3.0627 - val_acc: 0.6014\n",
      "Epoch 970/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.1233e-04 - acc: 1.0000 - val_loss: 3.0509 - val_acc: 0.6123\n",
      "Epoch 971/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.1157e-04 - acc: 1.0000 - val_loss: 3.0574 - val_acc: 0.6105\n",
      "Epoch 972/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.0541e-04 - acc: 1.0000 - val_loss: 3.0685 - val_acc: 0.6014\n",
      "Epoch 973/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.2132e-04 - acc: 1.0000 - val_loss: 3.0702 - val_acc: 0.6087\n",
      "Epoch 974/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.0229e-04 - acc: 1.0000 - val_loss: 3.0697 - val_acc: 0.6087\n",
      "Epoch 975/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.9542e-04 - acc: 1.0000 - val_loss: 3.0698 - val_acc: 0.6069\n",
      "Epoch 976/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.9709e-04 - acc: 1.0000 - val_loss: 3.0805 - val_acc: 0.6033\n",
      "Epoch 977/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.9921e-04 - acc: 1.0000 - val_loss: 3.0731 - val_acc: 0.6087\n",
      "Epoch 978/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.9384e-04 - acc: 1.0000 - val_loss: 3.0726 - val_acc: 0.6105\n",
      "Epoch 979/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.7905e-04 - acc: 1.0000 - val_loss: 3.0790 - val_acc: 0.6069\n",
      "Epoch 980/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.8345e-04 - acc: 1.0000 - val_loss: 3.0808 - val_acc: 0.6033\n",
      "Epoch 981/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.7409e-04 - acc: 1.0000 - val_loss: 3.0825 - val_acc: 0.6069\n",
      "Epoch 982/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.7368e-04 - acc: 1.0000 - val_loss: 3.0870 - val_acc: 0.6087\n",
      "Epoch 983/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.7206e-04 - acc: 1.0000 - val_loss: 3.0784 - val_acc: 0.6105\n",
      "Epoch 984/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.6446e-04 - acc: 1.0000 - val_loss: 3.0797 - val_acc: 0.6105\n",
      "Epoch 985/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.7890e-04 - acc: 1.0000 - val_loss: 3.0778 - val_acc: 0.6123\n",
      "Epoch 986/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.5898e-04 - acc: 1.0000 - val_loss: 3.0889 - val_acc: 0.6123\n",
      "Epoch 987/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.6157e-04 - acc: 1.0000 - val_loss: 3.0943 - val_acc: 0.6123\n",
      "Epoch 988/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.7574e-04 - acc: 1.0000 - val_loss: 3.0935 - val_acc: 0.6105\n",
      "Epoch 989/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.4712e-04 - acc: 1.0000 - val_loss: 3.1096 - val_acc: 0.6033\n",
      "Epoch 990/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.6697e-04 - acc: 1.0000 - val_loss: 3.1049 - val_acc: 0.6033\n",
      "Epoch 991/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.4234e-04 - acc: 1.0000 - val_loss: 3.1131 - val_acc: 0.6069\n",
      "Epoch 992/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.3230e-04 - acc: 1.0000 - val_loss: 3.0962 - val_acc: 0.6123\n",
      "Epoch 993/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.2803e-04 - acc: 1.0000 - val_loss: 3.1062 - val_acc: 0.6105\n",
      "Epoch 994/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.7779e-04 - acc: 1.0000 - val_loss: 3.0959 - val_acc: 0.6069\n",
      "Epoch 995/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.3129e-04 - acc: 1.0000 - val_loss: 3.1163 - val_acc: 0.6033\n",
      "Epoch 996/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.1733e-04 - acc: 1.0000 - val_loss: 3.1169 - val_acc: 0.6069\n",
      "Epoch 997/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.1466e-04 - acc: 1.0000 - val_loss: 3.1173 - val_acc: 0.6087\n",
      "Epoch 998/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.1084e-04 - acc: 1.0000 - val_loss: 3.1159 - val_acc: 0.6123\n",
      "Epoch 999/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.2273e-04 - acc: 1.0000 - val_loss: 3.1307 - val_acc: 0.6069\n",
      "Epoch 1000/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.0972e-04 - acc: 1.0000 - val_loss: 3.1389 - val_acc: 0.6033\n",
      "Epoch 1001/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.0470e-04 - acc: 1.0000 - val_loss: 3.1183 - val_acc: 0.6123\n",
      "Epoch 1002/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 3.9889e-04 - acc: 1.0000 - val_loss: 3.1281 - val_acc: 0.6051\n",
      "Epoch 1003/1200\n",
      "1287/1287 [==============================] - 3s 3ms/step - loss: 3.9782e-04 - acc: 1.0000 - val_loss: 3.1322 - val_acc: 0.6087\n",
      "Epoch 1004/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.0292e-04 - acc: 1.0000 - val_loss: 3.1248 - val_acc: 0.6105\n",
      "Epoch 1005/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.0717e-04 - acc: 1.0000 - val_loss: 3.1271 - val_acc: 0.6123\n",
      "Epoch 1006/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.9644e-04 - acc: 1.0000 - val_loss: 3.1343 - val_acc: 0.6105\n",
      "Epoch 1007/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.8073e-04 - acc: 1.0000 - val_loss: 3.1392 - val_acc: 0.6087\n",
      "Epoch 1008/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.8679e-04 - acc: 1.0000 - val_loss: 3.1252 - val_acc: 0.6123\n",
      "Epoch 1009/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.8417e-04 - acc: 1.0000 - val_loss: 3.1421 - val_acc: 0.6069\n",
      "Epoch 1010/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.7884e-04 - acc: 1.0000 - val_loss: 3.1416 - val_acc: 0.6123\n",
      "Epoch 1011/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.7921e-04 - acc: 1.0000 - val_loss: 3.1491 - val_acc: 0.6069\n",
      "Epoch 1012/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.6866e-04 - acc: 1.0000 - val_loss: 3.1622 - val_acc: 0.6087\n",
      "Epoch 1013/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.5359e-04 - acc: 1.0000 - val_loss: 3.1366 - val_acc: 0.6105\n",
      "Epoch 1014/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.5508e-04 - acc: 1.0000 - val_loss: 3.1554 - val_acc: 0.6105\n",
      "Epoch 1015/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.6367e-04 - acc: 1.0000 - val_loss: 3.1501 - val_acc: 0.6123\n",
      "Epoch 1016/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.4828e-04 - acc: 1.0000 - val_loss: 3.1608 - val_acc: 0.6123\n",
      "Epoch 1017/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.4473e-04 - acc: 1.0000 - val_loss: 3.1660 - val_acc: 0.6105\n",
      "Epoch 1018/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.3914e-04 - acc: 1.0000 - val_loss: 3.1694 - val_acc: 0.6105\n",
      "Epoch 1019/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.3087e-04 - acc: 1.0000 - val_loss: 3.1662 - val_acc: 0.6123\n",
      "Epoch 1020/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.3460e-04 - acc: 1.0000 - val_loss: 3.1699 - val_acc: 0.6087\n",
      "Epoch 1021/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.4989e-04 - acc: 1.0000 - val_loss: 3.1928 - val_acc: 0.6087\n",
      "Epoch 1022/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.4379e-04 - acc: 1.0000 - val_loss: 3.1777 - val_acc: 0.6105\n",
      "Epoch 1023/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.3735e-04 - acc: 1.0000 - val_loss: 3.1782 - val_acc: 0.6105\n",
      "Epoch 1024/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.1426e-04 - acc: 1.0000 - val_loss: 3.1759 - val_acc: 0.6123\n",
      "Epoch 1025/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.2315e-04 - acc: 1.0000 - val_loss: 3.2022 - val_acc: 0.6105\n",
      "Epoch 1026/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.4782e-04 - acc: 1.0000 - val_loss: 3.1961 - val_acc: 0.6087\n",
      "Epoch 1027/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.1769e-04 - acc: 1.0000 - val_loss: 3.1879 - val_acc: 0.6123\n",
      "Epoch 1028/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.0867e-04 - acc: 1.0000 - val_loss: 3.1855 - val_acc: 0.6123\n",
      "Epoch 1029/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.3219e-04 - acc: 1.0000 - val_loss: 3.1899 - val_acc: 0.6123\n",
      "Epoch 1030/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.0240e-04 - acc: 1.0000 - val_loss: 3.1906 - val_acc: 0.6123\n",
      "Epoch 1031/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.9687e-04 - acc: 1.0000 - val_loss: 3.1871 - val_acc: 0.6141\n",
      "Epoch 1032/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.9235e-04 - acc: 1.0000 - val_loss: 3.2003 - val_acc: 0.6105\n",
      "Epoch 1033/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.8714e-04 - acc: 1.0000 - val_loss: 3.2060 - val_acc: 0.6123\n",
      "Epoch 1034/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.2025e-04 - acc: 1.0000 - val_loss: 3.2344 - val_acc: 0.6105\n",
      "Epoch 1035/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.8998e-04 - acc: 1.0000 - val_loss: 3.1928 - val_acc: 0.6105\n",
      "Epoch 1036/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.8924e-04 - acc: 1.0000 - val_loss: 3.2319 - val_acc: 0.6069\n",
      "Epoch 1037/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.9169e-04 - acc: 1.0000 - val_loss: 3.2160 - val_acc: 0.6105\n",
      "Epoch 1038/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.7264e-04 - acc: 1.0000 - val_loss: 3.2212 - val_acc: 0.6123\n",
      "Epoch 1039/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.6250e-04 - acc: 1.0000 - val_loss: 3.2195 - val_acc: 0.6087\n",
      "Epoch 1040/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.6560e-04 - acc: 1.0000 - val_loss: 3.2271 - val_acc: 0.6141\n",
      "Epoch 1041/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.6279e-04 - acc: 1.0000 - val_loss: 3.2300 - val_acc: 0.6123\n",
      "Epoch 1042/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.5557e-04 - acc: 1.0000 - val_loss: 3.2382 - val_acc: 0.6105\n",
      "Epoch 1043/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.7609e-04 - acc: 1.0000 - val_loss: 3.2451 - val_acc: 0.6069\n",
      "Epoch 1044/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.6686e-04 - acc: 1.0000 - val_loss: 3.2248 - val_acc: 0.6159\n",
      "Epoch 1045/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.6986e-04 - acc: 1.0000 - val_loss: 3.2294 - val_acc: 0.6159\n",
      "Epoch 1046/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.4702e-04 - acc: 1.0000 - val_loss: 3.2307 - val_acc: 0.6159\n",
      "Epoch 1047/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.4392e-04 - acc: 1.0000 - val_loss: 3.2540 - val_acc: 0.6087\n",
      "Epoch 1048/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.3904e-04 - acc: 1.0000 - val_loss: 3.2386 - val_acc: 0.6159\n",
      "Epoch 1049/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.3538e-04 - acc: 1.0000 - val_loss: 3.2617 - val_acc: 0.6141\n",
      "Epoch 1050/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 3.2602 - val_acc: 0.6123\n",
      "Epoch 1051/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.3440e-04 - acc: 1.0000 - val_loss: 3.2559 - val_acc: 0.6123\n",
      "Epoch 1052/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.4153e-04 - acc: 1.0000 - val_loss: 3.2577 - val_acc: 0.6141\n",
      "Epoch 1053/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.0933e-04 - acc: 1.0000 - val_loss: 3.2476 - val_acc: 0.6123\n",
      "Epoch 1054/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.3862e-04 - acc: 1.0000 - val_loss: 3.2740 - val_acc: 0.6051\n",
      "Epoch 1055/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.1577e-04 - acc: 1.0000 - val_loss: 3.2821 - val_acc: 0.6105\n",
      "Epoch 1056/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.3565e-04 - acc: 1.0000 - val_loss: 3.2858 - val_acc: 0.6087\n",
      "Epoch 1057/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.3284e-04 - acc: 1.0000 - val_loss: 3.2601 - val_acc: 0.6123\n",
      "Epoch 1058/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.1147e-04 - acc: 1.0000 - val_loss: 3.2760 - val_acc: 0.6087\n",
      "Epoch 1059/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.0754e-04 - acc: 1.0000 - val_loss: 3.2744 - val_acc: 0.6123\n",
      "Epoch 1060/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.0819e-04 - acc: 1.0000 - val_loss: 3.2858 - val_acc: 0.6069\n",
      "Epoch 1061/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.0007e-04 - acc: 1.0000 - val_loss: 3.2898 - val_acc: 0.6087\n",
      "Epoch 1062/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.1656e-04 - acc: 1.0000 - val_loss: 3.2636 - val_acc: 0.6159\n",
      "Epoch 1063/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.9906e-04 - acc: 1.0000 - val_loss: 3.3168 - val_acc: 0.6033\n",
      "Epoch 1064/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.9145e-04 - acc: 1.0000 - val_loss: 3.2865 - val_acc: 0.6105\n",
      "Epoch 1065/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.8736e-04 - acc: 1.0000 - val_loss: 3.2972 - val_acc: 0.6069\n",
      "Epoch 1066/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.8214e-04 - acc: 1.0000 - val_loss: 3.3289 - val_acc: 0.6105\n",
      "Epoch 1067/1200\n",
      "1287/1287 [==============================] - 3s 3ms/step - loss: 1.9270e-04 - acc: 1.0000 - val_loss: 3.2935 - val_acc: 0.6178\n",
      "Epoch 1068/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 1.7930e-04 - acc: 1.0000 - val_loss: 3.2999 - val_acc: 0.6123\n",
      "Epoch 1069/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.8494e-04 - acc: 1.0000 - val_loss: 3.3125 - val_acc: 0.6105\n",
      "Epoch 1070/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.8080e-04 - acc: 1.0000 - val_loss: 3.2963 - val_acc: 0.6123\n",
      "Epoch 1071/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.7469e-04 - acc: 1.0000 - val_loss: 3.3033 - val_acc: 0.6159\n",
      "Epoch 1072/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.7704e-04 - acc: 1.0000 - val_loss: 3.3052 - val_acc: 0.6105\n",
      "Epoch 1073/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.7474e-04 - acc: 1.0000 - val_loss: 3.3066 - val_acc: 0.6141\n",
      "Epoch 1074/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.6528e-04 - acc: 1.0000 - val_loss: 3.3267 - val_acc: 0.6123\n",
      "Epoch 1075/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.8252e-04 - acc: 1.0000 - val_loss: 3.3283 - val_acc: 0.6105\n",
      "Epoch 1076/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.6961e-04 - acc: 1.0000 - val_loss: 3.3349 - val_acc: 0.6123\n",
      "Epoch 1077/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.5795e-04 - acc: 1.0000 - val_loss: 3.3373 - val_acc: 0.6105\n",
      "Epoch 1078/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.5456e-04 - acc: 1.0000 - val_loss: 3.3468 - val_acc: 0.6105\n",
      "Epoch 1079/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.5058e-04 - acc: 1.0000 - val_loss: 3.3504 - val_acc: 0.6069\n",
      "Epoch 1080/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.4918e-04 - acc: 1.0000 - val_loss: 3.3472 - val_acc: 0.6069\n",
      "Epoch 1081/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.4731e-04 - acc: 1.0000 - val_loss: 3.3256 - val_acc: 0.6123\n",
      "Epoch 1082/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.4983e-04 - acc: 1.0000 - val_loss: 3.3368 - val_acc: 0.6141\n",
      "Epoch 1083/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.4544e-04 - acc: 1.0000 - val_loss: 3.3521 - val_acc: 0.6105\n",
      "Epoch 1084/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.4013e-04 - acc: 1.0000 - val_loss: 3.3595 - val_acc: 0.6069\n",
      "Epoch 1085/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.4272e-04 - acc: 1.0000 - val_loss: 3.3385 - val_acc: 0.6159\n",
      "Epoch 1086/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.3755e-04 - acc: 1.0000 - val_loss: 3.3627 - val_acc: 0.6123\n",
      "Epoch 1087/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.5420e-04 - acc: 1.0000 - val_loss: 3.3268 - val_acc: 0.6178\n",
      "Epoch 1088/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.4711e-04 - acc: 1.0000 - val_loss: 3.3549 - val_acc: 0.6159\n",
      "Epoch 1089/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.3362e-04 - acc: 1.0000 - val_loss: 3.3775 - val_acc: 0.6105\n",
      "Epoch 1090/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.2911e-04 - acc: 1.0000 - val_loss: 3.3624 - val_acc: 0.6178\n",
      "Epoch 1091/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.2949e-04 - acc: 1.0000 - val_loss: 3.3599 - val_acc: 0.6159\n",
      "Epoch 1092/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.2555e-04 - acc: 1.0000 - val_loss: 3.3743 - val_acc: 0.6069\n",
      "Epoch 1093/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.2703e-04 - acc: 1.0000 - val_loss: 3.3905 - val_acc: 0.6051\n",
      "Epoch 1094/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.2688e-04 - acc: 1.0000 - val_loss: 3.3976 - val_acc: 0.6051\n",
      "Epoch 1095/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.1998e-04 - acc: 1.0000 - val_loss: 3.3882 - val_acc: 0.6087\n",
      "Epoch 1096/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.2035e-04 - acc: 1.0000 - val_loss: 3.3948 - val_acc: 0.6105\n",
      "Epoch 1097/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.1764e-04 - acc: 1.0000 - val_loss: 3.3894 - val_acc: 0.6087\n",
      "Epoch 1098/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.1766e-04 - acc: 1.0000 - val_loss: 3.3923 - val_acc: 0.6105\n",
      "Epoch 1099/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.1242e-04 - acc: 1.0000 - val_loss: 3.4106 - val_acc: 0.6087\n",
      "Epoch 1100/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.0754e-04 - acc: 1.0000 - val_loss: 3.4063 - val_acc: 0.6141\n",
      "Epoch 1101/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.1103e-04 - acc: 1.0000 - val_loss: 3.3937 - val_acc: 0.6141\n",
      "Epoch 1102/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.0688e-04 - acc: 1.0000 - val_loss: 3.4290 - val_acc: 0.6105\n",
      "Epoch 1103/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.0234e-04 - acc: 1.0000 - val_loss: 3.4280 - val_acc: 0.6051\n",
      "Epoch 1104/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 9.9243e-05 - acc: 1.0000 - val_loss: 3.4047 - val_acc: 0.6159\n",
      "Epoch 1105/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 9.7854e-05 - acc: 1.0000 - val_loss: 3.4408 - val_acc: 0.6123\n",
      "Epoch 1106/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.0246e-04 - acc: 1.0000 - val_loss: 3.4197 - val_acc: 0.6105\n",
      "Epoch 1107/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.0145e-04 - acc: 1.0000 - val_loss: 3.4406 - val_acc: 0.6123\n",
      "Epoch 1108/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 9.6141e-05 - acc: 1.0000 - val_loss: 3.4341 - val_acc: 0.6051\n",
      "Epoch 1109/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 9.2673e-05 - acc: 1.0000 - val_loss: 3.4328 - val_acc: 0.6087\n",
      "Epoch 1110/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 9.4733e-05 - acc: 1.0000 - val_loss: 3.4292 - val_acc: 0.6123\n",
      "Epoch 1111/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 9.7743e-05 - acc: 1.0000 - val_loss: 3.4214 - val_acc: 0.6159\n",
      "Epoch 1112/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 9.6971e-05 - acc: 1.0000 - val_loss: 3.4440 - val_acc: 0.6123\n",
      "Epoch 1113/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 9.1573e-05 - acc: 1.0000 - val_loss: 3.4312 - val_acc: 0.6123\n",
      "Epoch 1114/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 8.8763e-05 - acc: 1.0000 - val_loss: 3.4486 - val_acc: 0.6105\n",
      "Epoch 1115/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 8.4807e-05 - acc: 1.0000 - val_loss: 3.4450 - val_acc: 0.6087\n",
      "Epoch 1116/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 8.5190e-05 - acc: 1.0000 - val_loss: 3.4515 - val_acc: 0.6087\n",
      "Epoch 1117/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.9809e-05 - acc: 1.0000 - val_loss: 3.4552 - val_acc: 0.6105\n",
      "Epoch 1118/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.9620e-05 - acc: 1.0000 - val_loss: 3.4844 - val_acc: 0.6123\n",
      "Epoch 1119/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 8.0665e-05 - acc: 1.0000 - val_loss: 3.4603 - val_acc: 0.6123\n",
      "Epoch 1120/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.7800e-05 - acc: 1.0000 - val_loss: 3.4792 - val_acc: 0.6087\n",
      "Epoch 1121/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.4105e-05 - acc: 1.0000 - val_loss: 3.4901 - val_acc: 0.6087\n",
      "Epoch 1122/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.7036e-05 - acc: 1.0000 - val_loss: 3.4987 - val_acc: 0.6069\n",
      "Epoch 1123/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.6188e-05 - acc: 1.0000 - val_loss: 3.4863 - val_acc: 0.6105\n",
      "Epoch 1124/1200\n",
      "1287/1287 [==============================] - 3s 3ms/step - loss: 6.9349e-05 - acc: 1.0000 - val_loss: 3.4966 - val_acc: 0.6069\n",
      "Epoch 1125/1200\n",
      "1287/1287 [==============================] - 3s 3ms/step - loss: 7.0076e-05 - acc: 1.0000 - val_loss: 3.4593 - val_acc: 0.6123\n",
      "Epoch 1126/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.8551e-05 - acc: 1.0000 - val_loss: 3.4691 - val_acc: 0.6178\n",
      "Epoch 1127/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.8061e-05 - acc: 1.0000 - val_loss: 3.4988 - val_acc: 0.6123\n",
      "Epoch 1128/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.6372e-05 - acc: 1.0000 - val_loss: 3.4510 - val_acc: 0.6196\n",
      "Epoch 1129/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.5161e-05 - acc: 1.0000 - val_loss: 3.4590 - val_acc: 0.6196\n",
      "Epoch 1130/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.6064e-05 - acc: 1.0000 - val_loss: 3.5098 - val_acc: 0.6123\n",
      "Epoch 1131/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.1985e-05 - acc: 1.0000 - val_loss: 3.5036 - val_acc: 0.6069\n",
      "Epoch 1132/1200\n",
      "1287/1287 [==============================] - 3s 3ms/step - loss: 6.2621e-05 - acc: 1.0000 - val_loss: 3.5158 - val_acc: 0.6105\n",
      "Epoch 1133/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 6.1620e-05 - acc: 1.0000 - val_loss: 3.5086 - val_acc: 0.6105\n",
      "Epoch 1134/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.2334e-05 - acc: 1.0000 - val_loss: 3.5472 - val_acc: 0.6087\n",
      "Epoch 1135/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 7.4732e-05 - acc: 1.0000 - val_loss: 3.5398 - val_acc: 0.6069\n",
      "Epoch 1136/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.8737e-05 - acc: 1.0000 - val_loss: 3.5139 - val_acc: 0.6123\n",
      "Epoch 1137/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.5328e-05 - acc: 1.0000 - val_loss: 3.5561 - val_acc: 0.6087\n",
      "Epoch 1138/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.5216e-05 - acc: 1.0000 - val_loss: 3.5606 - val_acc: 0.6069\n",
      "Epoch 1139/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.6685e-05 - acc: 1.0000 - val_loss: 3.5714 - val_acc: 0.6069\n",
      "Epoch 1140/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.9175e-05 - acc: 1.0000 - val_loss: 3.5589 - val_acc: 0.6105\n",
      "Epoch 1141/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.1824e-05 - acc: 1.0000 - val_loss: 3.5226 - val_acc: 0.6141\n",
      "Epoch 1142/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 5.3440e-05 - acc: 1.0000 - val_loss: 3.5441 - val_acc: 0.6087\n",
      "Epoch 1143/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.9932e-05 - acc: 1.0000 - val_loss: 3.5409 - val_acc: 0.6087\n",
      "Epoch 1144/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.7624e-05 - acc: 1.0000 - val_loss: 3.5346 - val_acc: 0.6159\n",
      "Epoch 1145/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.8243e-05 - acc: 1.0000 - val_loss: 3.5521 - val_acc: 0.6105\n",
      "Epoch 1146/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.8988e-05 - acc: 1.0000 - val_loss: 3.5129 - val_acc: 0.6159\n",
      "Epoch 1147/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.6032e-05 - acc: 1.0000 - val_loss: 3.5699 - val_acc: 0.6105\n",
      "Epoch 1148/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.5212e-05 - acc: 1.0000 - val_loss: 3.5694 - val_acc: 0.6087\n",
      "Epoch 1149/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.6805e-05 - acc: 1.0000 - val_loss: 3.5939 - val_acc: 0.6051\n",
      "Epoch 1150/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.4312e-05 - acc: 1.0000 - val_loss: 3.5762 - val_acc: 0.6159\n",
      "Epoch 1151/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.5784e-05 - acc: 1.0000 - val_loss: 3.5493 - val_acc: 0.6178\n",
      "Epoch 1152/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.5482e-05 - acc: 1.0000 - val_loss: 3.5688 - val_acc: 0.6087\n",
      "Epoch 1153/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.2602e-05 - acc: 1.0000 - val_loss: 3.5793 - val_acc: 0.6105\n",
      "Epoch 1154/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 6.3721e-05 - acc: 1.0000 - val_loss: 3.5182 - val_acc: 0.6159\n",
      "Epoch 1155/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 4.9435e-05 - acc: 1.0000 - val_loss: 3.5854 - val_acc: 0.6123\n",
      "Epoch 1156/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.8851e-05 - acc: 1.0000 - val_loss: 3.6024 - val_acc: 0.6105\n",
      "Epoch 1157/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.7396e-05 - acc: 1.0000 - val_loss: 3.5710 - val_acc: 0.6178\n",
      "Epoch 1158/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.6846e-05 - acc: 1.0000 - val_loss: 3.5891 - val_acc: 0.6105\n",
      "Epoch 1159/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.7327e-05 - acc: 1.0000 - val_loss: 3.5951 - val_acc: 0.6141\n",
      "Epoch 1160/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.4625e-05 - acc: 1.0000 - val_loss: 3.5901 - val_acc: 0.6105\n",
      "Epoch 1161/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.4710e-05 - acc: 1.0000 - val_loss: 3.6154 - val_acc: 0.6105\n",
      "Epoch 1162/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.4574e-05 - acc: 1.0000 - val_loss: 3.5890 - val_acc: 0.6196\n",
      "Epoch 1163/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.3989e-05 - acc: 1.0000 - val_loss: 3.5960 - val_acc: 0.6178\n",
      "Epoch 1164/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.3237e-05 - acc: 1.0000 - val_loss: 3.6077 - val_acc: 0.6123\n",
      "Epoch 1165/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.4949e-05 - acc: 1.0000 - val_loss: 3.5858 - val_acc: 0.6159\n",
      "Epoch 1166/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.3696e-05 - acc: 1.0000 - val_loss: 3.5961 - val_acc: 0.6196\n",
      "Epoch 1167/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.1071e-05 - acc: 1.0000 - val_loss: 3.6387 - val_acc: 0.6087\n",
      "Epoch 1168/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.2572e-05 - acc: 1.0000 - val_loss: 3.6129 - val_acc: 0.6214\n",
      "Epoch 1169/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.0849e-05 - acc: 1.0000 - val_loss: 3.6166 - val_acc: 0.6141\n",
      "Epoch 1170/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.1062e-05 - acc: 1.0000 - val_loss: 3.6351 - val_acc: 0.6105\n",
      "Epoch 1171/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.1074e-05 - acc: 1.0000 - val_loss: 3.6740 - val_acc: 0.6087\n",
      "Epoch 1172/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 3.1944e-05 - acc: 1.0000 - val_loss: 3.5972 - val_acc: 0.6232\n",
      "Epoch 1173/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.9053e-05 - acc: 1.0000 - val_loss: 3.6378 - val_acc: 0.6123\n",
      "Epoch 1174/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.6731e-05 - acc: 1.0000 - val_loss: 3.6337 - val_acc: 0.6105\n",
      "Epoch 1175/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.6507e-05 - acc: 1.0000 - val_loss: 3.6392 - val_acc: 0.6069\n",
      "Epoch 1176/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.8675e-05 - acc: 1.0000 - val_loss: 3.6386 - val_acc: 0.6141\n",
      "Epoch 1177/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.5813e-05 - acc: 1.0000 - val_loss: 3.6578 - val_acc: 0.6196\n",
      "Epoch 1178/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.6021e-05 - acc: 1.0000 - val_loss: 3.6824 - val_acc: 0.6105\n",
      "Epoch 1179/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.8821e-05 - acc: 1.0000 - val_loss: 3.6367 - val_acc: 0.6141\n",
      "Epoch 1180/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.5073e-05 - acc: 1.0000 - val_loss: 3.6625 - val_acc: 0.6141\n",
      "Epoch 1181/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.3840e-05 - acc: 1.0000 - val_loss: 3.6658 - val_acc: 0.6123\n",
      "Epoch 1182/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.4563e-05 - acc: 1.0000 - val_loss: 3.6476 - val_acc: 0.6178\n",
      "Epoch 1183/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.3590e-05 - acc: 1.0000 - val_loss: 3.6539 - val_acc: 0.6087\n",
      "Epoch 1184/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.2991e-05 - acc: 1.0000 - val_loss: 3.6615 - val_acc: 0.6105\n",
      "Epoch 1185/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.4057e-05 - acc: 1.0000 - val_loss: 3.6333 - val_acc: 0.6178\n",
      "Epoch 1186/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.3227e-05 - acc: 1.0000 - val_loss: 3.6554 - val_acc: 0.6196\n",
      "Epoch 1187/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.2809e-05 - acc: 1.0000 - val_loss: 3.6649 - val_acc: 0.6105\n",
      "Epoch 1188/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.2258e-05 - acc: 1.0000 - val_loss: 3.6439 - val_acc: 0.6232\n",
      "Epoch 1189/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.1340e-05 - acc: 1.0000 - val_loss: 3.6584 - val_acc: 0.6232\n",
      "Epoch 1190/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.0473e-05 - acc: 1.0000 - val_loss: 3.6947 - val_acc: 0.6159\n",
      "Epoch 1191/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 2.1021e-05 - acc: 1.0000 - val_loss: 3.6819 - val_acc: 0.6087\n",
      "Epoch 1192/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.9044e-05 - acc: 1.0000 - val_loss: 3.6941 - val_acc: 0.6178\n",
      "Epoch 1193/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.9250e-05 - acc: 1.0000 - val_loss: 3.6988 - val_acc: 0.6159\n",
      "Epoch 1194/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.8551e-05 - acc: 1.0000 - val_loss: 3.7017 - val_acc: 0.6178\n",
      "Epoch 1195/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.8860e-05 - acc: 1.0000 - val_loss: 3.6925 - val_acc: 0.6105\n",
      "Epoch 1196/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.8820e-05 - acc: 1.0000 - val_loss: 3.6835 - val_acc: 0.6159\n",
      "Epoch 1197/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.8027e-05 - acc: 1.0000 - val_loss: 3.6997 - val_acc: 0.6141\n",
      "Epoch 1198/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 1.7942e-05 - acc: 1.0000 - val_loss: 3.6686 - val_acc: 0.6286\n",
      "Epoch 1199/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.7709e-05 - acc: 1.0000 - val_loss: 3.7209 - val_acc: 0.6178\n",
      "Epoch 1200/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 1.6350e-05 - acc: 1.0000 - val_loss: 3.7202 - val_acc: 0.6159\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.00001)\n",
    "# from keras.optimizers import SGD\n",
    "# opt = SGD(lr=0.00001)\n",
    "\n",
    "model1.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_9 = model1.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=1200,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_model_1 = model1.save('../models/grayscale_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
