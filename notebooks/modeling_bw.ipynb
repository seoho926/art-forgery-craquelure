{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.16.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.3.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (3.12)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/36/67f809c135c17ec9b8276466cc57f35b98c240f55c780689ea29fa32f512/pip-20.0.1-py2.py3-none-any.whl (1.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.5MB 23.8MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 10.0.1\n",
      "    Uninstalling pip-10.0.1:\n",
      "      Successfully uninstalled pip-10.0.1\n",
      "Successfully installed pip-20.0.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install keras\n",
    "# !pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 3 classes.\n",
      "Found 1815 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Directory path\n",
    "train_data_dir = '../data/bwcracks/train'\n",
    "test_data_dir = '../data/bwcracks/test'\n",
    "\n",
    "# Get all the data in the directory data/validation, and reshape them\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "        test_data_dir, \n",
    "        target_size=(256, 256), batch_size=463)\n",
    "\n",
    "# Get all the data in the directory data/train, and reshape them\n",
    "train_generator = ImageDataGenerator().flow_from_directory(\n",
    "        train_data_dir, \n",
    "        target_size=(256, 256), batch_size=1814)\n",
    "\n",
    "# Create the datasets\n",
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=256x256 at 0x7FB194436D30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previewing an image\n",
    "array_to_img(train_images[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1814, 256, 256, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (1814, 256, 256, 3) (1814, 2)\n",
      "test data shape: (0, 256, 256, 3) (0, 3)\n"
     ]
    }
   ],
   "source": [
    "# Checking shape of data\n",
    "print('train data shape:', np.shape(train_images), np.shape(train_labels))\n",
    "print('test data shape:', np.shape(test_images), np.shape(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_img: (1814, 196608)\n"
     ]
    }
   ],
   "source": [
    "# Unrowing/reshaping\n",
    "train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "print('train_img:', np.shape(train_img))\n",
    "\n",
    "# test_img = test_images.reshape(test_images.shape[0], -1)\n",
    "# print('test_img:', np.shape(test_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the labels\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dutch': 0, 'Flemish': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels final: (1814, 1)\n"
     ]
    }
   ],
   "source": [
    "# Transposing the labels\n",
    "train_y = np.reshape(train_labels[:,0], (1814,1))\n",
    "print('train labels final:', np.shape(train_y))\n",
    "\n",
    "# test_y = np.reshape(test_labels[:,0], (463,1))\n",
    "# print('test labels final:', np.shape(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(123)\n",
    "# model = models.Sequential()\n",
    "# model.add(layers.Dense(20, activation='relu', input_shape=(65536,))) #2 hidden layers\n",
    "# model.add(layers.Dense(10, activation='relu'))\n",
    "# model.add(layers.Dense(5, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# histoire = model.fit(train_img,\n",
    "#                     train_y,\n",
    "#                     epochs=5,\n",
    "#                     batch_size=100,\n",
    "#                     validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model1 = models.Sequential()\n",
    "model1.add(layers.Conv2D(filters=10, kernel_size=10, strides=2, activation='relu',\n",
    "                        input_shape=(256, 256,  3)))\n",
    "model1.add(layers.MaxPooling2D((10, 10)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=10, kernel_size=5, strides=2,activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((4, 4)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=10, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=10, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=10, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "\n",
    "model1.add(layers.Flatten())\n",
    "# model.add(layers.Dropout(0.2))\n",
    "model1.add(layers.Dense(20, activation='relu'))\n",
    "model1.add(layers.Dense(100, activation='relu'))\n",
    "model1.add(layers.Dense(200, activation='relu'))\n",
    "# model.add(layers.Dense(200, activation='relu'))\n",
    "# model.add(layers.Dense(200, activation='relu'))\n",
    "# model.add(layers.Dense(200, activation='relu'))\n",
    "model1.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 1269 samples, validate on 545 samples\n",
      "Epoch 1/1200\n",
      "1269/1269 [==============================] - 7s 5ms/step - loss: 3.4624 - acc: 0.5359 - val_loss: 2.7486 - val_acc: 0.5963\n",
      "Epoch 2/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 2.4435 - acc: 0.5705 - val_loss: 2.0724 - val_acc: 0.5670\n",
      "Epoch 3/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 1.7559 - acc: 0.5776 - val_loss: 1.7080 - val_acc: 0.5651\n",
      "Epoch 4/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 1.4375 - acc: 0.5950 - val_loss: 1.5051 - val_acc: 0.5413\n",
      "Epoch 5/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 1.2740 - acc: 0.5973 - val_loss: 1.3802 - val_acc: 0.5523\n",
      "Epoch 6/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 1.1675 - acc: 0.6036 - val_loss: 1.3197 - val_acc: 0.5578\n",
      "Epoch 7/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 1.1125 - acc: 0.5989 - val_loss: 1.2431 - val_acc: 0.5303\n",
      "Epoch 8/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 1.0471 - acc: 0.6036 - val_loss: 1.2154 - val_acc: 0.5266\n",
      "Epoch 9/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.9880 - acc: 0.6115 - val_loss: 1.1518 - val_acc: 0.5560\n",
      "Epoch 10/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.9265 - acc: 0.6036 - val_loss: 1.0954 - val_acc: 0.5927\n",
      "Epoch 11/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.8804 - acc: 0.6099 - val_loss: 1.1106 - val_acc: 0.5138\n",
      "Epoch 12/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.8480 - acc: 0.5965 - val_loss: 0.9899 - val_acc: 0.5853\n",
      "Epoch 13/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.7985 - acc: 0.6013 - val_loss: 0.9747 - val_acc: 0.5982\n",
      "Epoch 14/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.7696 - acc: 0.6028 - val_loss: 0.9390 - val_acc: 0.6037\n",
      "Epoch 15/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.7470 - acc: 0.6099 - val_loss: 0.9158 - val_acc: 0.5872\n",
      "Epoch 16/1200\n",
      "1269/1269 [==============================] - 7s 5ms/step - loss: 0.7286 - acc: 0.6194 - val_loss: 0.9063 - val_acc: 0.5615\n",
      "Epoch 17/1200\n",
      "1269/1269 [==============================] - 7s 6ms/step - loss: 0.7145 - acc: 0.6154 - val_loss: 0.8863 - val_acc: 0.5725\n",
      "Epoch 18/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.7019 - acc: 0.6351 - val_loss: 0.8980 - val_acc: 0.5193\n",
      "Epoch 19/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.6942 - acc: 0.6288 - val_loss: 0.8742 - val_acc: 0.5578\n",
      "Epoch 20/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.6902 - acc: 0.6273 - val_loss: 0.8613 - val_acc: 0.6147\n",
      "Epoch 21/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.6745 - acc: 0.6312 - val_loss: 0.8641 - val_acc: 0.5541\n",
      "Epoch 22/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.6666 - acc: 0.6241 - val_loss: 0.8467 - val_acc: 0.6257\n",
      "Epoch 23/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.6624 - acc: 0.6328 - val_loss: 0.8299 - val_acc: 0.5927\n",
      "Epoch 24/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.6510 - acc: 0.6351 - val_loss: 0.8304 - val_acc: 0.6092\n",
      "Epoch 25/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.6431 - acc: 0.6470 - val_loss: 0.8280 - val_acc: 0.5945\n",
      "Epoch 26/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.6347 - acc: 0.6391 - val_loss: 0.8190 - val_acc: 0.5982\n",
      "Epoch 27/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.6296 - acc: 0.6580 - val_loss: 0.8254 - val_acc: 0.5798\n",
      "Epoch 28/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.6252 - acc: 0.6407 - val_loss: 0.8191 - val_acc: 0.5927\n",
      "Epoch 29/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.6174 - acc: 0.6493 - val_loss: 0.8040 - val_acc: 0.6055\n",
      "Epoch 30/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.6181 - acc: 0.6438 - val_loss: 0.8154 - val_acc: 0.6037\n",
      "Epoch 31/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.6125 - acc: 0.6525 - val_loss: 0.8113 - val_acc: 0.5394\n",
      "Epoch 32/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.6103 - acc: 0.6509 - val_loss: 0.8102 - val_acc: 0.6037\n",
      "Epoch 33/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.6017 - acc: 0.6517 - val_loss: 0.8089 - val_acc: 0.6202\n",
      "Epoch 34/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5973 - acc: 0.6604 - val_loss: 0.8243 - val_acc: 0.6110\n",
      "Epoch 35/1200\n",
      "1269/1269 [==============================] - 6s 5ms/step - loss: 0.5983 - acc: 0.6619 - val_loss: 0.8162 - val_acc: 0.5541\n",
      "Epoch 36/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5874 - acc: 0.6714 - val_loss: 0.8020 - val_acc: 0.5651\n",
      "Epoch 37/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5786 - acc: 0.6714 - val_loss: 0.7937 - val_acc: 0.6147\n",
      "Epoch 38/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5839 - acc: 0.6667 - val_loss: 0.7958 - val_acc: 0.5890\n",
      "Epoch 39/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5727 - acc: 0.6714 - val_loss: 0.8170 - val_acc: 0.5339\n",
      "Epoch 40/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5774 - acc: 0.6769 - val_loss: 0.7951 - val_acc: 0.5963\n",
      "Epoch 41/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5653 - acc: 0.6738 - val_loss: 0.7896 - val_acc: 0.5945\n",
      "Epoch 42/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5658 - acc: 0.6832 - val_loss: 0.7834 - val_acc: 0.6055\n",
      "Epoch 43/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5593 - acc: 0.6832 - val_loss: 0.7929 - val_acc: 0.5596\n",
      "Epoch 44/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5553 - acc: 0.6887 - val_loss: 0.7936 - val_acc: 0.6220\n",
      "Epoch 45/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5564 - acc: 0.6887 - val_loss: 0.7860 - val_acc: 0.5817\n",
      "Epoch 46/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5539 - acc: 0.6911 - val_loss: 0.7733 - val_acc: 0.6110\n",
      "Epoch 47/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5484 - acc: 0.6927 - val_loss: 0.7790 - val_acc: 0.6000\n",
      "Epoch 48/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5453 - acc: 0.6911 - val_loss: 0.7887 - val_acc: 0.6165\n",
      "Epoch 49/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5447 - acc: 0.6990 - val_loss: 0.7830 - val_acc: 0.5945\n",
      "Epoch 50/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5391 - acc: 0.6935 - val_loss: 0.7981 - val_acc: 0.5743\n",
      "Epoch 51/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5355 - acc: 0.7045 - val_loss: 0.7870 - val_acc: 0.6055\n",
      "Epoch 52/1200\n",
      "1269/1269 [==============================] - 6s 5ms/step - loss: 0.5394 - acc: 0.6974 - val_loss: 0.7832 - val_acc: 0.5688\n",
      "Epoch 53/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5316 - acc: 0.7006 - val_loss: 0.7902 - val_acc: 0.5633\n",
      "Epoch 54/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5325 - acc: 0.6958 - val_loss: 0.7828 - val_acc: 0.6110\n",
      "Epoch 55/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5205 - acc: 0.7124 - val_loss: 0.8070 - val_acc: 0.5394\n",
      "Epoch 56/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5240 - acc: 0.7061 - val_loss: 0.7812 - val_acc: 0.5908\n",
      "Epoch 57/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5183 - acc: 0.7195 - val_loss: 0.7853 - val_acc: 0.6055\n",
      "Epoch 58/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5154 - acc: 0.7092 - val_loss: 0.7807 - val_acc: 0.5761\n",
      "Epoch 59/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5189 - acc: 0.7242 - val_loss: 0.7802 - val_acc: 0.6220\n",
      "Epoch 60/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5071 - acc: 0.7313 - val_loss: 0.7755 - val_acc: 0.6055\n",
      "Epoch 61/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5079 - acc: 0.7329 - val_loss: 0.7916 - val_acc: 0.5486\n",
      "Epoch 62/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5039 - acc: 0.7132 - val_loss: 0.7781 - val_acc: 0.6165\n",
      "Epoch 63/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5037 - acc: 0.7305 - val_loss: 0.7815 - val_acc: 0.5743\n",
      "Epoch 64/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.5065 - acc: 0.7289 - val_loss: 0.7808 - val_acc: 0.6037\n",
      "Epoch 65/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4979 - acc: 0.7281 - val_loss: 0.7826 - val_acc: 0.5982\n",
      "Epoch 66/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4958 - acc: 0.7368 - val_loss: 0.7869 - val_acc: 0.5743\n",
      "Epoch 67/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4986 - acc: 0.7510 - val_loss: 0.7856 - val_acc: 0.5761\n",
      "Epoch 68/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4939 - acc: 0.7407 - val_loss: 0.7796 - val_acc: 0.5963\n",
      "Epoch 69/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4998 - acc: 0.7368 - val_loss: 0.7859 - val_acc: 0.5890\n",
      "Epoch 70/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4862 - acc: 0.7486 - val_loss: 0.7832 - val_acc: 0.5890\n",
      "Epoch 71/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4865 - acc: 0.7526 - val_loss: 0.7844 - val_acc: 0.5780\n",
      "Epoch 72/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4815 - acc: 0.7565 - val_loss: 0.7858 - val_acc: 0.5835\n",
      "Epoch 73/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4783 - acc: 0.7589 - val_loss: 0.7785 - val_acc: 0.5945\n",
      "Epoch 74/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4767 - acc: 0.7565 - val_loss: 0.7792 - val_acc: 0.5890\n",
      "Epoch 75/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4727 - acc: 0.7699 - val_loss: 0.7863 - val_acc: 0.5817\n",
      "Epoch 76/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4719 - acc: 0.7612 - val_loss: 0.7839 - val_acc: 0.5743\n",
      "Epoch 77/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4707 - acc: 0.7667 - val_loss: 0.7902 - val_acc: 0.5651\n",
      "Epoch 78/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4765 - acc: 0.7644 - val_loss: 0.7831 - val_acc: 0.6037\n",
      "Epoch 79/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4649 - acc: 0.7746 - val_loss: 0.7834 - val_acc: 0.5853\n",
      "Epoch 80/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4614 - acc: 0.7825 - val_loss: 0.7928 - val_acc: 0.5743\n",
      "Epoch 81/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4794 - acc: 0.7463 - val_loss: 0.7729 - val_acc: 0.6073\n",
      "Epoch 82/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4624 - acc: 0.7738 - val_loss: 0.8029 - val_acc: 0.5486\n",
      "Epoch 83/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4600 - acc: 0.7723 - val_loss: 0.7831 - val_acc: 0.6239\n",
      "Epoch 84/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4539 - acc: 0.7707 - val_loss: 0.7840 - val_acc: 0.5890\n",
      "Epoch 85/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4566 - acc: 0.7691 - val_loss: 0.7741 - val_acc: 0.5982\n",
      "Epoch 86/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4549 - acc: 0.7691 - val_loss: 0.7852 - val_acc: 0.6037\n",
      "Epoch 87/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4569 - acc: 0.7723 - val_loss: 0.7756 - val_acc: 0.5982\n",
      "Epoch 88/1200\n",
      "1269/1269 [==============================] - 6s 4ms/step - loss: 0.4491 - acc: 0.7794 - val_loss: 0.7806 - val_acc: 0.5927\n",
      "Epoch 89/1200\n",
      "1269/1269 [==============================] - 8s 6ms/step - loss: 0.4498 - acc: 0.7833 - val_loss: 0.7786 - val_acc: 0.5945\n",
      "Epoch 90/1200\n",
      "1269/1269 [==============================] - 7s 6ms/step - loss: 0.4487 - acc: 0.7786 - val_loss: 0.8087 - val_acc: 0.5450\n",
      "Epoch 91/1200\n",
      "1269/1269 [==============================] - 7s 6ms/step - loss: 0.4486 - acc: 0.7794 - val_loss: 0.7887 - val_acc: 0.5688\n",
      "Epoch 92/1200\n",
      "1269/1269 [==============================] - 7s 6ms/step - loss: 0.4424 - acc: 0.7975 - val_loss: 0.7934 - val_acc: 0.5761\n",
      "Epoch 93/1200\n",
      "1269/1269 [==============================] - 7s 6ms/step - loss: 0.4419 - acc: 0.7801 - val_loss: 0.8070 - val_acc: 0.5560\n",
      "Epoch 94/1200\n",
      "1269/1269 [==============================] - 7s 6ms/step - loss: 0.4354 - acc: 0.7991 - val_loss: 0.7929 - val_acc: 0.5890\n",
      "Epoch 95/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4356 - acc: 0.8030 - val_loss: 0.7817 - val_acc: 0.6128\n",
      "Epoch 96/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4312 - acc: 0.7991 - val_loss: 0.7901 - val_acc: 0.5927\n",
      "Epoch 97/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4300 - acc: 0.8101 - val_loss: 0.7867 - val_acc: 0.6073\n",
      "Epoch 98/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4300 - acc: 0.8046 - val_loss: 0.7957 - val_acc: 0.5706\n",
      "Epoch 99/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4286 - acc: 0.8061 - val_loss: 0.7815 - val_acc: 0.5872\n",
      "Epoch 100/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4248 - acc: 0.8069 - val_loss: 0.7804 - val_acc: 0.5817\n",
      "Epoch 101/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4232 - acc: 0.8156 - val_loss: 0.7983 - val_acc: 0.5963\n",
      "Epoch 102/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4237 - acc: 0.8101 - val_loss: 0.7877 - val_acc: 0.5890\n",
      "Epoch 103/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4199 - acc: 0.8030 - val_loss: 0.7913 - val_acc: 0.5835\n",
      "Epoch 104/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4198 - acc: 0.8109 - val_loss: 0.7903 - val_acc: 0.5982\n",
      "Epoch 105/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4207 - acc: 0.8125 - val_loss: 0.8023 - val_acc: 0.5780\n",
      "Epoch 106/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4189 - acc: 0.8054 - val_loss: 0.7791 - val_acc: 0.6055\n",
      "Epoch 107/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4184 - acc: 0.8109 - val_loss: 0.8164 - val_acc: 0.5560\n",
      "Epoch 108/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4146 - acc: 0.8117 - val_loss: 0.7851 - val_acc: 0.6037\n",
      "Epoch 109/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4126 - acc: 0.8085 - val_loss: 0.8013 - val_acc: 0.5817\n",
      "Epoch 110/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4092 - acc: 0.8156 - val_loss: 0.7946 - val_acc: 0.5835\n",
      "Epoch 111/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4097 - acc: 0.8195 - val_loss: 0.8034 - val_acc: 0.6165\n",
      "Epoch 112/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4120 - acc: 0.8093 - val_loss: 0.7952 - val_acc: 0.5963\n",
      "Epoch 113/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4033 - acc: 0.8211 - val_loss: 0.8040 - val_acc: 0.5817\n",
      "Epoch 114/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4033 - acc: 0.8140 - val_loss: 0.7950 - val_acc: 0.5982\n",
      "Epoch 115/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4024 - acc: 0.8266 - val_loss: 0.8067 - val_acc: 0.5963\n",
      "Epoch 116/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4006 - acc: 0.8251 - val_loss: 0.7929 - val_acc: 0.5890\n",
      "Epoch 117/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4135 - acc: 0.8132 - val_loss: 0.8118 - val_acc: 0.5780\n",
      "Epoch 118/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.4020 - acc: 0.8203 - val_loss: 0.7982 - val_acc: 0.6092\n",
      "Epoch 119/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3916 - acc: 0.8385 - val_loss: 0.8001 - val_acc: 0.5982\n",
      "Epoch 120/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3981 - acc: 0.8282 - val_loss: 0.8029 - val_acc: 0.6018\n",
      "Epoch 121/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3968 - acc: 0.8290 - val_loss: 0.8132 - val_acc: 0.5670\n",
      "Epoch 122/1200\n",
      "1269/1269 [==============================] - 6s 5ms/step - loss: 0.3882 - acc: 0.8432 - val_loss: 0.8118 - val_acc: 0.5725\n",
      "Epoch 123/1200\n",
      "1269/1269 [==============================] - 6s 5ms/step - loss: 0.3911 - acc: 0.8424 - val_loss: 0.8131 - val_acc: 0.5633\n",
      "Epoch 124/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3858 - acc: 0.8314 - val_loss: 0.8071 - val_acc: 0.5908\n",
      "Epoch 125/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3850 - acc: 0.8479 - val_loss: 0.8126 - val_acc: 0.5688\n",
      "Epoch 126/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3832 - acc: 0.8463 - val_loss: 0.8049 - val_acc: 0.6110\n",
      "Epoch 127/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3820 - acc: 0.8479 - val_loss: 0.8055 - val_acc: 0.6000\n",
      "Epoch 128/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3821 - acc: 0.8392 - val_loss: 0.8110 - val_acc: 0.5853\n",
      "Epoch 129/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3792 - acc: 0.8463 - val_loss: 0.8102 - val_acc: 0.5872\n",
      "Epoch 130/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3833 - acc: 0.8329 - val_loss: 0.8388 - val_acc: 0.5541\n",
      "Epoch 131/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3776 - acc: 0.8448 - val_loss: 0.8327 - val_acc: 0.5798\n",
      "Epoch 132/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3767 - acc: 0.8448 - val_loss: 0.8242 - val_acc: 0.5670\n",
      "Epoch 133/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3772 - acc: 0.8487 - val_loss: 0.8128 - val_acc: 0.5835\n",
      "Epoch 134/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3736 - acc: 0.8519 - val_loss: 0.8102 - val_acc: 0.5817\n",
      "Epoch 135/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3669 - acc: 0.8597 - val_loss: 0.8244 - val_acc: 0.5908\n",
      "Epoch 136/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3717 - acc: 0.8526 - val_loss: 0.7998 - val_acc: 0.5963\n",
      "Epoch 137/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3724 - acc: 0.8550 - val_loss: 0.8113 - val_acc: 0.5927\n",
      "Epoch 138/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3673 - acc: 0.8526 - val_loss: 0.8243 - val_acc: 0.5945\n",
      "Epoch 139/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3610 - acc: 0.8613 - val_loss: 0.8161 - val_acc: 0.5798\n",
      "Epoch 140/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3759 - acc: 0.8432 - val_loss: 0.8144 - val_acc: 0.5890\n",
      "Epoch 141/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3760 - acc: 0.8479 - val_loss: 0.8061 - val_acc: 0.6018\n",
      "Epoch 142/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3637 - acc: 0.8495 - val_loss: 0.8645 - val_acc: 0.5523\n",
      "Epoch 143/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3728 - acc: 0.8345 - val_loss: 0.8170 - val_acc: 0.5927\n",
      "Epoch 144/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3615 - acc: 0.8597 - val_loss: 0.8195 - val_acc: 0.5835\n",
      "Epoch 145/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3569 - acc: 0.8582 - val_loss: 0.8210 - val_acc: 0.5982\n",
      "Epoch 146/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3554 - acc: 0.8621 - val_loss: 0.8177 - val_acc: 0.5927\n",
      "Epoch 147/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3593 - acc: 0.8589 - val_loss: 0.8219 - val_acc: 0.5853\n",
      "Epoch 148/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3549 - acc: 0.8684 - val_loss: 0.8418 - val_acc: 0.5761\n",
      "Epoch 149/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3519 - acc: 0.8637 - val_loss: 0.8181 - val_acc: 0.5780\n",
      "Epoch 150/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3513 - acc: 0.8629 - val_loss: 0.8272 - val_acc: 0.5908\n",
      "Epoch 151/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3488 - acc: 0.8755 - val_loss: 0.8364 - val_acc: 0.5890\n",
      "Epoch 152/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3485 - acc: 0.8786 - val_loss: 0.8150 - val_acc: 0.6000\n",
      "Epoch 153/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3479 - acc: 0.8660 - val_loss: 0.8230 - val_acc: 0.5927\n",
      "Epoch 154/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3469 - acc: 0.8605 - val_loss: 0.8216 - val_acc: 0.5688\n",
      "Epoch 155/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3431 - acc: 0.8739 - val_loss: 0.8354 - val_acc: 0.5817\n",
      "Epoch 156/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3414 - acc: 0.8645 - val_loss: 0.8293 - val_acc: 0.5908\n",
      "Epoch 157/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3386 - acc: 0.8676 - val_loss: 0.8453 - val_acc: 0.5706\n",
      "Epoch 158/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3460 - acc: 0.8652 - val_loss: 0.8359 - val_acc: 0.5743\n",
      "Epoch 159/1200\n",
      "1269/1269 [==============================] - 7s 6ms/step - loss: 0.3393 - acc: 0.8763 - val_loss: 0.8351 - val_acc: 0.5560\n",
      "Epoch 160/1200\n",
      "1269/1269 [==============================] - 8s 6ms/step - loss: 0.3383 - acc: 0.8723 - val_loss: 0.8304 - val_acc: 0.5872\n",
      "Epoch 161/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3367 - acc: 0.8771 - val_loss: 0.8312 - val_acc: 0.5890\n",
      "Epoch 162/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3367 - acc: 0.8763 - val_loss: 0.8467 - val_acc: 0.5817\n",
      "Epoch 163/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3353 - acc: 0.8779 - val_loss: 0.8377 - val_acc: 0.5780\n",
      "Epoch 164/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3304 - acc: 0.8763 - val_loss: 0.8510 - val_acc: 0.5706\n",
      "Epoch 165/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3376 - acc: 0.8755 - val_loss: 0.8417 - val_acc: 0.5761\n",
      "Epoch 166/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3353 - acc: 0.8826 - val_loss: 0.8559 - val_acc: 0.5743\n",
      "Epoch 167/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3281 - acc: 0.8826 - val_loss: 0.8382 - val_acc: 0.5743\n",
      "Epoch 168/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3280 - acc: 0.8865 - val_loss: 0.8481 - val_acc: 0.5927\n",
      "Epoch 169/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3303 - acc: 0.8692 - val_loss: 0.8672 - val_acc: 0.5706\n",
      "Epoch 170/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3286 - acc: 0.8857 - val_loss: 0.8628 - val_acc: 0.5817\n",
      "Epoch 171/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3252 - acc: 0.8842 - val_loss: 0.8550 - val_acc: 0.5853\n",
      "Epoch 172/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3255 - acc: 0.8913 - val_loss: 0.8506 - val_acc: 0.5651\n",
      "Epoch 173/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3233 - acc: 0.8936 - val_loss: 0.8413 - val_acc: 0.5725\n",
      "Epoch 174/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3215 - acc: 0.8873 - val_loss: 0.8642 - val_acc: 0.5982\n",
      "Epoch 175/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3224 - acc: 0.8794 - val_loss: 0.8621 - val_acc: 0.5651\n",
      "Epoch 176/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3195 - acc: 0.8897 - val_loss: 0.8640 - val_acc: 0.5651\n",
      "Epoch 177/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3138 - acc: 0.8889 - val_loss: 0.8623 - val_acc: 0.5982\n",
      "Epoch 178/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3181 - acc: 0.8865 - val_loss: 0.8725 - val_acc: 0.5560\n",
      "Epoch 179/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3130 - acc: 0.8952 - val_loss: 0.8687 - val_acc: 0.5670\n",
      "Epoch 180/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3119 - acc: 0.8881 - val_loss: 0.8739 - val_acc: 0.5615\n",
      "Epoch 181/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3095 - acc: 0.9015 - val_loss: 0.8565 - val_acc: 0.5780\n",
      "Epoch 182/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3106 - acc: 0.8952 - val_loss: 0.8742 - val_acc: 0.5670\n",
      "Epoch 183/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3121 - acc: 0.8936 - val_loss: 0.8676 - val_acc: 0.5761\n",
      "Epoch 184/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3140 - acc: 0.8881 - val_loss: 0.8750 - val_acc: 0.5908\n",
      "Epoch 185/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3081 - acc: 0.8952 - val_loss: 0.8626 - val_acc: 0.5633\n",
      "Epoch 186/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3064 - acc: 0.8976 - val_loss: 0.8669 - val_acc: 0.5798\n",
      "Epoch 187/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3034 - acc: 0.8991 - val_loss: 0.8601 - val_acc: 0.5725\n",
      "Epoch 188/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.3007 - acc: 0.9023 - val_loss: 0.8766 - val_acc: 0.5835\n",
      "Epoch 189/1200\n",
      "1269/1269 [==============================] - 5s 4ms/step - loss: 0.2996 - acc: 0.9023 - val_loss: 0.8582 - val_acc: 0.5761\n",
      "Epoch 190/1200\n",
      "  64/1269 [>.............................] - ETA: 3s - loss: 0.3149 - acc: 0.8750"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.00001)\n",
    "# from keras.optimizers import SGD\n",
    "# opt = SGD(lr=0.00001)\n",
    "\n",
    "model1.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_9 = model1.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=1200,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
