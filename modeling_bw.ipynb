{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 17.6MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from keras) (1.14.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from keras) (1.11.0)\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/56/4bcec5a8d9503a87e58e814c4e32ac2b32c37c685672c30bc8c54c6e478a/Keras_Applications-1.0.8.tar.gz (289kB)\n",
      "\u001b[K    100% |████████████████████████████████| 296kB 26.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from keras) (2.8.0)\n",
      "Building wheels for collected packages: keras-applications\n",
      "  Running setup.py bdist_wheel for keras-applications ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/dd/f2/5d/2689b5547f32c4e258c3b7ccbe7f1d0f2afbb84fb01e830792\n",
      "Successfully built keras-applications\n",
      "\u001b[31mtyping-extensions 3.7.4.1 has requirement typing>=3.7.4; python_version < \"3.5\", but you'll have typing 3.6.4 which is incompatible.\u001b[0m\n",
      "Installing collected packages: keras-preprocessing, keras-applications, keras\n",
      "Successfully installed keras-2.3.1 keras-applications-1.0.8 keras-preprocessing-1.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting pip\n",
      "  Using cached https://files.pythonhosted.org/packages/00/b6/9cfa56b4081ad13874b0c6f96af8ce16cfbc1cb06bedf8e9164ce5551ec1/pip-19.3.1-py2.py3-none-any.whl\n",
      "\u001b[31mtyping-extensions 3.7.4.1 has requirement typing>=3.7.4; python_version < \"3.5\", but you'll have typing 3.6.4 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pip\n",
      "  Found existing installation: pip 10.0.1\n",
      "    Uninstalling pip-10.0.1:\n",
      "      Successfully uninstalled pip-10.0.1\n",
      "Successfully installed pip-19.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 3 classes.\n",
      "Found 1814 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Directory path\n",
    "train_data_dir = 'data/bwcracks/train'\n",
    "test_data_dir = 'data/bwcracks/test'\n",
    "\n",
    "# Get all the data in the directory data/validation, and reshape them\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "        test_data_dir, \n",
    "        target_size=(256, 256), batch_size=463)\n",
    "\n",
    "# Get all the data in the directory data/train, and reshape them\n",
    "train_generator = ImageDataGenerator().flow_from_directory(\n",
    "        train_data_dir, \n",
    "        target_size=(256, 256), batch_size=1814)\n",
    "\n",
    "# Create the datasets\n",
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=256x256 at 0x7F4FD72F3780>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previewing an image\n",
    "array_to_img(train_images[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1814, 256, 256, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (1814, 256, 256, 3) (1814, 3)\n",
      "test data shape: (0, 256, 256, 3) (0, 3)\n"
     ]
    }
   ],
   "source": [
    "# Checking shape of data\n",
    "print('train data shape:', np.shape(train_images), np.shape(train_labels))\n",
    "print('test data shape:', np.shape(test_images), np.shape(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_img: (1814, 196608)\n"
     ]
    }
   ],
   "source": [
    "# Unrowing/reshaping\n",
    "train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "print('train_img:', np.shape(train_img))\n",
    "\n",
    "# test_img = test_images.reshape(test_images.shape[0], -1)\n",
    "# print('test_img:', np.shape(test_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the labels\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.ipynb_checkpoints': 0, 'Dutch': 1, 'Flemish': 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels final: (1839, 1)\n",
      "test labels final: (463, 1)\n"
     ]
    }
   ],
   "source": [
    "# Transposing the labels\n",
    "train_y = np.reshape(train_labels[:,0], (1839,1))\n",
    "print('train labels final:', np.shape(train_y))\n",
    "\n",
    "test_y = np.reshape(test_labels[:,0], (463,1))\n",
    "print('test labels final:', np.shape(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(20, activation='relu', input_shape=(65536,))) #2 hidden layers\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1287 samples, validate on 552 samples\n",
      "Epoch 1/5\n",
      "1287/1287 [==============================] - 2s 1ms/step - loss: 8.9437 - acc: 0.4359 - val_loss: 9.1102 - val_acc: 0.4348\n",
      "Epoch 2/5\n",
      "1287/1287 [==============================] - 1s 439us/step - loss: 9.0923 - acc: 0.4359 - val_loss: 9.1102 - val_acc: 0.4348\n",
      "Epoch 3/5\n",
      "1287/1287 [==============================] - 1s 431us/step - loss: 9.0923 - acc: 0.4359 - val_loss: 9.1102 - val_acc: 0.4348\n",
      "Epoch 4/5\n",
      "1287/1287 [==============================] - 1s 432us/step - loss: 9.0923 - acc: 0.4359 - val_loss: 9.1102 - val_acc: 0.4348\n",
      "Epoch 5/5\n",
      "1287/1287 [==============================] - 1s 436us/step - loss: 9.0923 - acc: 0.4359 - val_loss: 9.1102 - val_acc: 0.4348\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "histoire = model.fit(train_img,\n",
    "                    train_y,\n",
    "                    epochs=5,\n",
    "                    batch_size=100,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=10, kernel_size=10, strides=2, activation='relu',\n",
    "                        input_shape=(256, 256,  1)))\n",
    "model.add(layers.MaxPooling2D((10, 10)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=10, kernel_size=5, strides=2,activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((4, 4)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=10, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=10, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=10, kernel_size=1, strides=2,activation='relu'))\n",
    "# model.add(layers.AveragePooling2D((1, 1)))\n",
    "\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "# model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "# model.add(layers.Dense(200, activation='relu'))\n",
    "# model.add(layers.Dense(200, activation='relu'))\n",
    "# model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1287 samples, validate on 552 samples\n",
      "Epoch 1/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.8795 - acc: 0.5338 - val_loss: 0.7437 - val_acc: 0.5525\n",
      "Epoch 2/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.7255 - acc: 0.5548 - val_loss: 0.7317 - val_acc: 0.5489\n",
      "Epoch 3/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.7010 - acc: 0.5726 - val_loss: 0.7255 - val_acc: 0.5217\n",
      "Epoch 4/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6928 - acc: 0.5789 - val_loss: 0.7298 - val_acc: 0.5362\n",
      "Epoch 5/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6868 - acc: 0.5781 - val_loss: 0.7219 - val_acc: 0.5399\n",
      "Epoch 6/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6751 - acc: 0.5952 - val_loss: 0.7111 - val_acc: 0.5616\n",
      "Epoch 7/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6693 - acc: 0.5944 - val_loss: 0.7115 - val_acc: 0.5489\n",
      "Epoch 8/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6684 - acc: 0.5921 - val_loss: 0.7063 - val_acc: 0.5580\n",
      "Epoch 9/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6611 - acc: 0.5983 - val_loss: 0.6992 - val_acc: 0.5652\n",
      "Epoch 10/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6579 - acc: 0.6045 - val_loss: 0.7040 - val_acc: 0.5725\n",
      "Epoch 11/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6572 - acc: 0.6123 - val_loss: 0.6964 - val_acc: 0.5598\n",
      "Epoch 12/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6532 - acc: 0.5967 - val_loss: 0.6977 - val_acc: 0.5435\n",
      "Epoch 13/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6483 - acc: 0.6099 - val_loss: 0.6961 - val_acc: 0.5489\n",
      "Epoch 14/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6457 - acc: 0.6115 - val_loss: 0.6915 - val_acc: 0.5634\n",
      "Epoch 15/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6442 - acc: 0.6076 - val_loss: 0.6957 - val_acc: 0.5507\n",
      "Epoch 16/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6425 - acc: 0.6185 - val_loss: 0.6889 - val_acc: 0.5707\n",
      "Epoch 17/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6413 - acc: 0.6146 - val_loss: 0.6920 - val_acc: 0.5616\n",
      "Epoch 18/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6373 - acc: 0.6239 - val_loss: 0.6966 - val_acc: 0.5707\n",
      "Epoch 19/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6405 - acc: 0.6131 - val_loss: 0.6901 - val_acc: 0.5562\n",
      "Epoch 20/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6367 - acc: 0.6239 - val_loss: 0.6945 - val_acc: 0.5489\n",
      "Epoch 21/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6355 - acc: 0.6325 - val_loss: 0.6873 - val_acc: 0.5725\n",
      "Epoch 22/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6333 - acc: 0.6232 - val_loss: 0.6879 - val_acc: 0.5634\n",
      "Epoch 23/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6312 - acc: 0.6255 - val_loss: 0.6907 - val_acc: 0.5598\n",
      "Epoch 24/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6308 - acc: 0.6247 - val_loss: 0.6856 - val_acc: 0.5634\n",
      "Epoch 25/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6290 - acc: 0.6301 - val_loss: 0.6846 - val_acc: 0.5670\n",
      "Epoch 26/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6286 - acc: 0.6270 - val_loss: 0.6865 - val_acc: 0.5670\n",
      "Epoch 27/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6264 - acc: 0.6294 - val_loss: 0.6862 - val_acc: 0.5670\n",
      "Epoch 28/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6260 - acc: 0.6356 - val_loss: 0.6868 - val_acc: 0.5725\n",
      "Epoch 29/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6208 - acc: 0.6457 - val_loss: 0.6880 - val_acc: 0.5652\n",
      "Epoch 30/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6202 - acc: 0.6434 - val_loss: 0.6919 - val_acc: 0.5634\n",
      "Epoch 31/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6221 - acc: 0.6356 - val_loss: 0.6824 - val_acc: 0.5670\n",
      "Epoch 32/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6218 - acc: 0.6441 - val_loss: 0.6833 - val_acc: 0.5725\n",
      "Epoch 33/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6225 - acc: 0.6441 - val_loss: 0.6872 - val_acc: 0.5652\n",
      "Epoch 34/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6164 - acc: 0.6410 - val_loss: 0.6880 - val_acc: 0.5652\n",
      "Epoch 35/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6125 - acc: 0.6503 - val_loss: 0.6878 - val_acc: 0.5688\n",
      "Epoch 36/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6152 - acc: 0.6441 - val_loss: 0.6809 - val_acc: 0.5688\n",
      "Epoch 37/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6114 - acc: 0.6550 - val_loss: 0.6862 - val_acc: 0.5670\n",
      "Epoch 38/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6148 - acc: 0.6418 - val_loss: 0.6839 - val_acc: 0.5616\n",
      "Epoch 39/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6093 - acc: 0.6581 - val_loss: 0.6806 - val_acc: 0.5707\n",
      "Epoch 40/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6084 - acc: 0.6542 - val_loss: 0.6813 - val_acc: 0.5688\n",
      "Epoch 41/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6079 - acc: 0.6472 - val_loss: 0.6832 - val_acc: 0.5652\n",
      "Epoch 42/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6070 - acc: 0.6566 - val_loss: 0.6877 - val_acc: 0.5761\n",
      "Epoch 43/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6052 - acc: 0.6597 - val_loss: 0.6821 - val_acc: 0.5707\n",
      "Epoch 44/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6047 - acc: 0.6643 - val_loss: 0.6804 - val_acc: 0.5761\n",
      "Epoch 45/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6030 - acc: 0.6573 - val_loss: 0.6840 - val_acc: 0.5634\n",
      "Epoch 46/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6031 - acc: 0.6566 - val_loss: 0.6817 - val_acc: 0.5761\n",
      "Epoch 47/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6049 - acc: 0.6511 - val_loss: 0.6857 - val_acc: 0.5725\n",
      "Epoch 48/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.6010 - acc: 0.6597 - val_loss: 0.6832 - val_acc: 0.5743\n",
      "Epoch 49/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5981 - acc: 0.6682 - val_loss: 0.6798 - val_acc: 0.5743\n",
      "Epoch 50/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5960 - acc: 0.6651 - val_loss: 0.6804 - val_acc: 0.5743\n",
      "Epoch 51/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5964 - acc: 0.6597 - val_loss: 0.6844 - val_acc: 0.5652\n",
      "Epoch 52/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5943 - acc: 0.6636 - val_loss: 0.6849 - val_acc: 0.5707\n",
      "Epoch 53/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5936 - acc: 0.6667 - val_loss: 0.6871 - val_acc: 0.5652\n",
      "Epoch 54/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5938 - acc: 0.6612 - val_loss: 0.6884 - val_acc: 0.5797\n",
      "Epoch 55/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5917 - acc: 0.6713 - val_loss: 0.6885 - val_acc: 0.5652\n",
      "Epoch 56/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5927 - acc: 0.6682 - val_loss: 0.6836 - val_acc: 0.5652\n",
      "Epoch 57/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5931 - acc: 0.6636 - val_loss: 0.6869 - val_acc: 0.5851\n",
      "Epoch 58/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5877 - acc: 0.6651 - val_loss: 0.6830 - val_acc: 0.5815\n",
      "Epoch 59/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5863 - acc: 0.6706 - val_loss: 0.6828 - val_acc: 0.5815\n",
      "Epoch 60/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5858 - acc: 0.6636 - val_loss: 0.6840 - val_acc: 0.5797\n",
      "Epoch 61/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5856 - acc: 0.6698 - val_loss: 0.6937 - val_acc: 0.5725\n",
      "Epoch 62/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5847 - acc: 0.6768 - val_loss: 0.6862 - val_acc: 0.5815\n",
      "Epoch 63/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5825 - acc: 0.6760 - val_loss: 0.6870 - val_acc: 0.5688\n",
      "Epoch 64/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5802 - acc: 0.6799 - val_loss: 0.6856 - val_acc: 0.5815\n",
      "Epoch 65/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5784 - acc: 0.6900 - val_loss: 0.6881 - val_acc: 0.5815\n",
      "Epoch 66/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5804 - acc: 0.6791 - val_loss: 0.6841 - val_acc: 0.5815\n",
      "Epoch 67/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.5779 - acc: 0.6783 - val_loss: 0.6854 - val_acc: 0.5797\n",
      "Epoch 68/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5775 - acc: 0.6830 - val_loss: 0.6865 - val_acc: 0.5851\n",
      "Epoch 69/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5760 - acc: 0.6783 - val_loss: 0.6847 - val_acc: 0.5797\n",
      "Epoch 70/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5721 - acc: 0.6931 - val_loss: 0.6914 - val_acc: 0.5815\n",
      "Epoch 71/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.5732 - acc: 0.6814 - val_loss: 0.6874 - val_acc: 0.5797\n",
      "Epoch 72/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.5737 - acc: 0.6822 - val_loss: 0.6865 - val_acc: 0.5833\n",
      "Epoch 73/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.5735 - acc: 0.6869 - val_loss: 0.6859 - val_acc: 0.5870\n",
      "Epoch 74/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.5697 - acc: 0.6892 - val_loss: 0.6870 - val_acc: 0.5851\n",
      "Epoch 75/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.5701 - acc: 0.6970 - val_loss: 0.6906 - val_acc: 0.5870\n",
      "Epoch 76/1200\n",
      "1287/1287 [==============================] - 4s 3ms/step - loss: 0.5671 - acc: 0.6884 - val_loss: 0.6907 - val_acc: 0.5870\n",
      "Epoch 77/1200\n",
      "1287/1287 [==============================] - 3s 2ms/step - loss: 0.5672 - acc: 0.6962 - val_loss: 0.6880 - val_acc: 0.5797\n",
      "Epoch 78/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5650 - acc: 0.6915 - val_loss: 0.6922 - val_acc: 0.5779\n",
      "Epoch 79/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5633 - acc: 0.6939 - val_loss: 0.6865 - val_acc: 0.5797\n",
      "Epoch 80/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5632 - acc: 0.6946 - val_loss: 0.6923 - val_acc: 0.5833\n",
      "Epoch 81/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5628 - acc: 0.6923 - val_loss: 0.6932 - val_acc: 0.5906\n",
      "Epoch 82/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5656 - acc: 0.6853 - val_loss: 0.6928 - val_acc: 0.5924\n",
      "Epoch 83/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5580 - acc: 0.7040 - val_loss: 0.6942 - val_acc: 0.5924\n",
      "Epoch 84/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5581 - acc: 0.7024 - val_loss: 0.6916 - val_acc: 0.5870\n",
      "Epoch 85/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5592 - acc: 0.7016 - val_loss: 0.6918 - val_acc: 0.5870\n",
      "Epoch 86/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5630 - acc: 0.6977 - val_loss: 0.6909 - val_acc: 0.5833\n",
      "Epoch 87/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5548 - acc: 0.7009 - val_loss: 0.6879 - val_acc: 0.5833\n",
      "Epoch 88/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5548 - acc: 0.7055 - val_loss: 0.6935 - val_acc: 0.5906\n",
      "Epoch 89/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5518 - acc: 0.7040 - val_loss: 0.7003 - val_acc: 0.5870\n",
      "Epoch 90/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5538 - acc: 0.7001 - val_loss: 0.6956 - val_acc: 0.5978\n",
      "Epoch 91/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5521 - acc: 0.7024 - val_loss: 0.6948 - val_acc: 0.5924\n",
      "Epoch 92/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5520 - acc: 0.7032 - val_loss: 0.6955 - val_acc: 0.5870\n",
      "Epoch 93/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5516 - acc: 0.6962 - val_loss: 0.6940 - val_acc: 0.5870\n",
      "Epoch 94/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5493 - acc: 0.7047 - val_loss: 0.7054 - val_acc: 0.5652\n",
      "Epoch 95/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5487 - acc: 0.7086 - val_loss: 0.6932 - val_acc: 0.5870\n",
      "Epoch 96/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5501 - acc: 0.7009 - val_loss: 0.6925 - val_acc: 0.5851\n",
      "Epoch 97/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5448 - acc: 0.7125 - val_loss: 0.6952 - val_acc: 0.5870\n",
      "Epoch 98/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5441 - acc: 0.7110 - val_loss: 0.6969 - val_acc: 0.5924\n",
      "Epoch 99/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5504 - acc: 0.7032 - val_loss: 0.6964 - val_acc: 0.5978\n",
      "Epoch 100/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5422 - acc: 0.7078 - val_loss: 0.6966 - val_acc: 0.5960\n",
      "Epoch 101/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5414 - acc: 0.7156 - val_loss: 0.6971 - val_acc: 0.5906\n",
      "Epoch 102/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5416 - acc: 0.7055 - val_loss: 0.7009 - val_acc: 0.5924\n",
      "Epoch 103/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5375 - acc: 0.7148 - val_loss: 0.6949 - val_acc: 0.5888\n",
      "Epoch 104/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5365 - acc: 0.7102 - val_loss: 0.7018 - val_acc: 0.6014\n",
      "Epoch 105/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5359 - acc: 0.7195 - val_loss: 0.6992 - val_acc: 0.5906\n",
      "Epoch 106/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5352 - acc: 0.7086 - val_loss: 0.7014 - val_acc: 0.5996\n",
      "Epoch 107/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5403 - acc: 0.7226 - val_loss: 0.7081 - val_acc: 0.5707\n",
      "Epoch 108/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5362 - acc: 0.7047 - val_loss: 0.7018 - val_acc: 0.5906\n",
      "Epoch 109/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5335 - acc: 0.7141 - val_loss: 0.6992 - val_acc: 0.5833\n",
      "Epoch 110/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5320 - acc: 0.7148 - val_loss: 0.7005 - val_acc: 0.6033\n",
      "Epoch 111/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5314 - acc: 0.7086 - val_loss: 0.6984 - val_acc: 0.5942\n",
      "Epoch 112/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5312 - acc: 0.7110 - val_loss: 0.7097 - val_acc: 0.5707\n",
      "Epoch 113/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5295 - acc: 0.7133 - val_loss: 0.6971 - val_acc: 0.5924\n",
      "Epoch 114/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5280 - acc: 0.7288 - val_loss: 0.7023 - val_acc: 0.5906\n",
      "Epoch 115/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5255 - acc: 0.7117 - val_loss: 0.7079 - val_acc: 0.5851\n",
      "Epoch 116/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5237 - acc: 0.7218 - val_loss: 0.7068 - val_acc: 0.5906\n",
      "Epoch 117/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5241 - acc: 0.7280 - val_loss: 0.7035 - val_acc: 0.5942\n",
      "Epoch 118/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5244 - acc: 0.7211 - val_loss: 0.7083 - val_acc: 0.5978\n",
      "Epoch 119/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5214 - acc: 0.7203 - val_loss: 0.7057 - val_acc: 0.5996\n",
      "Epoch 120/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5229 - acc: 0.7218 - val_loss: 0.7056 - val_acc: 0.5906\n",
      "Epoch 121/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5200 - acc: 0.7350 - val_loss: 0.7100 - val_acc: 0.5978\n",
      "Epoch 122/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5170 - acc: 0.7273 - val_loss: 0.7104 - val_acc: 0.5924\n",
      "Epoch 123/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5180 - acc: 0.7343 - val_loss: 0.7087 - val_acc: 0.5924\n",
      "Epoch 124/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5162 - acc: 0.7319 - val_loss: 0.7043 - val_acc: 0.5960\n",
      "Epoch 125/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5130 - acc: 0.7350 - val_loss: 0.7094 - val_acc: 0.5942\n",
      "Epoch 126/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5130 - acc: 0.7335 - val_loss: 0.7079 - val_acc: 0.6033\n",
      "Epoch 127/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5162 - acc: 0.7288 - val_loss: 0.7082 - val_acc: 0.5978\n",
      "Epoch 128/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5115 - acc: 0.7358 - val_loss: 0.7157 - val_acc: 0.5924\n",
      "Epoch 129/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5135 - acc: 0.7405 - val_loss: 0.7152 - val_acc: 0.5851\n",
      "Epoch 130/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5088 - acc: 0.7343 - val_loss: 0.7120 - val_acc: 0.6087\n",
      "Epoch 131/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5087 - acc: 0.7312 - val_loss: 0.7143 - val_acc: 0.5815\n",
      "Epoch 132/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5089 - acc: 0.7312 - val_loss: 0.7182 - val_acc: 0.5906\n",
      "Epoch 133/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5092 - acc: 0.7343 - val_loss: 0.7136 - val_acc: 0.5996\n",
      "Epoch 134/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5121 - acc: 0.7242 - val_loss: 0.7143 - val_acc: 0.5996\n",
      "Epoch 135/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5048 - acc: 0.7420 - val_loss: 0.7182 - val_acc: 0.5942\n",
      "Epoch 136/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5082 - acc: 0.7397 - val_loss: 0.7129 - val_acc: 0.5996\n",
      "Epoch 137/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5006 - acc: 0.7382 - val_loss: 0.7241 - val_acc: 0.5924\n",
      "Epoch 138/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5035 - acc: 0.7413 - val_loss: 0.7149 - val_acc: 0.5960\n",
      "Epoch 139/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5004 - acc: 0.7475 - val_loss: 0.7291 - val_acc: 0.5779\n",
      "Epoch 140/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.5004 - acc: 0.7389 - val_loss: 0.7216 - val_acc: 0.5888\n",
      "Epoch 141/1200\n",
      "1287/1287 [==============================] - 3s 3ms/step - loss: 0.4958 - acc: 0.7467 - val_loss: 0.7183 - val_acc: 0.6033\n",
      "Epoch 142/1200\n",
      "1287/1287 [==============================] - 3s 3ms/step - loss: 0.4947 - acc: 0.7529 - val_loss: 0.7223 - val_acc: 0.5978\n",
      "Epoch 143/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4940 - acc: 0.7576 - val_loss: 0.7252 - val_acc: 0.5924\n",
      "Epoch 144/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4949 - acc: 0.7436 - val_loss: 0.7200 - val_acc: 0.6051\n",
      "Epoch 145/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4925 - acc: 0.7451 - val_loss: 0.7275 - val_acc: 0.5960\n",
      "Epoch 146/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4949 - acc: 0.7483 - val_loss: 0.7192 - val_acc: 0.5978\n",
      "Epoch 147/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4913 - acc: 0.7584 - val_loss: 0.7243 - val_acc: 0.5942\n",
      "Epoch 148/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4934 - acc: 0.7529 - val_loss: 0.7231 - val_acc: 0.5996\n",
      "Epoch 149/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4874 - acc: 0.7599 - val_loss: 0.7232 - val_acc: 0.5942\n",
      "Epoch 150/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4873 - acc: 0.7560 - val_loss: 0.7267 - val_acc: 0.5942\n",
      "Epoch 151/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4862 - acc: 0.7607 - val_loss: 0.7247 - val_acc: 0.5996\n",
      "Epoch 152/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4825 - acc: 0.7552 - val_loss: 0.7323 - val_acc: 0.5851\n",
      "Epoch 153/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4827 - acc: 0.7568 - val_loss: 0.7390 - val_acc: 0.5761\n",
      "Epoch 154/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4819 - acc: 0.7576 - val_loss: 0.7249 - val_acc: 0.6051\n",
      "Epoch 155/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4837 - acc: 0.7552 - val_loss: 0.7239 - val_acc: 0.6033\n",
      "Epoch 156/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4808 - acc: 0.7638 - val_loss: 0.7297 - val_acc: 0.6033\n",
      "Epoch 157/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4807 - acc: 0.7700 - val_loss: 0.7338 - val_acc: 0.6014\n",
      "Epoch 158/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4784 - acc: 0.7646 - val_loss: 0.7354 - val_acc: 0.5888\n",
      "Epoch 159/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4786 - acc: 0.7552 - val_loss: 0.7340 - val_acc: 0.5942\n",
      "Epoch 160/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4770 - acc: 0.7669 - val_loss: 0.7301 - val_acc: 0.6051\n",
      "Epoch 161/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4732 - acc: 0.7677 - val_loss: 0.7288 - val_acc: 0.6105\n",
      "Epoch 162/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4733 - acc: 0.7700 - val_loss: 0.7317 - val_acc: 0.6087\n",
      "Epoch 163/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4709 - acc: 0.7692 - val_loss: 0.7375 - val_acc: 0.5996\n",
      "Epoch 164/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4693 - acc: 0.7778 - val_loss: 0.7326 - val_acc: 0.6033\n",
      "Epoch 165/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4672 - acc: 0.7739 - val_loss: 0.7405 - val_acc: 0.5942\n",
      "Epoch 166/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4718 - acc: 0.7638 - val_loss: 0.7297 - val_acc: 0.6087\n",
      "Epoch 167/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4674 - acc: 0.7669 - val_loss: 0.7423 - val_acc: 0.6051\n",
      "Epoch 168/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4689 - acc: 0.7731 - val_loss: 0.7393 - val_acc: 0.6051\n",
      "Epoch 169/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4652 - acc: 0.7622 - val_loss: 0.7462 - val_acc: 0.5906\n",
      "Epoch 170/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4618 - acc: 0.7879 - val_loss: 0.7398 - val_acc: 0.6069\n",
      "Epoch 171/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4658 - acc: 0.7630 - val_loss: 0.7421 - val_acc: 0.6033\n",
      "Epoch 172/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4662 - acc: 0.7661 - val_loss: 0.7395 - val_acc: 0.6014\n",
      "Epoch 173/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4630 - acc: 0.7700 - val_loss: 0.7442 - val_acc: 0.5960\n",
      "Epoch 174/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4574 - acc: 0.7739 - val_loss: 0.7480 - val_acc: 0.5960\n",
      "Epoch 175/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4570 - acc: 0.7747 - val_loss: 0.7469 - val_acc: 0.6051\n",
      "Epoch 176/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4572 - acc: 0.7817 - val_loss: 0.7468 - val_acc: 0.5978\n",
      "Epoch 177/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4535 - acc: 0.7824 - val_loss: 0.7467 - val_acc: 0.6087\n",
      "Epoch 178/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4540 - acc: 0.7824 - val_loss: 0.7441 - val_acc: 0.6069\n",
      "Epoch 179/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4542 - acc: 0.7894 - val_loss: 0.7494 - val_acc: 0.6105\n",
      "Epoch 180/1200\n",
      "1287/1287 [==============================] - 2s 2ms/step - loss: 0.4610 - acc: 0.7723 - val_loss: 0.7518 - val_acc: 0.6014\n",
      "Epoch 181/1200\n",
      " 704/1287 [===============>..............] - ETA: 0s - loss: 0.4539 - acc: 0.7812"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.00001)\n",
    "# from keras.optimizers import SGD\n",
    "# opt = SGD(lr=0.00001)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_9 = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=1200,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
